#!/bin/bash
# Atlassian API Jira Helper
# Usage: source jira-helper.sh

# Version
JIRA_HELPER_VERSION="1.0.0-rc11"

# Detect GNU tools (prefer GNU versions over macOS BSD versions)
AWK=$(command -v gawk || command -v awk)
SED=$(command -v gsed || command -v sed)
GREP=$(command -v ggrep || command -v grep)
DATE=$(command -v gdate || command -v date)
FIND=$(command -v gfind || command -v find)
XARGS=$(command -v gxargs || command -v xargs)

# Validate we have the minimum required tools
if [ -z "$AWK" ] || [ -z "$SED" ]; then
  echo "Error: Required tools (awk/sed) not found" >&2
  return 1 2>/dev/null || exit 1
fi

# Load Atlassian credentials (if available) - basic load, validation happens in functions
if [ -f ~/.jira ]; then
  # shellcheck source=/dev/null
  source ~/.jira
fi

# Cache directory is relative to the script location
# When installed: ~/.jira-helper/.atlassian-cache
# When in dev: <repo>/.atlassian-cache
CACHE_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)/.atlassian-cache"
JIRA_HELPER_QUIET="${JIRA_HELPER_QUIET:-false}"

# Ensure cache directory exists
mkdir -p "$CACHE_DIR" 2>/dev/null || true

# Load library modules
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
if [ -f "${SCRIPT_DIR}/lib/http-helpers.sh" ]; then
  # shellcheck source=lib/http-helpers.sh
  source "${SCRIPT_DIR}/lib/http-helpers.sh"
fi
if [ -f "${SCRIPT_DIR}/lib/cache-helpers.sh" ]; then
  # shellcheck source=lib/cache-helpers.sh
  source "${SCRIPT_DIR}/lib/cache-helpers.sh"
fi

# GitHub repository configuration for self_update
JIRA_HELPER_GITHUB_REPO="${JIRA_HELPER_GITHUB_REPO:-uniphore/platform-utilities}"
JIRA_HELPER_GITHUB_REF="${JIRA_HELPER_GITHUB_REF:-v${JIRA_HELPER_VERSION}}"  # Use tag by default, not branch
JIRA_HELPER_GITHUB_PATH="${JIRA_HELPER_GITHUB_PATH:-jira-helper/jira-helper.sh}"

# Central registry for tracking all jira-helper workspaces
JIRA_HELPER_REGISTRY="${HOME}/.jira-helper/registry.json"

# Helper function to output messages (respects quiet mode)
_log() {
  if [ "$JIRA_HELPER_QUIET" != "true" ]; then
    echo "$@" >&2
  fi
}

# IMPORTANT: This script MUST comply with ~/.claude/config directives
# - Use ASCII-only output (no Unicode characters)
# - Follow formatting standards defined in config
# ASCII-only is the default for jira-helper output
export JIRA_HELPER_ASCII_ONLY=true

# Validate jq output count against expected count
# Usage: _validate_jq_output "$output" "$expected_count" "$description"
# Returns: 0 if valid, 1 if mismatch (with warning to stderr)
_validate_jq_output() {
  local output="$1"
  local expected="$2"
  local description="${3:-jq output}"
  local pattern="${4:-^[^ ]}"  # Default pattern: lines starting with non-space

  local actual
  actual=$(echo "$output" | grep -c "$pattern" 2>/dev/null || echo 0)

  if [ "$actual" -lt "$expected" ]; then
    echo "[WARN] $description: Expected $expected items but got $actual" >&2
    if [ -s /tmp/jira-helper-jq-stderr.log ]; then
      echo "[WARN] jq stderr: $(head -3 /tmp/jira-helper-jq-stderr.log)" >&2
    fi
    return 1
  fi
  return 0
}

# Safely source ~/.jira credentials file with error checking
# Usage: _source_credentials || return 1
_source_credentials() {
  if [ ! -f ~/.jira ]; then
    echo "Error: ~/.jira credentials file not found" >&2
    echo "" >&2
    echo "Please run setup-credentials.sh to create credentials file:" >&2
    echo "  cd $(dirname "${BASH_SOURCE[0]}")" >&2
    echo "  ./setup-credentials.sh" >&2
    return 1
  fi

  # Test if file has syntax errors before sourcing
  if ! bash -n ~/.jira 2>/dev/null; then
    echo "Error: ~/.jira file has syntax errors" >&2
    echo "Please fix the file or run setup-credentials.sh again" >&2
    return 1
  fi

  # Use safe credentials loading
  source ~/.jira || {
    echo "Error: Failed to source ~/.jira" >&2
    return 1
  }

  # Validate required variables are set
  if [ -z "$ATLASSIAN_USER" ] || [ -z "$ATLASSIAN_API_TOKEN" ] || [ -z "$ATLASSIAN_SITE_URL" ]; then
    echo "Error: Required credentials missing in ~/.jira" >&2
    echo "Required: ATLASSIAN_USER, ATLASSIAN_API_TOKEN, ATLASSIAN_SITE_URL" >&2
    echo "Please run setup-credentials.sh" >&2
    return 1
  fi

  return 0
}

# Display usage information
# Usage: jira_helper_cmd
jira_helper_cmd() {
  local subcommand="$1"

  # If subcommand specified, show context-specific help
  if [ -n "$subcommand" ]; then
    case "$subcommand" in
      issue)
        cat << 'EOF'
Usage: jira-helper issue <key|URL> [--json]
       jira-helper issue open <key|URL>
       jira-helper issue transitions <key|URL>
       jira-helper issue comment <key|URL> "comment text"
       jira-helper issue update <key|URL> <field> <value>
       jira-helper issue update-comment <key|URL> <comment-id> "text"
       jira-helper issue create <project> "summary" "description" [type]
       jira-helper issue create-subtask <parent-key> "summary" "description"

Get or interact with Jira issues

Examples:
  jira-helper issue PANK-1485
  jira-helper issue https://site.atlassian.net/browse/PANK-1485
  jira-helper issue PANK-1485 --json
  jira-helper issue open PANK-1485                              # Open in browser
  jira-helper issue transitions PANK-1485
  jira-helper issue comment PANK-1485 "Working on this now"
  jira-helper issue update PANK-1485 priority High
  jira-helper issue update PANK-1485 status 51
  jira-helper issue update-comment PANK-1485 12345 "Updated comment text"
  jira-helper issue create PANK "Bug in login" "Users can't log in" Bug
  jira-helper issue create-subtask PANK-1485 "Subtask summary" "Description"

Flags:
  --json    Output raw JSON instead of formatted text

Subcommands:
  open              Open issue in default browser
  transitions       Show available workflow transitions for the issue
  comment           Add a comment to the issue
  update            Update issue field (priority, assignee, status)
  update-comment    Update existing comment
  create            Create new issue
  create-subtask    Create subtask under parent issue
EOF
        ;;
      issues)
        cat << 'EOF'
Usage: jira-helper issues [search|mine] [days|"text query"]

Search Jira issues - by time period OR text query
Displays: issue type, status, priority, parent/child hierarchy, and relationships

Examples:
  jira-helper issues                        # Your updates today
  jira-helper issues mine 7                 # Your updates last 7 days
  jira-helper issues search "jira-helper"   # Search for text in your issues
  jira-helper issues search 30              # Your updates last 30 days

Output includes:
  - Issue type (Story, Task, Bug, Epic, Sub-task)
  - Hierarchical display (subtasks grouped under parents with ↳)
  - Relationships (relates to, blocks, implements, etc.)
  - Status/priority transitions (Bklog->InProg, Low->High)

Parameters:
  days          Number of days to search (numeric, default: 1)
  "text query"  Search text in summary, description, and comments

Subcommands:
  search    Search your issues by days or text
  mine      Same as search (alias)
EOF
        ;;
      doc)
        cat << 'EOF'
Usage: jira-helper doc <page-id>
       jira-helper doc open <page-id>
       jira-helper doc source <page-id>
       jira-helper doc set-source <page-id> <path>

Confluence page operations

Examples:
  jira-helper doc 4214128641                           # Get page
  jira-helper doc open 4214128641                      # Open in browser
  jira-helper doc source 4214128641                    # Get source path
  jira-helper doc set-source 4214128641 /path/doc.md   # Set source path

Parameters:
  page-id    Confluence page ID (numeric)
  path       Local file path for source mapping

Subcommands:
  <page-id>       Get Confluence page data
  open            Open page in default browser
  source          Get source file path for a page
  set-source      Map a page to its local source file
EOF
        ;;
      docs)
        cat << 'EOF'
Usage: jira-helper docs [mine|sources] [days|text]

Search Confluence pages - defaults to 'mine' if no subcommand

Examples:
  jira-helper docs                    # Your page updates (last 5 days)
  jira-helper docs mine 7             # Your updates last 7 days
  jira-helper docs mine "jira-helper" # Text search your pages
  jira-helper docs sources            # List all source mappings

Parameters:
  days    Number of days to search (default: 5)
  text    Text to search in page content

Subcommands:
  mine       Search pages you updated (supports days or text)
  sources    List all source file mappings
EOF
        ;;
      get-issue|show-issue)
        cat << 'EOF'
Usage: jira-helper get-issue <issue-key or URL> [--json]

Get a Jira issue - formatted by default, JSON with --json flag
(Legacy alias - prefer 'jira-helper issue' for new usage)

Examples:
  jira-helper get-issue PANK-1485
  jira-helper get-issue https://site.atlassian.net/browse/PANK-1485
  jira-helper get-issue PANK-1485 --json

Flags:
  --json    Output raw JSON instead of formatted text
EOF
        ;;
      set-source)
        cat << 'EOF'
Usage: jira-helper set-source <page-id> <source-file-path>

Map a Confluence page to its local source file

Examples:
  jira-helper set-source 4225433659 /path/to/source.md

Parameters:
  page-id          Confluence page ID (numeric)
  source-file-path  Path to local markdown file
EOF
        ;;
      get-source)
        cat << 'EOF'
Usage: jira-helper get-source <page-id>

Get the source file path for a Confluence page

Examples:
  jira-helper get-source 4225433659

Returns:
  File path if mapping exists, empty string otherwise
EOF
        ;;
      eod|eod-report)
        cat << 'EOF'
Usage: jira-helper eod [days] [template]

Generate an End of Day status report

Examples:
  jira-helper eod                    # Today's report
  jira-helper eod 1                  # Today's report
  jira-helper eod 7                  # Last 7 days
  jira-helper eod 1 slack            # Today in Slack format
  jira-helper eod 7 slack_compact    # Week in compact Slack format

Parameters:
  days      Number of days to report (default: 1)
  template  Output format: default, slack, slack_compact, slack_plain
EOF
        ;;
      add-comment|comment)
        cat << 'EOF'
Usage: jira-helper add-comment <issue-key or URL> <comment-text> [--direct]

Add a comment to a Jira issue

Examples:
  jira-helper add-comment PANK-1485 "Working on this now"
  jira-helper add-comment PANK-1485 "Done" --direct

Flags:
  --direct  Skip style selection prompt (for scripts)
EOF
        ;;
      update-comment)
        cat << 'EOF'
Usage: jira-helper update-comment <issue-key or URL> <comment-id> <comment-text>

Update an existing Jira comment

Examples:
  jira-helper update-comment PANK-1485 463793 "Updated text"

Parameters:
  issue-key    Jira issue key (e.g., PANK-1485) or full URL
  comment-id   Numeric comment ID
  comment-text  New comment text
EOF
        ;;
      create-ticket|create)
        cat << 'EOF'
Usage: jira-helper create-ticket <project> <summary> [description] [issue-type]

Create a new Jira ticket

Examples:
  jira-helper create-ticket PANK "Fix login bug"
  jira-helper create-ticket PANK "Add feature" "Description here" Task
  jira-helper create-ticket PANK "Critical bug" "Details" Bug

Parameters:
  project     Project key (e.g., PANK)
  summary     Brief title
  description  Optional detailed description
  issue-type   Task (default), Story, Bug, or Epic
EOF
        ;;
      create-subtask|subtask)
        cat << 'EOF'
Usage: jira-helper create-subtask <parent-key> <summary> [description]

Create a Jira sub-task

Examples:
  jira-helper create-subtask PANK-1485 "Subtask title"
  jira-helper create-subtask PANK-1485 "Subtask" "Detailed description"

Parameters:
  parent-key   Parent issue key (e.g., PANK-1485)
  summary      Brief title for subtask
  description   Optional detailed description
EOF
        ;;
      update-issue)
        cat << 'EOF'
Usage: jira-helper update-issue <issue-key or URL> <field-type> <value>

Update a Jira issue field

Examples:
  jira-helper update-issue PANK-1485 priority High
  jira-helper update-issue PANK-1485 priority 2
  jira-helper update-issue PANK-1485 status 51

Fields:
  priority    Priority name (High, Medium, Low) or numeric ID
  status      Status transition ID (use get-transitions to see available)
EOF
        ;;
      get-transitions|transitions)
        cat << 'EOF'
Usage: jira-helper get-transitions <issue-key or URL>

List available status transitions for an issue

Examples:
  jira-helper get-transitions PANK-1485

Output:
  Table of transition IDs and names that can be used with update-issue
EOF
        ;;
      search-issues|search)
        cat << 'EOF'
Usage: jira-helper search-issues [days]

Search for Jira issues you updated in the last N days

Examples:
  jira-helper search-issues      # Last 5 days (default)
  jira-helper search-issues 7    # Last 7 days

Parameters:
  days  Number of days to search (default: 5)
EOF
        ;;
      get-page|page)
        cat << 'EOF'
Usage: jira-helper get-page <page-id>

Get Confluence page data

Examples:
  jira-helper get-page 4214128641

Parameters:
  page-id  Confluence page ID (numeric)
EOF
        ;;
      search-pages)
        cat << 'EOF'
Usage: jira-helper search-pages [days]

Search for Confluence pages you updated in the last N days

Examples:
  jira-helper search-pages      # Last 5 days (default)
  jira-helper search-pages 7    # Last 7 days

Parameters:
  days  Number of days to search (default: 5)
EOF
        ;;
      metrics-*)
        cat << 'EOF'
Usage: jira-helper <metrics-command> [parameters]

Available metrics commands:
  metrics-volume [days] [project]     Created/closed/open counts
  metrics-creation [days]             Top creators and trends
  metrics-age [project] [max-pages]   Age distribution
  metrics-priority [days] [project]   Priority health
  metrics-churn [days] [project]      Reopened tickets
  metrics-personal [email]            Your workload
  metrics-painpoints [project]        Blocked and unassigned

Examples:
  jira-helper metrics-volume 7
  jira-helper metrics-personal
  jira-helper metrics-painpoints PANK

Use 'jira-helper help' for full documentation
EOF
        ;;
      *)
        echo "Unknown command: $subcommand"
        echo "Use 'jira-helper help' to see all commands"
        return 1
        ;;
    esac
    return 0
  fi

  # Show full help if no subcommand specified
  cat << 'EOF'
Atlassian API Jira Helper - Available Commands
===============================================

NEW: SIMPLE COMMANDS (Option 3)
-------------------------------
  # View and search
  jira-helper issue PANK-1485                         # Show issue (formatted)
  jira-helper issue PANK-1485 --json                  # Show issue (JSON)
  jira-helper issue transitions PANK-1485             # Show transitions
  jira-helper issues                                  # Your updates today
  jira-helper issues mine 7                           # Your updates last 7 days

  # Modify issues
  jira-helper issue comment PANK-1485 "Working on this"       # Add comment
  jira-helper issue update PANK-1485 priority High            # Update field
  jira-helper issue update-comment PANK-1485 12345 "Updated"  # Update comment
  jira-helper issue create PANK "Summary" "Description" Bug   # Create issue
  jira-helper issue create-subtask PANK-1485 "Summary" "Desc" # Create subtask

  # Confluence docs
  jira-helper doc 4214128641                          # Get page
  jira-helper doc update 4214128641 "content"         # Update page
  jira-helper doc replace 4214128641 "content"        # Replace page
  jira-helper docs                                    # Your page updates

  # Reports and metrics
  jira-helper eod 1 slack                             # Daily standup
  jira-helper metrics volume 7                        # Volume metrics

Use 'jira-helper help issue' or 'jira-helper help issues' for more details.

JIRA ISSUE COMMANDS:
  needs_jira_refresh <issue-key>
    Check if a Jira issue cache needs refreshing
    Example: needs_jira_refresh PANK-1334
    Returns: 0 if refresh needed, 1 if cache is current

  fetch_jira_issue <issue-key>
    Force fetch and cache a Jira issue from API
    Example: fetch_jira_issue PANK-1334
    Creates: jira-<issue-key>.json and jira-<issue-key>.timestamp

  get_jira_issue <issue-key>
    Get Jira issue data (auto-refreshes if needed)
    Example: get_jira_issue PANK-1334
    Output: Formatted text by default, JSON with --json flag

CONFLUENCE PAGE COMMANDS:
  needs_confluence_refresh <page-id>
    Check if a Confluence page cache needs refreshing
    Example: needs_confluence_refresh 4214128641
    Returns: 0 if refresh needed, 1 if cache is current

  fetch_confluence_page <page-id>
    Force fetch and cache a Confluence page from API
    Example: fetch_confluence_page 4214128641
    Creates: confluence-<page-id>.json and confluence-<page-id>.timestamp

  get_confluence_page <page-id>
    Get Confluence page data (auto-refreshes if needed)
    Example: get_confluence_page 4214128641
    Output: JSON data to stdout

CONFLUENCE SOURCE MAPPING COMMANDS:
  set_confluence_source <page-id> <source-file-path>
    Map a Confluence page to its local source file
    Example: set_confluence_source 4225433659 /path/to/source.md
    Creates: confluence-<page-id>.metadata

  get_confluence_source <page-id>
    Get the source file path for a Confluence page
    Example: get_confluence_source 4225433659
    Output: File path to stdout (empty if no mapping exists)

  list_confluence_sources
    List all Confluence pages with their source file mappings
    Example: list_confluence_sources
    Output: Table of page IDs and source files

SEARCH COMMANDS:
  search_my_jira_updates [days]
    Search for Jira issues you updated in the last N days (default: 5)
    Example: search_my_jira_updates
    Example: search_my_jira_updates 7
    Output: List of issue keys with summaries

  search_my_confluence_updates [days]
    Search for Confluence pages you updated in the last N days (default: 5)
    Example: search_my_confluence_updates
    Example: search_my_confluence_updates 7
    Output: List of page IDs with titles

JIRA UPDATE COMMANDS:
  add_jira_comment <issue-key> <comment-text>
    Add a comment to a Jira issue
    Example: add_jira_comment PANK-1499 "Working on this now"
    Output: Comment ID if successful

  update_jira_issue <issue-key> <field-type> <value>
    Update a Jira issue field (priority, assignee, or status)
    Example: update_jira_issue PANK-1499 priority 3
    Example: update_jira_issue PANK-1499 status 51
    Note: Automatically invalidates cache after update

  get_jira_transitions <issue-key>
    List available status transitions for an issue
    Example: get_jira_transitions PANK-1499
    Output: Table of status transition IDs and names

REPORTING COMMANDS:
  eod_report [days] [template_file] [is_slack]
    Generate an End of Day status report (default: 1 day)
    Example: eod_report
    Example: eod_report 2
    Example: eod_report 1 eod-report-slack-template.txt
    Example: eod_report 1 eod-report-template.txt false
    Output: Formatted report of Jira and Confluence activity
    Note: Auto-detects Slack format if template filename contains "slack"

    Available template variables:
      {{DATE_LABEL}}         - "Today" or "Last N Days"
      {{REPORT_DATE}}        - Current date/time
      {{USER_EMAIL}}         - Your Atlassian user email
      {{JIRA_ISSUES}}        - Issues you updated recently
      {{JIRA_ISSUES_NEXT}}   - Top 3 assigned issues by priority & age
      {{CONFLUENCE_PAGES}}   - Pages you updated recently
      {{JIRA_COUNT}}         - Count of updated issues
      {{CONFLUENCE_COUNT}}   - Count of updated pages

UTILITY COMMANDS:
  jira_helper_cmd
    Display this help message

  jira_helper_info
    Show working directory, cache location, and directory structure
    Example: jira_helper_info

  self_update [ref]
    Update jira-helper.sh from GitHub repository
    Repository: ${JIRA_HELPER_GITHUB_REPO}
    Default ref: ${JIRA_HELPER_GITHUB_REF} (tag)
    Path: ${JIRA_HELPER_GITHUB_PATH}
    Example: self_update              # Use default tag
    Example: self_update v1.0.0       # Specific tag
    Example: self_update main         # Specific branch

WORKSPACE MANAGEMENT:
  list_workspaces
    List all registered jira-helper workspaces
    Example: list_workspaces
    Note: Workspaces are auto-registered when you source jira-helper.sh

  discover_workspaces [search-path] [max-depth]
    Scan filesystem and register all jira-helper installations
    Default: searches $HOME with max depth of 5
    Example: discover_workspaces
    Example: discover_workspaces /Users/you/projects 3
    Looks for directories with both .atlassian-cache/ and markdowns/

  workspace_stats
    Show statistics across all workspaces (cache sizes, file counts)
    Example: workspace_stats

  cleanup_workspaces
    Remove stale workspace entries (directories that no longer exist)
    Example: cleanup_workspaces

  Registry location: ~/.jira-helper/registry.json

QUIET MODE:
  Set JIRA_HELPER_QUIET=true to suppress informational messages
  Example: JIRA_HELPER_QUIET=true source jira-helper.sh
  Example: export JIRA_HELPER_QUIET=true  # For entire session

CACHE DIRECTORY:
  Location: $CACHE_DIR

REQUIRED CREDENTIALS:
  Loaded from: ~/.jira
  Variables: ATLASSIAN_USER, ATLASSIAN_API_TOKEN, ATLASSIAN_DOCS, ATLASSIAN_SITE_URL

NOTES:
  - All commands assume the script has been sourced: source jira-helper.sh
  - Jira caches are invalidated when the issue's 'updated' timestamp changes
  - Confluence caches are invalidated when the page version number increases
  - Use get_* functions for automatic cache management
  - Use fetch_* functions to force refresh regardless of cache status
EOF
}

# Display working directory and cache information
# Usage: jira_helper_info
jira_helper_info() {
  local script_path="${BASH_SOURCE[0]}"
  local script_dir="$(cd "$(dirname "${script_path}")" && pwd)"
  local parent_dir="$(dirname "${script_dir}")"

  echo "=============================================="
  echo "Jira Helper - Working Directory Information"
  echo "=============================================="
  echo ""

  # Show script location
  echo "Script Location:"
  echo "  ${script_path}"
  echo ""

  # Show cache directory
  echo "Cache Directory:"
  echo "  ${CACHE_DIR}"

  # Count cache files if directory exists
  if [ -d "${CACHE_DIR}" ]; then
    local jira_cache_count=$(find "${CACHE_DIR}" -name "jira-*.json" -type f 2>/dev/null | wc -l | tr -d ' ')
    local confluence_cache_count=$(find "${CACHE_DIR}" -name "confluence-*.json" -type f 2>/dev/null | wc -l | tr -d ' ')
    local total_cache_count=$(find "${CACHE_DIR}" -type f 2>/dev/null | wc -l | tr -d ' ')

    echo "  Status: ✓ exists"
    echo "  Files: ${total_cache_count} total (${jira_cache_count} Jira, ${confluence_cache_count} Confluence)"

    # Show cache size
    local cache_size=$(du -sh "${CACHE_DIR}" 2>/dev/null | awk '{print $1}')
    echo "  Size: ${cache_size}"
  else
    echo "  Status: ✗ does not exist"
  fi
  echo ""

  # Check for markdowns directory
  echo "Documentation:"
  local markdowns_dir="${parent_dir}/markdowns"
  if [ -d "${markdowns_dir}" ]; then
    local markdown_count=$(find "${markdowns_dir}" -name "*.md" -type f 2>/dev/null | wc -l | tr -d ' ')
    echo "  Markdowns: ${markdowns_dir}"
    echo "  Status: ✓ exists (${markdown_count} files)"

    # List markdown files
    if [ "${markdown_count}" -gt 0 ]; then
      echo "  Files:"
      find "${markdowns_dir}" -name "*.md" -type f 2>/dev/null | sort | while read -r file; do
        echo "    - $(basename "$file")"
      done
    fi
  else
    echo "  Markdowns: ${markdowns_dir}"
    echo "  Status: ✗ does not exist"
  fi
  echo ""

  # Check for README
  local readme_path="${parent_dir}/README.md"
  if [ -f "${readme_path}" ]; then
    echo "README: ${readme_path} ✓"
  else
    echo "README: ${readme_path} ✗"
  fi
  echo ""

  # Show directory structure
  echo "Directory Structure:"
  if command -v tree >/dev/null 2>&1; then
    tree -L 2 -I '.git' "${parent_dir}" 2>/dev/null | head -30
  else
    echo "  ${parent_dir}/"
    [ -f "${parent_dir}/jira-helper.sh" ] && echo "  ├── jira-helper.sh"
    [ -f "${parent_dir}/README.md" ] && echo "  ├── README.md"
    [ -f "${parent_dir}/install.sh" ] && echo "  ├── install.sh"
    [ -f "${parent_dir}/setup-credentials.sh" ] && echo "  ├── setup-credentials.sh"
    [ -d "${markdowns_dir}" ] && echo "  ├── markdowns/"
    [ -d "${CACHE_DIR}" ] && echo "  └── .atlassian-cache/"
  fi
  echo ""

  # Show user info
  echo "User Configuration:"
  if [ -n "${ATLASSIAN_USER}" ]; then
    echo "  User: ${ATLASSIAN_USER} ✓"
  else
    echo "  User: Not configured ✗"
  fi

  if [ -n "${ATLASSIAN_SITE_URL}" ]; then
    echo "  Site: ${ATLASSIAN_SITE_URL} ✓"
  else
    echo "  Site: Not configured ✗"
  fi
  echo ""

  echo "=============================================="
}

# Show version information
jira_helper_version() {
  local script_path="${BASH_SOURCE[0]}"
  local script_dir="$(cd "$(dirname "${script_path}")" && pwd)"
  local version_file="${script_dir}/VERSION"

  echo "jira-helper version info:"
  echo ""

  # Try to read VERSION file first
  if [ -f "$version_file" ]; then
    # shellcheck source=/dev/null
    source "$version_file" 2>/dev/null || true

    echo "Version:     ${JIRA_HELPER_VERSION:-unknown}"
    echo "Commit:      ${JIRA_HELPER_COMMIT:-unknown}"
    echo "Branch:      ${JIRA_HELPER_BRANCH:-unknown}"
    echo "Installed:   ${JIRA_HELPER_INSTALL_DATE:-unknown}"
    echo "Install dir: ${script_dir}"

    # If git root is accessible, show live git status
    if [ -n "$JIRA_HELPER_GIT_ROOT" ] && [ -d "${JIRA_HELPER_GIT_ROOT}/.git" ]; then
      echo ""
      echo "Source repository status:"
      echo "Git root:    ${JIRA_HELPER_GIT_ROOT}"
      echo "Source dir:  ${JIRA_HELPER_SOURCE_DIR}"
      echo "Live commit: $(git -C "$JIRA_HELPER_GIT_ROOT" rev-parse --short HEAD 2>/dev/null || echo 'unknown')"
      echo "Live branch: $(git -C "$JIRA_HELPER_GIT_ROOT" branch --show-current 2>/dev/null || echo 'detached')"
    fi
  else
    # Fallback: try to find git repo
    local git_root=$(cd "$script_dir" && git rev-parse --show-toplevel 2>/dev/null || echo "")

    if [ -n "$git_root" ] && [ -d "${git_root}/.git" ]; then
      echo "Version:     $(git -C "$git_root" describe --tags --abbrev=0 2>/dev/null || echo 'unknown')"
      echo "Commit:      $(git -C "$git_root" rev-parse --short HEAD 2>/dev/null || echo 'unknown')"
      echo "Branch:      $(git -C "$git_root" branch --show-current 2>/dev/null || echo 'detached')"
      echo "Git root:    ${git_root}"
      echo "Install dir: ${script_dir}"
    else
      echo "⚠ VERSION file not found and not a git repository"
      echo "Install dir: ${script_dir}"
      echo ""
      echo "Run install.sh again to capture version metadata"
    fi
  fi
}

# Update jira-helper.sh from GitHub repository
# Usage: self_update [--yes|--force] [ref]
# ref can be a tag (v1.0.0), branch (main), or commit hash
# --yes/--force: Skip confirmation and auto-select latest version
self_update() {
  local force_flag=false
  local ref=""

  # Parse arguments
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --yes|--force)
        force_flag=true
        shift
        ;;
      v*)
        ref="$1"
        shift
        ;;
      *)
        ref="$1"
        shift
        ;;
    esac
  done

  local script_path="${BASH_SOURCE[0]}"
  local script_dir="$(cd "$(dirname "${script_path}")" && pwd)"
  local script_file="$(basename "${script_path}")"
  local backup_file="${script_dir}/${script_file}.backup"

  # Fetch available tags from GitHub API
  echo "Fetching available versions..."
  local tags_json=$(curl -fsSL "https://api.github.com/repos/${JIRA_HELPER_GITHUB_REPO}/tags" 2>/dev/null)

  if [ $? -ne 0 ] || [ -z "$tags_json" ]; then
    echo "Error: Failed to fetch available versions from GitHub"
    return 1
  fi

  # Extract tag names (filter for jira-helper versions)
  local available_tags=$(echo "$tags_json" | "$GREP" -oP '"name":\s*"\K[^"]+' | "$GREP" '^v[0-9]')

  if [ -z "$available_tags" ]; then
    echo "Error: No valid version tags found"
    return 1
  fi

  # Get latest tag if no ref specified or if force flag is set
  if [ -z "$ref" ] || [ "$force_flag" = true ]; then
    ref=$(echo "$available_tags" | head -1)
    echo ""
    echo "Available versions:"
    echo "$available_tags" | while read -r tag; do
      if [ "$tag" = "v${JIRA_HELPER_VERSION}" ]; then
        echo "  $tag (current)"
      elif [ "$tag" = "$ref" ]; then
        echo "  $tag (latest) <--"
      else
        echo "  $tag"
      fi
    done
    echo ""

    if [ "$force_flag" = true ]; then
      echo "Auto-selecting latest version: $ref"
    fi
  fi

  # Construct GitHub raw URL
  local github_url="https://raw.githubusercontent.com/${JIRA_HELPER_GITHUB_REPO}/${ref}/${JIRA_HELPER_GITHUB_PATH}"

  echo ""
  echo "=============================================="
  echo "Self Update - Jira Helper"
  echo "=============================================="
  echo ""
  echo "Source:"
  echo "  Repository: ${JIRA_HELPER_GITHUB_REPO}"
  echo "  Ref: ${ref}"
  echo "  Current: v${JIRA_HELPER_VERSION}"
  echo "  Path: ${JIRA_HELPER_GITHUB_PATH}"
  echo ""
  echo "Target:"
  echo "  File: ${script_dir}/${script_file}"
  echo ""
  echo "This will:"
  echo "  1. Create a backup of your current version"
  echo "  2. Download the selected version from GitHub"
  echo "  3. Validate and install the new version"
  echo "  4. Keep the backup for rollback if needed"
  echo ""

  # Prompt for confirmation unless force flag is set
  if [ "$force_flag" = false ]; then
    read -p "Continue with update? (y/N): " -n 1 -r
    echo ""
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
      echo "Update cancelled."
      return 0
    fi
    echo ""
  fi

  # Create backup
  echo "Creating backup..."
  if cp "${script_dir}/${script_file}" "${backup_file}"; then
    echo "  ✓ Backup created: ${backup_file}"
  else
    echo "  ✗ Failed to create backup"
    return 1
  fi
  echo ""

  # Download new version
  echo "Downloading latest version..."
  local temp_file="${script_dir}/${script_file}.tmp"

  if curl -fsSL "${github_url}" -o "${temp_file}"; then
    echo "  ✓ Download successful"
  else
    echo "  ✗ Download failed"
    echo ""
    echo "Restoring backup..."
    mv "${backup_file}" "${script_dir}/${script_file}"
    return 1
  fi
  echo ""

  # Validate downloaded file (check if it's a bash script)
  if head -1 "${temp_file}" | grep -q "^#!/bin/bash"; then
    echo "Validating downloaded file..."
    echo "  ✓ File appears to be a valid bash script"
  else
    echo "  ✗ Downloaded file doesn't appear to be a bash script"
    echo ""
    echo "Restoring backup..."
    rm -f "${temp_file}"
    mv "${backup_file}" "${script_dir}/${script_file}"
    return 1
  fi
  echo ""

  # Install new version
  echo "Installing new version..."
  if mv "${temp_file}" "${script_dir}/${script_file}"; then
    echo "  ✓ Installation successful"
  else
    echo "  ✗ Installation failed"
    echo ""
    echo "Restoring backup..."
    mv "${backup_file}" "${script_dir}/${script_file}"
    return 1
  fi
  echo ""

  # Show version comparison
  echo "Update Summary:"
  local old_size=$(wc -l < "${backup_file}" | tr -d ' ')
  local new_size=$(wc -l < "${script_dir}/${script_file}" | tr -d ' ')
  echo "  Old version: ${old_size} lines"
  echo "  New version: ${new_size} lines"
  echo "  Difference: $((new_size - old_size)) lines"
  echo ""

  echo "Backup kept at: ${backup_file}"
  echo ""
  echo "=============================================="
  echo "✓ Update complete!"
  echo "=============================================="
  echo ""
  echo "To apply changes, reload the script:"
  echo "  source ${script_dir}/${script_file}"
  echo ""
}

# ============================================================================
# Workspace Registry Functions
# ============================================================================

# Register current workspace in central registry
# Usage: _register_workspace
_register_workspace() {
  local script_path="${BASH_SOURCE[0]}"
  local script_dir="$(cd "$(dirname "${script_path}")" && pwd)"
  local parent_dir="$(dirname "${script_dir}")"

  # Skip registration for test directories
  if echo "${parent_dir}" | grep -qE '(jira-helper-tests|/tmp/|testHelper)'; then
    return 0
  fi

  local workspace_id="$(echo "${parent_dir}" | md5sum 2>/dev/null | cut -d' ' -f1 || echo "${parent_dir}" | md5 | cut -d' ' -f1)"

  # Create registry directory if it doesn't exist
  mkdir -p "$(dirname "${JIRA_HELPER_REGISTRY}")"

  # Initialize registry if it doesn't exist
  if [ ! -f "${JIRA_HELPER_REGISTRY}" ]; then
    echo '{"workspaces": {}}' > "${JIRA_HELPER_REGISTRY}"
  fi

  # Add or update this workspace
  local temp_file="${JIRA_HELPER_REGISTRY}.tmp"
  jq --arg id "${workspace_id}" \
     --arg path "${parent_dir}" \
     --arg cache "${CACHE_DIR}" \
     --arg script "${script_dir}/$(basename "${script_path}")" \
     --arg timestamp "$(date -u +"%Y-%m-%dT%H:%M:%SZ")" \
     '.workspaces[$id] = {
       path: $path,
       cache_dir: $cache,
       script_path: $script,
       last_used: $timestamp
     }' "${JIRA_HELPER_REGISTRY}" > "${temp_file}" && mv "${temp_file}" "${JIRA_HELPER_REGISTRY}"
}

# List all registered workspaces
# Usage: list_workspaces
list_workspaces() {
  if [ ! -f "${JIRA_HELPER_REGISTRY}" ]; then
    echo "No workspaces registered yet."
    echo ""
    echo "Workspaces are automatically registered when you source jira-helper.sh"
    return 0
  fi

  echo "=============================================="
  echo "Registered Jira-Helper Workspaces"
  echo "=============================================="
  echo ""

  local count=$(jq '.workspaces | length' "${JIRA_HELPER_REGISTRY}")
  echo "Total workspaces: ${count}"
  echo ""

  if [ "$count" -eq 0 ]; then
    echo "No workspaces registered yet."
    return 0
  fi

  # Display workspaces sorted by last_used
  jq -r '.workspaces | to_entries | sort_by(.value.last_used) | reverse | .[] |
    "Path: \(.value.path)\n" +
    "  Cache: \(.value.cache_dir)\n" +
    "  Script: \(.value.script_path)\n" +
    "  Last used: \(.value.last_used)\n"' "${JIRA_HELPER_REGISTRY}"

  echo "=============================================="
  echo ""
  echo "Current workspace: $(dirname "$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)")"
}

# Clean up stale workspace entries (paths that no longer exist)
# Usage: cleanup_workspaces
cleanup_workspaces() {
  if [ ! -f "${JIRA_HELPER_REGISTRY}" ]; then
    echo "No registry file found."
    return 0
  fi

  echo "Cleaning up stale workspace entries..."
  echo ""

  local temp_file="${JIRA_HELPER_REGISTRY}.tmp"
  local removed=0

  # Read all workspace paths and check if they exist
  jq -r '.workspaces | to_entries | .[] | "\(.key)|\(.value.path)"' "${JIRA_HELPER_REGISTRY}" | while IFS='|' read -r id path; do
    if [ ! -d "$path" ]; then
      echo "  Removing: $path (no longer exists)"
      removed=$((removed + 1))
    fi
  done

  # Remove non-existent workspaces
  jq '.workspaces | to_entries | map(select(.value.path | . as $p | $p != "" and ($p | test("^/")) and ($p | split("/") | length > 0))) | from_entries | {workspaces: .}' "${JIRA_HELPER_REGISTRY}" > "${temp_file}"

  # Actually filter by checking filesystem
  local workspace_ids=$(jq -r '.workspaces | to_entries | .[] | "\(.key)|\(.value.path)"' "${JIRA_HELPER_REGISTRY}")

  # Create new registry with only existing paths
  echo '{"workspaces": {}}' > "${temp_file}"

  echo "$workspace_ids" | while IFS='|' read -r id path; do
    if [ -d "$path" ]; then
      # Keep this workspace
      local workspace_data=$(jq --arg id "$id" '.workspaces[$id]' "${JIRA_HELPER_REGISTRY}")
      jq --arg id "$id" --argjson data "$workspace_data" '.workspaces[$id] = $data' "${temp_file}" > "${temp_file}.2" && mv "${temp_file}.2" "${temp_file}"
    else
      echo "  Removed: $path"
      removed=$((removed + 1))
    fi
  done

  mv "${temp_file}" "${JIRA_HELPER_REGISTRY}"

  echo ""
  if [ "$removed" -gt 0 ]; then
    echo "✓ Removed ${removed} stale workspace(s)"
  else
    echo "✓ No stale workspaces found"
  fi
}

# Show workspace statistics
# Usage: workspace_stats
workspace_stats() {
  if [ ! -f "${JIRA_HELPER_REGISTRY}" ]; then
    echo "No workspaces registered yet."
    return 0
  fi

  echo "=============================================="
  echo "Workspace Statistics"
  echo "=============================================="
  echo ""

  local total=$(jq '.workspaces | length' "${JIRA_HELPER_REGISTRY}")
  echo "Total workspaces: ${total}"
  echo ""

  if [ "$total" -eq 0 ]; then
    return 0
  fi

  # Count total cache files across all workspaces
  local total_cache_files=0
  local total_cache_size=0

  jq -r '.workspaces[].cache_dir' "${JIRA_HELPER_REGISTRY}" | while read -r cache_dir; do
    if [ -d "$cache_dir" ]; then
      local file_count=$(find "$cache_dir" -type f 2>/dev/null | wc -l | tr -d ' ')
      total_cache_files=$((total_cache_files + file_count))
    fi
  done

  echo "Total cache files across all workspaces: ${total_cache_files}"
  echo ""

  echo "Workspaces by size:"
  jq -r '.workspaces | to_entries[] | "\(.value.cache_dir)|\(.value.path)"' "${JIRA_HELPER_REGISTRY}" | while IFS='|' read -r cache_dir workspace_path; do
    if [ -d "$cache_dir" ]; then
      local size=$(du -sh "$cache_dir" 2>/dev/null | awk '{print $1}')
      local count=$(find "$cache_dir" -type f 2>/dev/null | wc -l | tr -d ' ')
      echo "  $workspace_path"
      echo "    Cache: $size ($count files)"
    fi
  done

  echo ""
  echo "=============================================="
}

# Discover and register all jira-helper workspaces
# Usage: discover_workspaces [search-path]
discover_workspaces() {
  local search_path="${1:-${HOME}}"
  local max_depth="${2:-5}"

  echo "=============================================="
  echo "Discovering Jira-Helper Workspaces"
  echo "=============================================="
  echo ""
  echo "Search path: ${search_path}"
  echo "Max depth: ${max_depth}"
  echo ""
  echo "Looking for directories with both:"
  echo "  - .atlassian-cache/"
  echo "  - markdowns/"
  echo ""

  # Create registry if it doesn't exist
  mkdir -p "$(dirname "${JIRA_HELPER_REGISTRY}")"
  if [ ! -f "${JIRA_HELPER_REGISTRY}" ]; then
    echo '{"workspaces": {}}' > "${JIRA_HELPER_REGISTRY}"
  fi

  local found=0
  local registered=0
  local skipped=0

  echo "Scanning (this may take a moment)..."
  echo ""

  # Find all .atlassian-cache directories
  # Use process substitution to avoid subshell issues
  while IFS= read -r cache_dir; do
    # Get parent directory
    local workspace_path="$(dirname "$cache_dir")"

    # Check if markdowns directory exists
    if [ -d "${workspace_path}/markdowns" ]; then
      found=$((found + 1))

      # Check if jira-helper.sh exists
      local script_path=""
      if [ -f "${cache_dir}/jira-helper.sh" ]; then
        script_path="${cache_dir}/jira-helper.sh"
      fi

      # Calculate workspace ID
      local workspace_id="$(echo "${workspace_path}" | md5sum 2>/dev/null | cut -d' ' -f1 || echo "${workspace_path}" | md5 | cut -d' ' -f1)"

      # Check if already registered
      local already_exists=$(jq --arg id "${workspace_id}" '.workspaces | has($id)' "${JIRA_HELPER_REGISTRY}" 2>/dev/null)

      if [ "$already_exists" = "true" ]; then
        echo "  Found (already registered): ${workspace_path}"
        skipped=$((skipped + 1))
      else
        echo "  Found (registering): ${workspace_path}"

        # Register this workspace
        local temp_file="${JIRA_HELPER_REGISTRY}.tmp"
        jq --arg id "${workspace_id}" \
           --arg path "${workspace_path}" \
           --arg cache "${cache_dir}" \
           --arg script "${script_path}" \
           --arg timestamp "$(date -u +"%Y-%m-%dT%H:%M:%SZ")" \
           '.workspaces[$id] = {
             path: $path,
             cache_dir: $cache,
             script_path: $script,
             last_used: $timestamp,
             discovered: true
           }' "${JIRA_HELPER_REGISTRY}" > "${temp_file}" && mv "${temp_file}" "${JIRA_HELPER_REGISTRY}"

        registered=$((registered + 1))
      fi
    fi
  done < <(find "${search_path}" -maxdepth ${max_depth} -type d -name ".atlassian-cache" 2>/dev/null)

  echo ""
  echo "=============================================="
  echo "Discovery Complete"
  echo "=============================================="
  echo ""
  echo "Total found: ${found}"
  echo "  Already registered: ${skipped}"
  echo "  Newly registered: ${registered}"
  echo ""

  if [ "$registered" -gt 0 ]; then
    echo "✓ Registered ${registered} new workspace(s)"
    echo ""
    echo "View all workspaces: jira-helper workspaces"
  elif [ "$found" -eq 0 ]; then
    echo "No jira-helper workspaces found in ${search_path}"
    echo ""
    echo "A valid workspace needs both:"
    echo "  - .atlassian-cache/ directory"
    echo "  - markdowns/ directory"
  else
    echo "All found workspaces were already registered"
  fi
  echo ""
}

# Automatically register workspace on source
_register_workspace 2>/dev/null

# Parse Jira ticket key from URL or key string
# Usage: parse_jira_key "https://site.atlassian.net/browse/PANK-1485" -> "PANK-1485"
# Usage: parse_jira_key "PANK-1485" -> "PANK-1485"
# Usage: parse_jira_key "PANK-1485:hint" -> "PANK-1485" (strips completion hints)
parse_jira_key() {
  local input="$1"

  # Strip completion hints (everything after colon)
  input="${input%%:*}"

  # Extract from URL pattern: /browse/KEY or /KEY at end
  if [[ "$input" =~ /browse/([A-Z]+-[0-9]+) ]]; then
    echo "${BASH_REMATCH[1]}"
  elif [[ "$input" =~ ([A-Z]+-[0-9]+)$ ]]; then
    echo "${BASH_REMATCH[1]}"
  else
    # Already a plain key or invalid
    echo "$input"
  fi
}

# Open Jira issue in default browser
# Usage: open_jira_issue PANK-1485
open_jira_issue() {
  local issue_key=$(parse_jira_key "$1")

  # Use safe credentials loading
  _source_credentials || return 1

  local url="https://${ATLASSIAN_SITE_URL}/browse/${issue_key}"

  _log "Opening ${issue_key} in browser..."

  # Cross-platform open command
  if command -v open >/dev/null 2>&1; then
    # macOS
    open "$url"
  elif command -v xdg-open >/dev/null 2>&1; then
    # Linux
    xdg-open "$url"
  elif command -v wslview >/dev/null 2>&1; then
    # WSL
    wslview "$url"
  else
    echo "URL: $url"
    echo "Could not detect browser command. Please open manually."
    return 1
  fi
}

# Open Confluence page in default browser
# Usage: open_confluence_page 4214128641
open_confluence_page() {
  local page_id="$1"

  # Use safe credentials loading
  _source_credentials || return 1

  local url="https://${ATLASSIAN_SITE_URL}/wiki/spaces/viewpage.action?pageId=${page_id}"

  _log "Opening page ${page_id} in browser..."

  # Cross-platform open command
  if command -v open >/dev/null 2>&1; then
    # macOS
    open "$url"
  elif command -v xdg-open >/dev/null 2>&1; then
    # Linux
    xdg-open "$url"
  elif command -v wslview >/dev/null 2>&1; then
    # WSL
    wslview "$url"
  else
    echo "URL: $url"
    echo "Could not detect browser command. Please open manually."
    return 1
  fi
}

# Check if Jira issue cache needs refresh
# Usage: needs_jira_refresh PANK-1334
needs_jira_refresh() {
  local issue_key="$1"
  local cache_file="${CACHE_DIR}/jira-${issue_key}.json"
  local timestamp_file="${CACHE_DIR}/jira-${issue_key}.timestamp"

  # No cache exists
  if [ ! -f "$cache_file" ]; then
    return 0  # needs refresh
  fi

  # Get cached timestamp
  local cached_time
  cached_time=$(cat "$timestamp_file" 2>/dev/null || echo "1970-01-01T00:00:00.000Z")

  # Get remote updated time (lightweight query)
  _source_credentials || return 1
  local remote_time
  remote_time=$(curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
    -H 'Accept: application/json' \
    "https://${ATLASSIAN_SITE_URL}/rest/api/3/issue/${issue_key}?fields=updated" | \
    jq -r '.fields.updated')

  if [ "$remote_time" != "$cached_time" ]; then
    return 0  # needs refresh
  else
    return 1  # cache is current
  fi
}

# Fetch and cache Jira issue
# Usage: fetch_jira_issue PANK-1334
fetch_jira_issue() {
  local issue_key="$1"
  local cache_file="${CACHE_DIR}/jira-${issue_key}.json"
  local timestamp_file="${CACHE_DIR}/jira-${issue_key}.timestamp"

  _source_credentials || return 1
  curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
    -H 'Accept: application/json' \
    "https://${ATLASSIAN_SITE_URL}/rest/api/3/issue/${issue_key}?expand=comment,changelog" \
    -o "$cache_file"

  # Save timestamp
  jq -r '.fields.updated' "$cache_file" > "$timestamp_file"

  _log "Cached: $cache_file"
}

# Internal: Get Jira issue JSON (from cache or API)
# Usage: _get_jira_issue_json PANK-1334
_get_jira_issue_json() {
  local input="$1"
  local issue_key
  issue_key=$(parse_jira_key "$input")
  local cache_file="${CACHE_DIR}/jira-${issue_key}.json"

  if needs_jira_refresh "$issue_key"; then
    _log "Refreshing cache for $issue_key..."
    fetch_jira_issue "$issue_key"
  else
    _log "Using cached data for $issue_key"
  fi

  cat "$cache_file"
}

# Check if current user is related to a ticket (assignee, reporter, or watcher)
# Usage: _check_user_relationship PANK-1234
# Returns: 0 if related, 1 if unrelated, 2 if error
_check_user_relationship() {
  local issue_key="$1"

  # Get the issue data
  local issue_json
  issue_json=$(_get_jira_issue_json "$issue_key" 2>/dev/null)

  if [ -z "$issue_json" ]; then
    echo "Error: Could not fetch issue $issue_key" >&2
    return 2
  fi

  # Use safe credentials loading
  _source_credentials || return 1

  # Check if user is assignee
  local assignee_email
  assignee_email=$(echo "$issue_json" | jq -r '.fields.assignee.emailAddress // empty')
  if [ "$assignee_email" = "$ATLASSIAN_USER" ]; then
    return 0
  fi

  # Check if user is reporter
  local reporter_email
  reporter_email=$(echo "$issue_json" | jq -r '.fields.reporter.emailAddress // empty')
  if [ "$reporter_email" = "$ATLASSIAN_USER" ]; then
    return 0
  fi

  # Check if user is watcher
  local is_watcher
  is_watcher=$(echo "$issue_json" | jq -r --arg user "$ATLASSIAN_USER" '.fields.watches.watchers[]?.emailAddress // empty | select(. == $user)')
  if [ -n "$is_watcher" ]; then
    return 0
  fi

  return 1
}

# Confirm action on unrelated ticket, unless --yes or --force flag is present
# Usage: _confirm_unrelated_action PANK-1234 "$@"
# Returns: 0 to proceed, 1 to abort
_confirm_unrelated_action() {
  local issue_key="$1"
  shift

  # Check for bypass flags
  for arg in "$@"; do
    if [ "$arg" = "--yes" ] || [ "$arg" = "--force" ]; then
      return 0
    fi
  done

  # Check relationship
  _check_user_relationship "$issue_key"
  local relationship_status=$?

  # If error checking relationship, abort
  if [ $relationship_status -eq 2 ]; then
    return 1
  fi

  # If related, proceed
  if [ $relationship_status -eq 0 ]; then
    return 0
  fi

  # User is unrelated - prompt for confirmation
  echo "Warning: You're not related to $issue_key (not assignee, reporter, or watcher)." >&2
  echo -n "Continue? (y/n): " >&2
  read -r response

  case "$response" in
    [yY]|[yY][eE][sS])
      return 0
      ;;
    *)
      echo "Aborted." >&2
      return 1
      ;;
  esac
}

# Get Jira issue - formatted by default, JSON with --json flag
# Usage: get_jira_issue PANK-1334
# Usage: get_jira_issue https://site.atlassian.net/browse/PANK-1334
# Usage: get_jira_issue PANK-1334 --json
get_jira_issue() {
  local input="$1"
  local json_flag="$2"

  if [ -z "$input" ]; then
    echo "Error: Usage: get_jira_issue <issue-key or URL> [--json]" >&2
    return 1
  fi

  local issue_key
  issue_key=$(parse_jira_key "$input")

  # Get the issue JSON
  local issue_json
  issue_json=$(_get_jira_issue_json "$issue_key" 2>/dev/null)

  if [ -z "$issue_json" ]; then
    echo "Error: Could not fetch issue $issue_key" >&2
    return 1
  fi

  # Output format based on flag
  if [ "$json_flag" = "--json" ]; then
    echo "$issue_json"
  else
    # Format as human-readable text
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo "$issue_json" | jq -r '"Key:        \(.key)"'
    echo "$issue_json" | jq -r '"Summary:    \(.fields.summary)"'
    echo "$issue_json" | jq -r '"Status:     \(.fields.status.name)"'
    echo "$issue_json" | jq -r '"Priority:   \(.fields.priority.name // "None")"'
    echo "$issue_json" | jq -r '"Assignee:   \(.fields.assignee.displayName // "Unassigned")"'
    echo "$issue_json" | jq -r '"Reporter:   \(.fields.reporter.displayName)"'
    echo "$issue_json" | jq -r '"Created:    \(.fields.created)"'
    echo "$issue_json" | jq -r '"Updated:    \(.fields.updated)"'
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo ""
    echo "Description:"
    echo "$issue_json" | jq -r '.fields.description.content[]?.content[]?.text // "No description"' 2>/dev/null || echo "No description"
    echo ""

    # Show comments if available
    local comment_count=$(echo "$issue_json" | jq -r '.fields.comment.total // 0')
    if [ "$comment_count" -gt 0 ]; then
      echo "Comments ($comment_count):"
      echo "$issue_json" | jq -r '.fields.comment.comments[]? | "  [\(.created)] \(.author.displayName):\n  \(.body.content[]?.content[]?.text // .body)\n"' 2>/dev/null
      echo ""
      echo "Add comment: jira-helper issue comment $issue_key \"Your comment text\""
    else
      echo "No comments yet."
      echo "Add comment: jira-helper issue comment $issue_key \"Your comment text\""
    fi
    echo ""
    echo "URL: https://${ATLASSIAN_SITE_URL}/browse/${issue_key}"
  fi
}

# Show Jira issue in formatted text (alias for get_jira_issue without --json)
# Usage: show_jira_issue PANK-1334
# Usage: show_jira_issue https://site.atlassian.net/browse/PANK-1334
show_jira_issue() {
  get_jira_issue "$@"
}

# Check if Confluence page cache needs refresh
# Usage: needs_confluence_refresh 4214128641
needs_confluence_refresh() {
  local page_id="$1"
  local cache_file="${CACHE_DIR}/confluence-${page_id}.json"
  local timestamp_file="${CACHE_DIR}/confluence-${page_id}.timestamp"

  # No cache exists
  if [ ! -f "$cache_file" ]; then
    return 0  # needs refresh
  fi

  # Get cached version number
  local cached_version
  cached_version=$(jq -r '.version.number' "$cache_file" 2>/dev/null || echo 0)

  # Get remote version (lightweight query)
  _source_credentials || return 1
  local remote_version
  remote_version=$(curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_DOCS}" \
    -H 'Accept: application/json' \
    "https://${ATLASSIAN_SITE_URL}/wiki/rest/api/content/${page_id}?expand=version" | \
    jq -r '.version.number')

  if [ "$remote_version" -gt "$cached_version" ]; then
    return 0  # needs refresh
  else
    return 1  # cache is current
  fi
}

# Fetch and cache Confluence page
# Usage: fetch_confluence_page 4214128641
fetch_confluence_page() {
  local page_id="$1"
  local cache_file="${CACHE_DIR}/confluence-${page_id}.json"
  local timestamp_file="${CACHE_DIR}/confluence-${page_id}.timestamp"

  _source_credentials || return 1
  curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_DOCS}" \
    -H 'Accept: application/json' \
    "https://${ATLASSIAN_SITE_URL}/wiki/rest/api/content/${page_id}?expand=body.storage,version" \
    -o "$cache_file"

  # Save timestamp
  jq -r '.version.when' "$cache_file" > "$timestamp_file"

  echo "Cached: $cache_file"
}

# Get Confluence page (from cache or API)
# Usage: get_confluence_page 4214128641
get_confluence_page() {
  local page_id="$1"
  local cache_file="${CACHE_DIR}/confluence-${page_id}.json"

  if needs_confluence_refresh "$page_id"; then
    _log "Refreshing cache for Confluence page $page_id..."
    fetch_confluence_page "$page_id"
  else
    _log "Using cached data for Confluence page $page_id"
  fi

  cat "$cache_file"
}

# Set metadata mapping between Confluence page and source file
# Usage: set_confluence_source 4225433659 /Users/jimsander/repos/jds-sandbox1/PANK-1334-ANNOUNCEMENT-WORKING-DETAILED.md
set_confluence_source() {
  local page_id="$1"
  local source_file="$2"

  if [ -z "$page_id" ] || [ -z "$source_file" ]; then
    echo "Error: Usage: set_confluence_source <page-id> <source-file-path>" >&2
    return 1
  fi

  local metadata_file="${CACHE_DIR}/confluence-${page_id}.metadata"

  cat > "$metadata_file" << EOF
{
  "page_id": "$page_id",
  "source_file": "$source_file",
  "last_updated": "$(date -u +"%Y-%m-%dT%H:%M:%S.000Z")"
}
EOF

  echo "Metadata saved: $metadata_file"
}

# Get source file for Confluence page
# Usage: get_confluence_source 4225433659
get_confluence_source() {
  local page_id="$1"

  if [ -z "$page_id" ]; then
    echo "Error: Usage: get_confluence_source <page-id>" >&2
    return 1
  fi

  local metadata_file="${CACHE_DIR}/confluence-${page_id}.metadata"

  if [ -f "$metadata_file" ]; then
    jq -r '.source_file' "$metadata_file"
  else
    echo ""
  fi
}

# Check if query cache is fresh (within TTL)
# Usage: is_cache_fresh cache_file.json 3600  (3600 seconds = 1 hour)
is_cache_fresh() {
  local cache_file="$1"
  local ttl_seconds="${2:-3600}"  # Default 1 hour

  # No cache exists
  if [ ! -f "$cache_file" ]; then
    return 1  # not fresh
  fi

  # Check file age
  local now
  now=$(date +%s)
  local file_time
  file_time=$(stat -f %m "$cache_file" 2>/dev/null || stat -c %Y "$cache_file" 2>/dev/null || echo "0")

  # If we couldn't get file time, treat as stale
  if [ -z "$file_time" ] || [ "$file_time" = "0" ]; then
    return 1  # stale
  fi

  local age=$((now - file_time))

  if [ "$age" -lt "$ttl_seconds" ]; then
    return 0  # fresh
  else
    return 1  # stale
  fi
}

# List all Confluence pages with source file mappings
# Usage: list_confluence_sources
list_confluence_sources() {
  echo "Confluence Page ID | Source File"
  echo "------------------|-------------"

  # Check if any metadata files exist
  local has_files=false
  for metadata_file in "${CACHE_DIR}"/confluence-*.metadata; do
    if [ -f "$metadata_file" ]; then
      local page_id=$(jq -r '.page_id' "$metadata_file")
      local source_file=$(jq -r '.source_file' "$metadata_file")

      # Skip empty entries
      if [ -n "$page_id" ] && [ "$page_id" != "null" ]; then
        has_files=true
        echo "$page_id | $source_file"
      fi
    fi
  done

  if [ "$has_files" = false ]; then
    echo "(No source mappings found)"
  fi
}

# Search for Jira issues updated by you in the last N days OR by text query
# Usage: search_my_jira_updates [days|"search query"]
#   - Numeric argument: search by days (e.g., 7 = last 7 days)
#   - Text argument: search in summary, description, and comments
search_my_jira_updates() {
  local query="${1:-5}"
  local days
  local jql
  local cache_file
  local max_results=50  # Default for time-based searches

  # Use safe credentials loading
  _source_credentials || return 1

  # Check if argument is numeric (days) or text (search query)
  if [[ "$query" =~ ^[0-9]+$ ]]; then
    # Numeric: treat as days
    days="$query"
    local date_from
    date_from=$(date -u -v-"${days}"d +"%Y-%m-%d" 2>/dev/null || date -u -d "${days} days ago" +"%Y-%m-%d")
    cache_file="${CACHE_DIR}/jira-my-updates-${days}days.json"

    _log "Searching for Jira issues updated by you in the last ${days} days..."
    _log ""

    # JQL: updatedBy requires explicit username and uses IN operator
    # Note: updatedBy() doesn't support currentUser(), must use ATLASSIAN_USER
    # Include both parents and their subtasks in results
    jql="(issue in updatedBy(\"${ATLASSIAN_USER}\", \"-${days}d\") OR parent in updatedBy(\"${ATLASSIAN_USER}\", \"-${days}d\")) ORDER BY updated DESC"
  else
    # Text: treat as search query
    local search_hash
    search_hash=$(echo -n "$query" | md5sum 2>/dev/null | cut -d' ' -f1 || echo -n "$query" | md5 2>/dev/null)
    cache_file="${CACHE_DIR}/jira-search-${search_hash}.json"
    max_results=200  # Higher limit for text searches to catch older matches

    _log "Searching for Jira issues containing: \"${query}\""
    _log ""

    # JQL: Search in summary, description, and comments for the text
    # AND filter to only issues you're involved with (assignee, reporter, watcher, or updated by you)
    # Include both issues and subtasks
    jql="(summary ~ \"${query}\" OR description ~ \"${query}\" OR comment ~ \"${query}\") AND (assignee = currentUser() OR reporter = currentUser() OR watcher = currentUser() OR issue in updatedBy(\"${ATLASSIAN_USER}\") OR parent in updatedBy(\"${ATLASSIAN_USER}\")) ORDER BY updated DESC"
  fi

  # Check if cache exists and has required fields, if not delete it
  if [ -f "$cache_file" ]; then
    # Check if cache has the new fields (issuetype, subtasks)
    if ! jq -e '.issues[0].fields.issuetype' "$cache_file" >/dev/null 2>&1; then
      _log "Cache missing required fields, refreshing..."
      rm -f "$cache_file"
    else
      # Check cache age - refresh if older than 5 minutes
      local cache_age_seconds
      local file_mtime
      if [ "$(uname)" = "Darwin" ]; then
        # macOS
        file_mtime=$(stat -f %m "$cache_file" 2>/dev/null || echo "0")
      else
        # Linux
        file_mtime=$(stat -c %Y "$cache_file" 2>/dev/null || echo "0")
      fi

      # If we couldn't get file time, refresh cache
      if [ "$file_mtime" = "0" ]; then
        _log "Could not determine cache age, refreshing..."
        rm -f "$cache_file"
      else
        cache_age_seconds=$(( $(date +%s) - file_mtime ))
        if [ "$cache_age_seconds" -gt 300 ]; then
          _log "Cache older than 5 minutes, refreshing..."
          rm -f "$cache_file"
        fi
      fi
    fi
  fi

  # Fetch from API (if cache was deleted or doesn't exist)
  if [ ! -f "$cache_file" ]; then
    curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
      -H 'Accept: application/json' \
      -G \
      --data-urlencode "jql=${jql}" \
      --data-urlencode "fields=key,summary,updated,status,priority,issuetype,parent,issuelinks,subtasks,changelog" \
      --data-urlencode "expand=changelog" \
      --data-urlencode "maxResults=${max_results}" \
      "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
      -o "$cache_file"

    # Cache each individual issue for tab completion
    # Extract each issue and save to individual cache files
    # Write in reverse order so most recent issues have newest timestamps
    jq -c '.issues[] | [.]' "$cache_file" 2>/dev/null | jq -c '.[]' | tac | while read -r issue; do
      local issue_key=$(echo "$issue" | jq -r '.key')
      if [ -n "$issue_key" ] && [ "$issue_key" != "null" ]; then
        local issue_cache="${CACHE_DIR}/jira-${issue_key}.json"
        echo "$issue" > "$issue_cache"
        # Touch file to ensure unique timestamp (files written in API result order)
        touch "$issue_cache"
        # Brief sleep to ensure distinct timestamps for tab completion ordering
        sleep 0.01
      fi
    done
  fi

  # Process and display with enhanced information: type, hierarchy, relationships
  # For text search (no days filter), use large value (365) for changelog filtering
  jq -r --arg days "${days:-365}" '
    # First pass: build parent-child map and collect all issues
    (.issues | map({key: .key, data: .})) as $all_issues |

    # Create lookup map for quick access
    ($all_issues | map({(.key): .data}) | add) as $issue_map |

    # Separate parents and children
    ($all_issues | map(select(.data.fields.parent == null and (.data.fields.subtasks | length) > 0))) as $parents |
    ($all_issues | map(select(.data.fields.parent != null))) as $children |

    # Build output with hierarchy
    (
      # First, output all parent issues
      ($parents | map(.data) | .[]? |
        . as $issue |

        # Get issue type
        .fields.issuetype.name as $type |
        .fields.priority.name as $priority |
        .fields.status.name as $status |

        # Check changelog for priority and status changes
        (if .changelog and .changelog.histories then
          (.changelog.histories |
           map(select(
             .created |
             sub("\\.[0-9]+\\+0000$"; "Z") |
             fromdateiso8601 > (now - ($days | tonumber * 86400))
           )) |
           map(.items[] | select(.field == "status" or .field == "priority")) |
           {
             priority: map(select(.field == "priority"))[0] // null,
             status: map(select(.field == "status"))[0] // null
           }
          )
        else
          {priority: null, status: null}
        end) as $changes |

        # Abbreviation function
        def abbrev:
          gsub(" "; "") |
          gsub("InProgress"; "InProg") |
          gsub("Progress"; "Prog") |
          gsub("Blocked"; "Blkd") |
          gsub("BACKLOG"; "Bklog") |
          gsub("Critical"; "Crit") |
          gsub("Medium"; "Med") |
          gsub("Waiting"; "Wait");

        # Build displays
        ($changes.priority // null) as $prioChange |
        (if $prioChange then
          ($prioChange.fromString | abbrev) + "->" + ($prioChange.toString | abbrev)
        else
          ($priority | abbrev)
        end) as $prioDisplay |

        ($changes.status // null) as $statusChange |
        (if $statusChange then
          ($statusChange.fromString | abbrev) + "->" + ($status | abbrev)
        else
          ($status | abbrev)
        end) as $statusDisplay |

        # Get issue links
        (.fields.issuelinks // [] |
         map(
           if .outwardIssue then
             "\(.type.outward)→\(.outwardIssue.key)"
           elif .inwardIssue then
             "\(.inwardIssue.key)→\(.type.inward)"
           else
             empty
           end
         ) | join(",")) as $links |

        # Output parent with metadata
        "PARENT|\(.key)|\(.fields.updated)|\($type)|[\($prioDisplay)] \($statusDisplay)|\($links)|\(.fields.summary)",

        # Output its subtasks indented (from full data in issue_map or minimal parent data)
        (.fields.subtasks // [] | .[] |
          # Try to get full data from issue_map, fall back to minimal data from parent
          (($issue_map[.key] // .) ) as $subtask_data |
          # For timestamp: use updated if available, otherwise created, otherwise "no-data"
          ($subtask_data.fields.updated // $subtask_data.fields.created // "no-data") as $subtask_updated |
          ($subtask_data.fields.issuetype.name // "Sub-task") as $subtype |
          ($subtask_data.fields.priority.name // "None") as $subprio |
          ($subtask_data.fields.status.name // "Unknown") as $substatus |
          ($subtask_data.fields.summary) as $subtask_summary |
          "CHILD|\(.key)|\($subtask_updated)|\($subtype)|[\($subprio | abbrev)] \($substatus | abbrev)||  ↳ \($subtask_summary)"
        )
      ),

      # Then output standalone issues (no parent, no children)
      ($all_issues | map(select(.data.fields.parent == null and (.data.fields.subtasks | length) == 0)) | .[].data |
        .fields.issuetype.name as $type |
        .fields.priority.name as $priority |
        .fields.status.name as $status |

        # Same changelog logic
        (if .changelog and .changelog.histories then
          (.changelog.histories |
           map(select(
             .created |
             sub("\\.[0-9]+\\+0000$"; "Z") |
             fromdateiso8601 > (now - ($days | tonumber * 86400))
           )) |
           map(.items[] | select(.field == "status" or .field == "priority")) |
           {
             priority: map(select(.field == "priority"))[0] // null,
             status: map(select(.field == "status"))[0] // null
           }
          )
        else
          {priority: null, status: null}
        end) as $changes |

        def abbrev:
          gsub(" "; "") |
          gsub("InProgress"; "InProg") |
          gsub("Progress"; "Prog") |
          gsub("Blocked"; "Blkd") |
          gsub("BACKLOG"; "Bklog") |
          gsub("Critical"; "Crit") |
          gsub("Medium"; "Med") |
          gsub("Waiting"; "Wait");

        ($changes.priority // null) as $prioChange |
        (if $prioChange then
          ($prioChange.fromString | abbrev) + "->" + ($prioChange.toString | abbrev)
        else
          ($priority | abbrev)
        end) as $prioDisplay |

        ($changes.status // null) as $statusChange |
        (if $statusChange then
          ($statusChange.fromString | abbrev) + "->" + ($status | abbrev)
        else
          ($status | abbrev)
        end) as $statusDisplay |

        # Get issue links
        (.fields.issuelinks // [] |
         map(
           if .outwardIssue then
             "\(.type.outward)→\(.outwardIssue.key)"
           elif .inwardIssue then
             "\(.inwardIssue.key)→\(.type.inward)"
           else
             empty
           end
         ) | join(",")) as $links |

        "STANDALONE|\(.key)|\(.fields.updated)|\($type)|[\($prioDisplay)] \($statusDisplay)|\($links)|\(.fields.summary)"
      ),

      # Finally, output orphaned children (children whose parents are not in results)
      ($children | map(select($issue_map[.data.fields.parent.key] == null)) | .[].data |
        .fields.issuetype.name as $type |
        .fields.priority.name as $priority |
        .fields.status.name as $status |

        # Same changelog logic
        (if .changelog and .changelog.histories then
          (.changelog.histories |
           map(select(
             .created |
             sub("\\.[0-9]+\\+0000$"; "Z") |
             fromdateiso8601 > (now - ($days | tonumber * 86400))
           )) |
           map(.items[] | select(.field == "status" or .field == "priority")) |
           {
             priority: map(select(.field == "priority"))[0] // null,
             status: map(select(.field == "status"))[0] // null
           }
          )
        else
          {priority: null, status: null}
        end) as $changes |

        def abbrev:
          gsub(" "; "") |
          gsub("InProgress"; "InProg") |
          gsub("Progress"; "Prog") |
          gsub("Blocked"; "Blkd") |
          gsub("BACKLOG"; "Bklog") |
          gsub("Critical"; "Crit") |
          gsub("Medium"; "Med") |
          gsub("Waiting"; "Wait");

        ($changes.priority // null) as $prioChange |
        (if $prioChange then
          ($prioChange.fromString | abbrev) + "->" + ($prioChange.toString | abbrev)
        else
          ($priority | abbrev)
        end) as $prioDisplay |

        ($changes.status // null) as $statusChange |
        (if $statusChange then
          ($statusChange.fromString | abbrev) + "->" + ($status | abbrev)
        else
          ($status | abbrev)
        end) as $statusDisplay |

        # Get issue links
        (.fields.issuelinks // [] |
         map(
           if .outwardIssue then
             "\(.type.outward)→\(.outwardIssue.key)"
           elif .inwardIssue then
             "\(.inwardIssue.key)→\(.type.inward)"
           else
             empty
           end
         ) | join(",")) as $links |

        # Show parent key in brackets for orphaned children
        "ORPHAN|\(.key)|\(.fields.updated)|\($type)|[\($prioDisplay)] \($statusDisplay)|\($links)|(\(.fields.parent.key)) \(.fields.summary)"
      )
    )
  ' "$cache_file" | while IFS='|' read -r hierarchy key timestamp type status links summary; do
    # Convert UTC timestamp to local time and format as YY-mm-ddTHH:MM(TZ)
    local_time=$(date -j -f "%Y-%m-%dT%H:%M:%S" "${timestamp%.*}" "+%y-%m-%dT%H:%M(%Z)" 2>/dev/null || echo "$timestamp")

    # Format links for display
    links_display=""
    if [ -n "$links" ]; then
      links_display=" [${links}]"
    fi

    # Output with type and hierarchy
    case "$hierarchy" in
      PARENT)
        printf "%s | %s | %s | %s | %s%s\n" "$key" "$type" "$status" "$local_time" "$summary" "$links_display"
        ;;
      CHILD)
        printf "%s | %s | %s | %s | %s\n" "$key" "$type" "$status" "$local_time" "$summary"
        ;;
      STANDALONE)
        printf "%s | %s | %s | %s | %s%s\n" "$key" "$type" "$status" "$local_time" "$summary" "$links_display"
        ;;
      ORPHAN)
        printf "%s | %s | %s | %s | %s%s\n" "$key" "$type" "$status" "$local_time" "$summary" "$links_display"
        ;;
    esac
  done | column -t -s '|'
}

# Search for issues newly assigned to you that you haven't touched yet
# Usage: search_newly_assigned_issues [days]
# Default: 7 days
search_newly_assigned_issues() {
  local days="${1:-7}"
  local date_from
  local jql
  local cache_file
  local max_results=50

  # Use safe credentials loading
  _source_credentials || return 1

  # Calculate date from N days ago
  date_from=$(date -u -v-"${days}"d +"%Y-%m-%d" 2>/dev/null || date -u -d "${days} days ago" +"%Y-%m-%d")
  cache_file="${CACHE_DIR}/jira-newly-assigned-${days}days.json"

  _log "Searching for issues newly assigned to you in the last ${days} days..."
  _log ""

  # JQL: Find issues assigned to you that:
  # 1. Were assigned to you in the last N days
  # 2. You haven't updated (NOT IN updatedBy)
  # 3. You haven't commented on (NOT IN commentedBy)
  jql="assignee = currentUser() AND assignee changed to currentUser() AFTER -${days}d AND issue NOT IN updatedBy(\"${ATLASSIAN_USER}\") AND issue NOT IN commentedBy(\"${ATLASSIAN_USER}\") ORDER BY updated DESC"

  # Check cache age - refresh if older than 5 minutes
  if [ -f "$cache_file" ]; then
    local cache_age_seconds
    local file_mtime
    if [ "$(uname)" = "Darwin" ]; then
      # macOS
      file_mtime=$(stat -f %m "$cache_file" 2>/dev/null || echo "0")
    else
      # Linux
      file_mtime=$(stat -c %Y "$cache_file" 2>/dev/null || echo "0")
    fi

    # If we couldn't get file time, refresh cache
    if [ "$file_mtime" = "0" ]; then
      _log "Could not determine cache age, refreshing..."
      rm -f "$cache_file"
    else
      cache_age_seconds=$(( $(date +%s) - file_mtime ))
      if [ "$cache_age_seconds" -gt 300 ]; then
        _log "Cache older than 5 minutes, refreshing..."
        rm -f "$cache_file"
      fi
    fi
  fi

  # Fetch from API (if cache was deleted or doesn't exist)
  if [ ! -f "$cache_file" ]; then
    curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
      -H 'Accept: application/json' \
      -G \
      --data-urlencode "jql=${jql}" \
      --data-urlencode "fields=key,summary,updated,status,priority,issuetype,created,assignee" \
      --data-urlencode "maxResults=${max_results}" \
      "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
      -o "$cache_file"

    # Cache each individual issue
    jq -c '.issues[] | [.]' "$cache_file" 2>/dev/null | jq -c '.[]' | while read -r issue; do
      local issue_key=$(echo "$issue" | jq -r '.key')
      if [ -n "$issue_key" ] && [ "$issue_key" != "null" ]; then
        local issue_cache="${CACHE_DIR}/jira-${issue_key}.json"
        echo "$issue" > "$issue_cache"
        touch "$issue_cache"
        sleep 0.01
      fi
    done
  fi

  # Display count using issue array length (API sometimes returns .total: 0 incorrectly)
  local count=$(jq -r '(.issues | length) // 0' "$cache_file")
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  echo "Found $count newly assigned issues (last ${days} days)"
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  echo ""

  # Display results with same formatting as search_my_jira_updates
  jq -r '.issues[] |
    .key as $key |
    .fields.issuetype.name as $type |
    .fields.status.name as $status |
    .fields.priority.name as $priority |
    .fields.summary as $summary |
    .fields.updated as $updated |

    # Convert UTC timestamp to local time
    ($updated | sub("\\.[0-9]+\\+0000$"; "Z") | fromdateiso8601 | strftime("%Y-%m-%d %H:%M")) as $local_time |

    # Abbreviate type/status for display
    ($type | gsub(" "; "") | gsub("InProgress"; "InProg")) as $type_abbr |
    ($status | gsub(" "; "") | gsub("InProgress"; "InProg") | gsub("Blocked"; "Blkd")) as $status_abbr |

    "\($key) | \($type_abbr) | \($status_abbr) | \($local_time) | \($summary)"
  ' "$cache_file" | column -t -s '|'

  echo ""
  echo "To view: jira-helper issue <key>"
  echo "To comment: jira-helper issue comment <key> \"your comment\""
}

# Search for Confluence pages updated by you in the last N days
# Usage: search_my_confluence_updates [days]
search_my_confluence_updates() {
  local query="${1:-5}"
  local days
  local date_from
  local cql
  local cache_file

  # Use safe credentials loading
  _source_credentials || return 1

  # Check if argument is numeric (days) or text (search query)
  if [[ "$query" =~ ^[0-9]+$ ]]; then
    # Numeric: treat as days
    days="$query"
    date_from=$(date -u -v-"${days}"d +"%Y-%m-%d" 2>/dev/null || date -u -d "${days} days ago" +"%Y-%m-%d")
    cache_file="${CACHE_DIR}/confluence-my-updates-${days}days.json"

    _log "Searching for Confluence pages updated by you in the last ${days} days..."
    _log ""

    # CQL: type=page AND lastModified >= date AND contributor=currentUser()
    cql="type=page AND lastModified >= ${date_from} AND contributor = currentUser() ORDER BY lastmodified DESC"
  else
    # Text: treat as search query
    cache_file="${CACHE_DIR}/confluence-search-$(echo "$query" | sed 's/[^a-zA-Z0-9]/-/g').json"

    _log "Searching for Confluence pages containing: \"${query}\""
    _log ""

    # CQL: text search with contributor filter
    cql="type=page AND text ~ \"${query}\" AND contributor = currentUser() ORDER BY lastmodified DESC"
  fi

  curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_DOCS}" \
    -H 'Accept: application/json' \
    -G \
    --data-urlencode "cql=${cql}" \
    --data-urlencode "limit=50" \
    --data-urlencode "expand=version,space" \
    "https://${ATLASSIAN_SITE_URL}/wiki/rest/api/content/search" \
    -o "$cache_file"

  # Cache each individual page for tab completion
  # Extract each page and save to individual cache files
  # Write in reverse order so most recent pages have newest timestamps
  jq -c '.results[] | [.]' "$cache_file" 2>/dev/null | jq -c '.[]' | tac | while read -r page; do
    local page_id=$(echo "$page" | jq -r '.id')
    if [ -n "$page_id" ] && [ "$page_id" != "null" ]; then
      local page_cache="${CACHE_DIR}/confluence-${page_id}.json"
      echo "$page" > "$page_cache"
      # Touch file to ensure unique timestamp (files written in API result order)
      touch "$page_cache"
      # Brief sleep to ensure distinct timestamps for tab completion ordering
      sleep 0.01
    fi
  done

  jq -r '.results[] | "\(.id) | \(.space.name // "Unknown") | \(.status) | \(if .version.when then (.version.when | sub("\\.[0-9]+Z$"; "Z") | fromdateiso8601 | strftime("%Y-%m-%d %H:%M")) else "N/A" end) | \(.title)"' "$cache_file" | \
    column -t -s '|'
}

# Add a comment to a Jira issue
# Usage: add_jira_comment PANK-1499 "This is my comment"
# Usage: add_jira_comment PANK-1499 "This is my comment" --yes
add_jira_comment() {
  local input="$1"
  local comment_text="$2"

  if [ -z "$input" ] || [ -z "$comment_text" ]; then
    echo "Error: Usage: add_jira_comment <issue-key or URL> <comment-text or file-path> [--yes|--force]" >&2
    return 1
  fi

  local issue_key
  issue_key=$(parse_jira_key "$input")

  # Check user relationship and confirm if needed
  if ! _confirm_unrelated_action "$issue_key" "$@"; then
    return 1
  fi

  # Use safe credentials loading
  _source_credentials || return 1

  # Check if comment_text is a file path and read it
  if [ -f "$comment_text" ]; then
    comment_text=$(cat "$comment_text")
  fi

  _log "Adding comment to ${issue_key}..."

  # Convert plain text with URLs to ADF format with clickable links and multiple paragraphs
  local adf_paragraphs
  adf_paragraphs=$(_text_to_adf_paragraphs "$comment_text")

  # Add footer with version and link
  local footer
  footer='[{"type": "paragraph", "content": [{"type": "text", "text": ""}]}, {"type": "paragraph", "content": [{"type": "text", "text": "Created with "}, {"type": "text", "text": "jira-helper", "marks": [{"type": "link", "attrs": {"href": "https://github.com/uniphore/platform-utilities/blob/v'$JIRA_HELPER_VERSION'/jira-helper/README.md"}}]}, {"type": "text", "text": " v'$JIRA_HELPER_VERSION' (RC)"}]}]'
  adf_paragraphs=$(echo "$adf_paragraphs" | jq --argjson footer "$footer" '. += $footer')

  local response
  response=$(curl -s -w "\n%{http_code}" -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
    -X POST \
    -H "Content-Type: application/json" \
    --data "$(cat <<EOF
{
  "body": {
    "type": "doc",
    "version": 1,
    "content": $adf_paragraphs
  }
}
EOF
)" \
    "https://${ATLASSIAN_SITE_URL}/rest/api/3/issue/${issue_key}/comment")

  local http_code
  http_code=$(echo "$response" | tail -n1)
  local body
  body=$(echo "$response" | sed '$d')

  if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
    _log "✓ Comment added successfully"
    echo "$body" | jq -r '.id'
  else
    _log "✗ Failed to add comment (HTTP $http_code)"
    echo "$body" | jq '.' >&2
    return 1
  fi
}

# Update an existing Jira comment
# Get the ID of the last comment on a Jira issue
# Usage: get_last_comment_id PANK-1499
# Usage: get_last_comment_id https://site.atlassian.net/browse/PANK-1499
get_last_comment_id() {
  local input="$1"

  if [ -z "$input" ]; then
    echo "Error: Usage: get_last_comment_id <issue-key or URL>" >&2
    return 1
  fi

  local issue_key
  issue_key=$(parse_jira_key "$input")

  # Use safe credentials loading
  _source_credentials || return 1

  local cache_file="${CACHE_DIR}/jira-comments-${issue_key}.json"
  local cache_age=0

  # Check cache age - refresh if older than 5 minutes
  if [ -f "$cache_file" ]; then
    local file_mtime
    if [ "$(uname)" = "Darwin" ]; then
      # macOS
      file_mtime=$(stat -f %m "$cache_file" 2>/dev/null || echo "0")
    else
      # Linux
      file_mtime=$(stat -c %Y "$cache_file" 2>/dev/null || echo "0")
    fi

    if [ "$file_mtime" != "0" ]; then
      cache_age=$(( $(date +%s) - file_mtime ))
    fi
  fi

  # Fetch from API if cache is stale or missing (5 minute TTL)
  if [ ! -f "$cache_file" ] || [ "$cache_age" -gt 300 ]; then
    curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
      -H 'Accept: application/json' \
      -G \
      --data-urlencode "expand=renderedBody" \
      "https://${ATLASSIAN_SITE_URL}/rest/api/3/issue/${issue_key}/comment" \
      -o "$cache_file"
  fi

  # Get the last comment ID
  jq -r '.comments[-1].id // empty' "$cache_file"
}

# Usage: update_jira_comment PANK-1499 463793 "Updated comment text"
# Usage: update_jira_comment https://site.atlassian.net/browse/PANK-1499 463793 "Updated comment text"
# Usage: update_jira_comment PANK-1499 463793 "Updated comment text" --yes
update_jira_comment() {
  local input="$1"
  local comment_id="$2"
  local comment_text="$3"

  if [ -z "$input" ] || [ -z "$comment_id" ] || [ -z "$comment_text" ]; then
    echo "Error: Usage: update_jira_comment <issue-key or URL> <comment-id> <comment-text or file-path> [--yes|--force]" >&2
    return 1
  fi

  local issue_key
  issue_key=$(parse_jira_key "$input")

  # Check user relationship and confirm if needed
  if ! _confirm_unrelated_action "$issue_key" "$@"; then
    return 1
  fi

  # Use safe credentials loading
  _source_credentials || return 1

  # Check if comment_text is a file path and read it
  if [ -f "$comment_text" ]; then
    comment_text=$(cat "$comment_text")
  fi

  _log "Updating comment ${comment_id} on ${issue_key}..."

  # Convert plain text with URLs to ADF format with clickable links and multiple paragraphs
  local adf_paragraphs
  adf_paragraphs=$(_text_to_adf_paragraphs "$comment_text")

  # Add footer with version and link
  local footer
  footer='[{"type": "paragraph", "content": [{"type": "text", "text": ""}]}, {"type": "paragraph", "content": [{"type": "text", "text": "Created with "}, {"type": "text", "text": "jira-helper", "marks": [{"type": "link", "attrs": {"href": "https://github.com/uniphore/platform-utilities/blob/v'$JIRA_HELPER_VERSION'/jira-helper/README.md"}}]}, {"type": "text", "text": " v'$JIRA_HELPER_VERSION' (RC)"}]}]'
  adf_paragraphs=$(echo "$adf_paragraphs" | jq --argjson footer "$footer" '. += $footer')

  local response
  response=$(curl -s -w "\n%{http_code}" -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
    -X PUT \
    -H "Content-Type: application/json" \
    --data "$(cat <<EOF
{
  "body": {
    "type": "doc",
    "version": 1,
    "content": $adf_paragraphs
  }
}
EOF
)" \
    "https://${ATLASSIAN_SITE_URL}/rest/api/3/issue/${issue_key}/comment/${comment_id}")

  local http_code
  http_code=$(echo "$response" | tail -n1)
  local body
  body=$(echo "$response" | sed '$d')

  if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
    _log "✓ Comment updated successfully"
    echo "$body" | jq -r '.id'
  else
    _log "✗ Failed to update comment (HTTP $http_code)"
    echo "$body" | jq '.' >&2
    return 1
  fi
}

# Helper function: Convert multi-line text to ADF paragraphs with clickable links
# Each line becomes a separate paragraph, URLs are converted to links
# Detects bullet lists (lines starting with - or *) and converts to bulletList nodes
_text_to_adf_paragraphs() {
  local text="$1"
  local paragraphs="[]"
  local in_list=false
  local list_items="[]"
  local in_code_block=false
  local code_buffer=""
  local code_language=""
  local in_table=false
  local table_rows="[]"
  local is_header_row=true

  # Split by newlines and process each line
  while IFS= read -r line; do
    # Check for code fence (opening or closing)
    if [[ "$line" =~ ^\`\`\`(.*)$ ]]; then
      if [ "$in_code_block" = false ]; then
        # Opening fence - close any open list/table first
        if [ "$in_table" = true ]; then
          paragraphs=$(echo "$paragraphs" | jq --argjson rows "$table_rows" '. += [{"type": "table", "content": $rows}]')
          table_rows="[]"
          in_table=false
          is_header_row=true
        fi
        if [ "$in_list" = true ]; then
          paragraphs=$(echo "$paragraphs" | jq --argjson items "$list_items" '. += [{"type": "bulletList", "content": $items}]')
          list_items="[]"
          in_list=false
        fi

        in_code_block=true
        code_language="${BASH_REMATCH[1]}"
        code_buffer=""
      else
        # Closing fence - output code block
        local code_text="$code_buffer"
        if [ -n "$code_language" ]; then
          paragraphs=$(echo "$paragraphs" | jq --arg lang "$code_language" --arg text "$code_text" \
            '. += [{"type": "codeBlock", "attrs": {"language": $lang}, "content": [{"type": "text", "text": $text}]}]')
        else
          paragraphs=$(echo "$paragraphs" | jq --arg text "$code_text" \
            '. += [{"type": "codeBlock", "content": [{"type": "text", "text": $text}]}]')
        fi
        in_code_block=false
        code_language=""
        code_buffer=""
      fi
      continue
    fi

    # If inside code block, buffer the line
    if [ "$in_code_block" = true ]; then
      if [ -z "$code_buffer" ]; then
        code_buffer="$line"
      else
        code_buffer="$code_buffer"$'\n'"$line"
      fi
      continue
    fi

    if [ -n "$line" ]; then
      # Check if line is a table row (contains |)
      if [[ "$line" =~ \| ]]; then
        # Skip separator rows (like |---|---|)
        if [[ "$line" =~ ^\|?[[:space:]]*[-:]+[[:space:]]*\|[[:space:]]*[-:]+[[:space:]]*\|?.*$ ]]; then
          is_header_row=false
          continue
        fi

        # Close any open list before starting table
        if [ "$in_list" = true ]; then
          paragraphs=$(echo "$paragraphs" | jq --argjson items "$list_items" '. += [{"type": "bulletList", "content": $items}]')
          list_items="[]"
          in_list=false
        fi

        # Parse table cells
        line="${line#|}"
        line="${line%|}"

        local IFS='|'
        read -ra cells <<< "$line"

        local row_cells="[]"
        local cell_type="tableHeader"
        if [ "$in_table" = true ] && [ "$is_header_row" = false ]; then
          cell_type="tableCell"
        fi

        for cell in "${cells[@]}"; do
          # Trim whitespace
          cell="${cell#"${cell%%[![:space:]]*}"}"
          cell="${cell%"${cell##*[![:space:]]}"}"

          local content=$(_text_to_adf_with_markdown "$cell")
          row_cells=$(echo "$row_cells" | jq --arg type "$cell_type" --argjson content "$content" '. += [{"type": $type, "content": [{"type": "paragraph", "content": $content}]}]')
        done

        table_rows=$(echo "$table_rows" | jq --argjson cells "$row_cells" '. += [{"type": "tableRow", "content": $cells}]')
        in_table=true

      # Check if line is a bullet point (starts with - or * followed by space)
      elif [[ "$line" =~ ^[[:space:]]*[-\*][[:space:]].* ]]; then
        # Close any open table before starting list
        if [ "$in_table" = true ]; then
          paragraphs=$(echo "$paragraphs" | jq --argjson rows "$table_rows" '. += [{"type": "table", "content": $rows}]')
          table_rows="[]"
          in_table=false
          is_header_row=true
        fi

        # Extract the content after the bullet marker
        local bullet_text="${line#"${line%%[-\*]*}"}"  # Remove leading whitespace
        bullet_text="${bullet_text#[-\*]}"  # Remove bullet marker
        bullet_text="${bullet_text#"${bullet_text%%[! ]*}"}"  # Remove leading spaces after marker

        local content=$(_text_to_adf_with_markdown "$bullet_text")
        list_items=$(echo "$list_items" | jq --argjson content "$content" '. += [{"type": "listItem", "content": [{"type": "paragraph", "content": $content}]}]')
        in_list=true
      else
        # Not a bullet point or table row - close any open table first
        if [ "$in_table" = true ]; then
          paragraphs=$(echo "$paragraphs" | jq --argjson rows "$table_rows" '. += [{"type": "table", "content": $rows}]')
          table_rows="[]"
          in_table=false
          is_header_row=true
        fi

        # If we were in a list, close it
        if [ "$in_list" = true ]; then
          paragraphs=$(echo "$paragraphs" | jq --argjson items "$list_items" '. += [{"type": "bulletList", "content": $items}]')
          list_items="[]"
          in_list=false
        fi

        # Check if line is a header (starts with ##)
        if [[ "$line" =~ ^##[[:space:]]+(.*) ]]; then
          local header_text="${BASH_REMATCH[1]}"
          local content=$(_text_to_adf_with_markdown "$header_text")
          paragraphs=$(echo "$paragraphs" | jq --argjson content "$content" '. += [{"type": "heading", "attrs": {"level": 2}, "content": $content}]')
        else
          # Add as regular paragraph
          local content=$(_text_to_adf_with_markdown "$line")
          paragraphs=$(echo "$paragraphs" | jq --argjson content "$content" '. += [{"type": "paragraph", "content": $content}]')
        fi
      fi
    fi
  done <<< "$text"

  # Close any remaining code block at the end (unclosed fence)
  if [ "$in_code_block" = true ]; then
    local code_text="$code_buffer"
    if [ -n "$code_language" ]; then
      paragraphs=$(echo "$paragraphs" | jq --arg lang "$code_language" --arg text "$code_text" \
        '. += [{"type": "codeBlock", "attrs": {"language": $lang}, "content": [{"type": "text", "text": $text}]}]')
    else
      paragraphs=$(echo "$paragraphs" | jq --arg text "$code_text" \
        '. += [{"type": "codeBlock", "content": [{"type": "text", "text": $text}]}]')
    fi
  fi

  # Close any remaining table at the end
  if [ "$in_table" = true ]; then
    paragraphs=$(echo "$paragraphs" | jq --argjson rows "$table_rows" '. += [{"type": "table", "content": $rows}]')
  fi

  # Close any remaining list at the end
  if [ "$in_list" = true ]; then
    paragraphs=$(echo "$paragraphs" | jq --argjson items "$list_items" '. += [{"type": "bulletList", "content": $items}]')
  fi

  echo "$paragraphs"
}

# Helper function: Convert plain text with URLs to ADF content nodes
# URLs are detected and converted to clickable links
_text_to_adf_with_links() {
  local text="$1"
  local result="[]"

  # URL regex pattern (simplified but catches most URLs)
  local url_pattern='https?://[^[:space:])>]+'

  # Split text by URLs and build ADF nodes
  local remaining="$text"
  local first_node=true

  while [[ "$remaining" =~ $url_pattern ]]; do
    local url="${BASH_REMATCH[0]}"
    local before="${remaining%%"$url"*}"
    local after="${remaining#*"$url"}"

    # Add text before URL (if any)
    if [ -n "$before" ]; then
      if [ "$first_node" = true ]; then
        result=$(echo "$result" | jq --arg text "$before" '. += [{"type": "text", "text": $text}]')
        first_node=false
      else
        result=$(echo "$result" | jq --arg text "$before" '. += [{"type": "text", "text": $text}]')
      fi
    fi

    # Add URL as link
    result=$(echo "$result" | jq --arg url "$url" '. += [{"type": "text", "text": $url, "marks": [{"type": "link", "attrs": {"href": $url}}]}]')
    first_node=false

    remaining="$after"
  done

  # Add any remaining text after last URL
  if [ -n "$remaining" ]; then
    if [ "$first_node" = true ]; then
      # No URLs found, just plain text
      result='[{"type": "text", "text": "'"${remaining}"'"}]'
    else
      result=$(echo "$result" | jq --arg text "$remaining" '. += [{"type": "text", "text": $text}]')
    fi
  fi

  echo "$result"
}

# Helper function: Convert text with markdown formatting to ADF content nodes
# Handles: **bold**, `code`, and URLs
_text_to_adf_with_markdown() {
  local text="$1"
  local result="[]"

  # Process text for markdown patterns: **bold**, `code`, URLs
  local remaining="$text"

  while [ -n "$remaining" ]; do
    local matched=false

    # Try to match **bold** (non-greedy)
    if [[ "$remaining" =~ \*\*([^*]+)\*\* ]]; then
      local full_match="${BASH_REMATCH[0]}"
      local bold_text="${BASH_REMATCH[1]}"
      local before="${remaining%%"$full_match"*}"
      local after="${remaining#*"$full_match"}"

      # Add text before bold
      if [ -n "$before" ]; then
        result=$(echo "$result" | jq --arg text "$before" '. += [{"type": "text", "text": $text}]')
      fi

      # Add bold text
      result=$(echo "$result" | jq --arg text "$bold_text" '. += [{"type": "text", "text": $text, "marks": [{"type": "strong"}]}]')
      remaining="$after"
      matched=true

    # Try to match `code`
    elif [[ "$remaining" =~ \`([^\`]+)\` ]]; then
      local full_match="${BASH_REMATCH[0]}"
      local code_text="${BASH_REMATCH[1]}"
      local before="${remaining%%"$full_match"*}"
      local after="${remaining#*"$full_match"}"

      # Add text before code
      if [ -n "$before" ]; then
        result=$(echo "$result" | jq --arg text "$before" '. += [{"type": "text", "text": $text}]')
      fi

      # Add code text
      result=$(echo "$result" | jq --arg text "$code_text" '. += [{"type": "text", "text": $text, "marks": [{"type": "code"}]}]')
      remaining="$after"
      matched=true

    # Try to match URLs
    elif [[ "$remaining" =~ https?://[^[:space:]\)\>]+ ]]; then
      local url="${BASH_REMATCH[0]}"
      local before="${remaining%%"$url"*}"
      local after="${remaining#*"$url"}"

      # Add text before URL
      if [ -n "$before" ]; then
        result=$(echo "$result" | jq --arg text "$before" '. += [{"type": "text", "text": $text}]')
      fi

      # Add URL as link
      result=$(echo "$result" | jq --arg url "$url" '. += [{"type": "text", "text": $url, "marks": [{"type": "link", "attrs": {"href": $url}}]}]')
      remaining="$after"
      matched=true
    fi

    # If nothing matched, add remaining text and break
    if [ "$matched" = false ]; then
      if [ -n "$remaining" ]; then
        result=$(echo "$result" | jq --arg text "$remaining" '. += [{"type": "text", "text": $text}]')
      fi
      break
    fi
  done

  # Handle empty result
  if [ "$result" = "[]" ]; then
    result='[{"type": "text", "text": ""}]'
  fi

  echo "$result"
}

# Create a new Confluence page
# Usage: create_confluence_page SPACE_KEY "Page Title" "<content-string>" [parent-page-id]
# IMPORTANT: Content must be a STRING (markdown or HTML), NOT a file path
# Example: create_confluence_page PE "My Page" "$(cat file.md)" 12345
# Note: Function auto-detects markdown and converts to HTML
# Returns: new page ID
create_confluence_page() {
  local space_key="$1"
  local title="$2"
  local content="$3"
  local parent_id="$4"

  if [ -z "$space_key" ] || [ -z "$title" ] || [ -z "$content" ]; then
    echo "Error: Usage: create_confluence_page <space-key> <title> <content> [parent-page-id]" >&2
    return 1
  fi

  # Use safe credentials loading
  _source_credentials || return 1

  # Detect if content is markdown (no HTML tags but has markdown markers)
  # If it looks like markdown, convert to basic HTML
  if ! echo "$content" | grep -q '<[^>]*>' && echo "$content" | grep -qE '^#{1,6} |^[-*] |\*\*|__|\[.*\]\('; then
    _log "Detected markdown content, converting to HTML..."
    # Enhanced markdown to HTML conversion
    # Process in stages to avoid conflicts between patterns

    # Stage 0: Fenced code blocks (```...```) - must come first to protect content
    content=$(echo "$content" | "$AWK" '
      BEGIN { in_code = 0; code_buffer = "" }
      /^```/ {
        if (in_code == 0) {
          # Start of code block
          in_code = 1
          code_buffer = ""
        } else {
          # End of code block - output buffered content on single line with <br/> tags
          printf "<pre><code>%s</code></pre>\n", code_buffer
          in_code = 0
          code_buffer = ""
        }
        next
      }
      in_code == 1 {
        # Inside code block - HTML escape special characters and buffer
        line = $0
        gsub(/&/, "\\&amp;", line)
        gsub(/</, "\\&lt;", line)
        gsub(/>/, "\\&gt;", line)
        # Append to buffer with <br/> separator (but not before first line)
        if (code_buffer == "") {
          code_buffer = line
        } else {
          code_buffer = code_buffer "<br/>" line
        }
        next
      }
      {
        # Outside code block - pass through for further processing
        print
      }
    ')

    # Stage 0.5: Tables (|...| format) - must come after code blocks, before other conversions
    content=$(echo "$content" | "$AWK" '
      BEGIN { in_table = 0; table_html = "" }
      /^\|.*\|$/ {
        if (in_table == 0) {
          # Start of table
          in_table = 1
          table_html = "<table><tbody>"
          # Parse header row
          gsub(/^\|[ \t]*/, "")
          gsub(/[ \t]*\|$/, "")
          n = split($0, cells, /[ \t]*\|[ \t]*/)
          table_html = table_html "<tr>"
          for (i = 1; i <= n; i++) {
            table_html = table_html "<th>" cells[i] "</th>"
          }
          table_html = table_html "</tr>"
        } else if ($0 ~ /^[|][-: |]+[|]$/) {
          # Separator row - skip it
          next
        } else {
          # Data row
          gsub(/^\|[ \t]*/, "")
          gsub(/[ \t]*\|$/, "")
          n = split($0, cells, /[ \t]*\|[ \t]*/)
          table_html = table_html "<tr>"
          for (i = 1; i <= n; i++) {
            table_html = table_html "<td>" cells[i] "</td>"
          }
          table_html = table_html "</tr>"
        }
        next
      }
      {
        # Not a table row
        if (in_table == 1) {
          # End of table
          print table_html "</tbody></table>"
          in_table = 0
          table_html = ""
        }
        print
      }
      END {
        if (in_table == 1) {
          print table_html "</tbody></table>"
        }
      }
    ')

    # Continue with other markdown conversions
    content=$(echo "$content" | "$SED" -E '
      # Stage 1: Inline code blocks (single backticks)
      s/`([^`]+)`/<code>\1<\/code>/g
    ' | "$SED" -E '
      # Stage 2: Links (must come before bold/italic to avoid bracket conflicts)
      s/\[([^]]+)\]\(([^)]+)\)/<a href="\2">\1<\/a>/g
    ' | "$SED" -E '
      # Stage 3: Bold (** or __) - must come before italic
      s/\*\*([^*]+)\*\*/<strong>\1<\/strong>/g
      s/__([^_]+)__/<strong>\1<\/strong>/g
    ' | "$SED" -E '
      # Stage 4: Italic (* or _) - only match word boundaries to avoid underscores in code
      s/\*([^*]+)\*/<em>\1<\/em>/g
      s/(^|[[:space:]])_([^_]+)_([[:space:]]|$)/\1<em>\2<\/em>\3/g
    ' | "$SED" -E '
      # Stage 5: Headers
      s/^# (.*)$/<h1>\1<\/h1>/g
      s/^## (.*)$/<h2>\1<\/h2>/g
      s/^### (.*)$/<h3>\1<\/h3>/g
      s/^#### (.*)$/<h4>\1<\/h4>/g
      s/^##### (.*)$/<h5>\1<\/h5>/g
      s/^###### (.*)$/<h6>\1<\/h6>/g
    ' | "$SED" -E '
      # Stage 6: Lists
      s/^[*-] (.*)$/<li>\1<\/li>/g
      s/^[0-9]+\. (.*)$/<li>\1<\/li>/g
    ' | "$SED" -E '
      # Stage 7: Paragraphs (lines not already HTML or list items)
      /^<[^>]+>/!s/^(.+)$/<p>\1<\/p>/g
    ')
  fi

  _log "Creating Confluence page '${title}' in space ${space_key}..."

  # Build JSON payload
  local payload
  if [ -n "$parent_id" ]; then
    payload=$(cat <<EOF
{
  "type": "page",
  "title": $(echo "$title" | jq -Rs .),
  "space": {"key": "${space_key}"},
  "ancestors": [{"id": "${parent_id}"}],
  "body": {
    "storage": {
      "value": $(echo "$content" | jq -Rs .),
      "representation": "storage"
    }
  }
}
EOF
)
  else
    payload=$(cat <<EOF
{
  "type": "page",
  "title": $(echo "$title" | jq -Rs .),
  "space": {"key": "${space_key}"},
  "body": {
    "storage": {
      "value": $(echo "$content" | jq -Rs .),
      "representation": "storage"
    }
  }
}
EOF
)
  fi

  # Create the page
  local response
  response=$(curl -s -w "\n%{http_code}" -u "${ATLASSIAN_USER}:${ATLASSIAN_DOCS}" \
    -X POST \
    -H "Content-Type: application/json" \
    --data "$payload" \
    "https://${ATLASSIAN_SITE_URL}/wiki/rest/api/content/")

  local http_code
  http_code=$(echo "$response" | tail -n1)
  local body
  body=$(echo "$response" | sed '$d')

  if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
    local new_page_id
    new_page_id=$(echo "$body" | jq -r '.id')
    local web_url
    web_url=$(echo "$body" | jq -r '._links.webui')
    _log "✓ Page created successfully: ${ATLASSIAN_SITE_URL}/wiki${web_url}"
    echo "$new_page_id"
  else
    _log "✗ Failed to create page (HTTP $http_code)"
    echo "$body" | jq '.' >&2
    return 1
  fi
}

# Update a Confluence page by appending content
# Usage: update_confluence_page PAGE_ID "New content in markdown"
# Returns: updated page version number
update_confluence_page() {
  local page_id="$1"
  local new_content="$2"

  if [ -z "$page_id" ] || [ -z "$new_content" ]; then
    echo "Error: Usage: update_confluence_page <page-id> <new-content>" >&2
    return 1
  fi

  # Use safe credentials loading
  _source_credentials || return 1

  _log "Updating Confluence page ${page_id}..."

  # Get current page data (title and version)
  local page_data
  page_data=$(curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_DOCS}" \
    "https://${ATLASSIAN_SITE_URL}/wiki/rest/api/content/${page_id}?expand=version,body.storage")

  local title
  title=$(echo "$page_data" | jq -r '.title')

  local current_version
  current_version=$(echo "$page_data" | jq -r '.version.number')

  local current_body
  current_body=$(echo "$page_data" | jq -r '.body.storage.value')

  _log "Current version: $current_version"

  # Append new content to existing body
  local updated_body="${current_body}${new_content}"

  # Update the page (build JSON safely)
  local data_file="${CACHE_DIR}/confluence-update-data-${page_id}.json"
  jq -n \
    --argjson version "$((current_version + 1))" \
    --arg title "$title" \
    --arg value "$updated_body" \
    '{
      version: {number: $version},
      title: $title,
      type: "page",
      body: {
        storage: {
          value: $value,
          representation: "storage"
        }
      }
    }' > "$data_file"

  local response
  response=$(curl -s -w "\n%{http_code}" -u "${ATLASSIAN_USER}:${ATLASSIAN_DOCS}" \
    -X PUT \
    -H "Content-Type: application/json" \
    --data @"$data_file" \
    "https://${ATLASSIAN_SITE_URL}/wiki/rest/api/content/${page_id}")

  local http_code
  http_code=$(echo "$response" | tail -n1)
  local body
  body=$(echo "$response" | sed '$d')

  if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
    local new_version
    new_version=$(echo "$body" | jq -r '.version.number')
    _log "✓ Page updated successfully to version $new_version"

    # Invalidate cache after successful update
    local cache_file="${CACHE_DIR}/confluence-${page_id}.json"
    if [ -f "$cache_file" ]; then
      rm -f "$cache_file" "${cache_file%.json}.timestamp"
      _log "Cache invalidated for page ${page_id}"
    fi

    echo "$new_version"
  else
    _log "✗ Failed to update page (HTTP $http_code)"
    echo "$body" | jq '.' >&2
    return 1
  fi
}

# Replace entire Confluence page content
# Usage: replace_confluence_page PAGE_ID "<content-string>"
# IMPORTANT: Content must be a STRING (markdown or HTML), NOT a file path
# Example: replace_confluence_page 123456 "$(cat file.md)"
# Note: Function auto-detects markdown and converts to HTML
# Returns: updated page version number
replace_confluence_page() {
  local page_id="$1"
  local new_content="$2"

  if [ -z "$page_id" ] || [ -z "$new_content" ]; then
    echo "Error: Usage: replace_confluence_page <page-id> <new-content>" >&2
    return 1
  fi

  # Use safe credentials loading
  _source_credentials || return 1

  # Detect if content is markdown (no HTML tags but has markdown markers)
  # If it looks like markdown, convert to basic HTML
  if ! echo "$new_content" | grep -q '<[^>]*>' && echo "$new_content" | grep -qE '^#{1,6} |^[-*] |\*\*|__|\[.*\]\('; then
    _log "Detected markdown content, converting to HTML..."
    # Enhanced markdown to HTML conversion
    # Process in stages to avoid conflicts between patterns

    # Stage 0: Fenced code blocks (```...```) - must come first to protect content
    new_content=$(echo "$new_content" | "$AWK" '
      BEGIN { in_code = 0; code_buffer = "" }
      /^```/ {
        if (in_code == 0) {
          # Start of code block
          in_code = 1
          code_buffer = ""
        } else {
          # End of code block - output buffered content on single line with <br/> tags
          printf "<pre><code>%s</code></pre>\n", code_buffer
          in_code = 0
          code_buffer = ""
        }
        next
      }
      in_code == 1 {
        # Inside code block - HTML escape special characters and buffer
        line = $0
        gsub(/&/, "\\&amp;", line)
        gsub(/</, "\\&lt;", line)
        gsub(/>/, "\\&gt;", line)
        # Append to buffer with <br/> separator (but not before first line)
        if (code_buffer == "") {
          code_buffer = line
        } else {
          code_buffer = code_buffer "<br/>" line
        }
        next
      }
      {
        # Outside code block - pass through for further processing
        print
      }
    ')

    # Stage 0.5: Tables (|...| format) - must come after code blocks, before other conversions
    new_content=$(echo "$new_content" | "$AWK" '
      BEGIN { in_table = 0; table_html = "" }
      /^\|.*\|$/ {
        if (in_table == 0) {
          # Start of table
          in_table = 1
          table_html = "<table><tbody>"
          # Parse header row
          gsub(/^\|[ \t]*/, "")
          gsub(/[ \t]*\|$/, "")
          n = split($0, cells, /[ \t]*\|[ \t]*/)
          table_html = table_html "<tr>"
          for (i = 1; i <= n; i++) {
            table_html = table_html "<th>" cells[i] "</th>"
          }
          table_html = table_html "</tr>"
        } else if ($0 ~ /^[|][-: |]+[|]$/) {
          # Separator row - skip it
          next
        } else {
          # Data row
          gsub(/^\|[ \t]*/, "")
          gsub(/[ \t]*\|$/, "")
          n = split($0, cells, /[ \t]*\|[ \t]*/)
          table_html = table_html "<tr>"
          for (i = 1; i <= n; i++) {
            table_html = table_html "<td>" cells[i] "</td>"
          }
          table_html = table_html "</tr>"
        }
        next
      }
      {
        # Not a table row
        if (in_table == 1) {
          # End of table
          print table_html "</tbody></table>"
          in_table = 0
          table_html = ""
        }
        print
      }
      END {
        if (in_table == 1) {
          print table_html "</tbody></table>"
        }
      }
    ')

    # Continue with other markdown conversions
    new_content=$(echo "$new_content" | "$SED" -E '
      # Stage 1: Inline code blocks (single backticks)
      s/`([^`]+)`/<code>\1<\/code>/g
    ' | "$SED" -E '
      # Stage 2: Links (must come before bold/italic to avoid bracket conflicts)
      s/\[([^]]+)\]\(([^)]+)\)/<a href="\2">\1<\/a>/g
    ' | "$SED" -E '
      # Stage 3: Bold (** or __) - must come before italic
      s/\*\*([^*]+)\*\*/<strong>\1<\/strong>/g
      s/__([^_]+)__/<strong>\1<\/strong>/g
    ' | "$SED" -E '
      # Stage 4: Italic (* or _) - only match word boundaries to avoid underscores in code
      s/\*([^*]+)\*/<em>\1<\/em>/g
      s/(^|[[:space:]])_([^_]+)_([[:space:]]|$)/\1<em>\2<\/em>\3/g
    ' | "$SED" -E '
      # Stage 5: Headers
      s/^# (.*)$/<h1>\1<\/h1>/g
      s/^## (.*)$/<h2>\1<\/h2>/g
      s/^### (.*)$/<h3>\1<\/h3>/g
      s/^#### (.*)$/<h4>\1<\/h4>/g
      s/^##### (.*)$/<h5>\1<\/h5>/g
      s/^###### (.*)$/<h6>\1<\/h6>/g
    ' | "$SED" -E '
      # Stage 6: Lists
      s/^[*-] (.*)$/<li>\1<\/li>/g
      s/^[0-9]+\. (.*)$/<li>\1<\/li>/g
    ' | "$SED" -E '
      # Stage 7: Paragraphs (lines not already HTML or list items)
      /^<[^>]+>/!s/^(.+)$/<p>\1<\/p>/g
    ')
  fi

  _log "Replacing Confluence page ${page_id} content..."

  # Get current page data (title and version)
  local page_data
  page_data=$(curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_DOCS}" \
    "https://${ATLASSIAN_SITE_URL}/wiki/rest/api/content/${page_id}?expand=version")

  local title
  title=$(echo "$page_data" | jq -r '.title')

  local current_version
  current_version=$(echo "$page_data" | jq -r '.version.number')

  _log "Current version: $current_version"

  # Replace with new content (build JSON safely)
  local data_file="${CACHE_DIR}/confluence-replace-data-${page_id}.json"
  jq -n \
    --argjson version "$((current_version + 1))" \
    --arg title "$title" \
    --arg value "$new_content" \
    '{
      version: {number: $version},
      title: $title,
      type: "page",
      body: {
        storage: {
          value: $value,
          representation: "storage"
        }
      }
    }' > "$data_file"

  local response
  response=$(curl -s -w "\n%{http_code}" -u "${ATLASSIAN_USER}:${ATLASSIAN_DOCS}" \
    -X PUT \
    -H "Content-Type: application/json" \
    --data @"$data_file" \
    "https://${ATLASSIAN_SITE_URL}/wiki/rest/api/content/${page_id}")

  local http_code
  http_code=$(echo "$response" | tail -n1)
  local body
  body=$(echo "$response" | sed '$d')

  if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
    local new_version
    new_version=$(echo "$body" | jq -r '.version.number')
    _log "✓ Page replaced successfully to version $new_version"

    # Invalidate cache after successful update
    local cache_file="${CACHE_DIR}/confluence-${page_id}.json"
    if [ -f "$cache_file" ]; then
      rm -f "$cache_file" "${cache_file%.json}.timestamp"
      _log "Cache invalidated for page ${page_id}"
    fi

    echo "$new_version"
  else
    _log "✗ Failed to replace page (HTTP $http_code)"
    echo "$body" | jq '.' >&2
    return 1
  fi
}

# Create a new Jira ticket
# Usage: create_jira_ticket PROJECT "Summary" "Description in markdown" "Task|Bug|Story"
create_jira_ticket() {
  local project="$1"
  local summary="$2"
  local description="$3"
  local issue_type="${4:-Task}"

  if [ -z "$project" ] || [ -z "$summary" ]; then
    echo "Error: Usage: create_jira_ticket <project> <summary> [description] [issue-type]" >&2
    return 1
  fi

  # Use safe credentials loading
  _source_credentials || return 1

  _log "Creating ticket in ${project}: ${summary}"

  local cache_file="${CACHE_DIR}/ticket-create-response.json"
  local data_file="${CACHE_DIR}/ticket-create-data.json"

  # Build JSON payload safely using jq (prevents shell injection)
  jq -n \
    --arg project "$project" \
    --arg summary "$summary" \
    --arg issue_type "$issue_type" \
    '{
      fields: {
        project: {key: $project},
        summary: $summary,
        issuetype: {name: $issue_type}
      }
    }' > "$data_file"

  # Add description if provided (convert to ADF format)
  if [ -n "$description" ]; then
    # Convert text to ADF paragraphs with clickable links
    local adf_paragraphs
    adf_paragraphs=$(_text_to_adf_paragraphs "$description")

    # Add ADF description to payload
    jq --argjson content "$adf_paragraphs" '.fields.description = {
      "type": "doc",
      "version": 1,
      "content": $content
    }' "$data_file" > "${data_file}.tmp" && mv "${data_file}.tmp" "$data_file"
  fi

  local http_code
  http_code=$(curl -s -w "%{http_code}" -o "$cache_file" -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
    -X POST \
    -H "Content-Type: application/json" \
    --data @"$data_file" \
    "https://${ATLASSIAN_SITE_URL}/rest/api/3/issue")

  if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
    local ticket_key
    ticket_key=$(jq -r '.key' "$cache_file")
    _log "✓ Created ticket: ${ticket_key}"
    echo "$ticket_key"
    return 0
  else
    _log "✗ Failed to create ticket (HTTP $http_code)"
    jq '.' "$cache_file" >&2
    return 1
  fi
}

# Create Jira sub-task
# Usage: create_jira_subtask PARENT-KEY "Summary" "Description"
create_jira_subtask() {
  local parent_key="$1"
  local summary="$2"
  local description="${3:-}"

  if [ -z "$parent_key" ] || [ -z "$summary" ]; then
    echo "Error: Usage: create_jira_subtask <parent-key> <summary> [description]" >&2
    return 1
  fi

  # Use safe credentials loading
  _source_credentials || return 1

  # Get parent issue to extract project key
  local parent_project=$(echo "$parent_key" | cut -d'-' -f1)

  local cache_file="${CACHE_DIR}/jira-create-subtask-$$.json"
  local data_file="${CACHE_DIR}/jira-create-subtask-data-$$.json"

  # Create JSON payload safely using jq (prevents shell injection)
  jq -n \
    --arg parent_project "$parent_project" \
    --arg parent_key "$parent_key" \
    --arg summary "$summary" \
    --arg description "$description" \
    '{
      fields: {
        project: {key: $parent_project},
        parent: {key: $parent_key},
        summary: $summary,
        description: {
          type: "doc",
          version: 1,
          content: [
            {
              type: "paragraph",
              content: [
                {
                  type: "text",
                  text: $description
                }
              ]
            }
          ]
        },
        issuetype: {name: "Sub-task"}
      }
    }' > "$data_file"

  _log "Creating sub-task under ${parent_key}..."

  local http_code
  http_code=$(curl -s -w "%{http_code}" -o "$cache_file" \
    -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
    -X POST \
    -H 'Content-Type: application/json' \
    --data @"$data_file" \
    "https://${ATLASSIAN_SITE_URL}/rest/api/3/issue")

  if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
    local subtask_key
    subtask_key=$(jq -r '.key' "$cache_file")
    _log "✓ Created sub-task: ${subtask_key}"
    echo "$subtask_key"
    return 0
  else
    _log "✗ Failed to create sub-task (HTTP $http_code)"
    jq '.' "$cache_file" >&2
    return 1
  fi
}

# Get priority ID from priority name (PANK-1798 enhancement #2)
# Usage: get_priority_id "High"
# Returns: Priority ID (e.g., "3" for High) or empty string if not found
get_priority_id() {
  local priority_name="$1"

  # Use safe credentials loading
  _source_credentials || return 1

  # Common priority mappings (these are instance-specific but commonly used)
  case "$priority_name" in
    "Highest"|"highest"|"1")
      echo "1"
      ;;
    "High"|"high"|"2")
      echo "2"
      ;;
    "Medium"|"medium"|"Med"|"med"|"3")
      echo "3"
      ;;
    "Low"|"low"|"4")
      echo "4"
      ;;
    "Lowest"|"lowest"|"5")
      echo "5"
      ;;
    *)
      # If it's already a number, return it as-is
      if [[ "$priority_name" =~ ^[0-9]+$ ]]; then
        echo "$priority_name"
      else
        # Try to fetch from Jira API
        local priorities=$(curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
          -H 'Accept: application/json' \
          "https://${ATLASSIAN_SITE_URL}/rest/api/3/priority" 2>/dev/null)

        echo "$priorities" | jq -r --arg name "$priority_name" '.[] | select(.name == $name) | .id' | head -1
      fi
      ;;
  esac
}

# Update Jira issue fields (priority, assignee, status, etc.)
# Usage: update_jira_issue PANK-1499 priority "High"  (or priority ID)
#        update_jira_issue PANK-1499 status 51
# Get user account ID from email
# Usage: get_user_account_id "email@example.com"
get_user_account_id() {
  local email="$1"

  if [ -z "$email" ]; then
    echo "Error: Usage: get_user_account_id <email>" >&2
    return 1
  fi

  # Use safe credentials loading
  _source_credentials || return 1

  local account_id
  account_id=$(curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
    "https://${ATLASSIAN_SITE_URL}/rest/api/3/user/search?query=${email}" | jq -r '.[0].accountId')

  if [ -z "$account_id" ] || [ "$account_id" = "null" ]; then
    echo "Error: User not found: $email" >&2
    return 1
  fi

  echo "$account_id"
}

# PANK-1798 enhancement #2: Now accepts priority names (High, Medium, Low, etc.)
# Usage: update_jira_issue PANK-1499 priority High
# Usage: update_jira_issue https://site.atlassian.net/browse/PANK-1499 priority High
# Usage: update_jira_issue PANK-1499 priority High --yes
update_jira_issue() {
  local input="$1"
  local field_type="$2"
  local field_value="$3"

  if [ -z "$input" ] || [ -z "$field_type" ] || [ -z "$field_value" ]; then
    echo "Error: Usage: update_jira_issue <issue-key or URL> <field-type> <value> [--yes|--force]" >&2
    echo "  field-type: priority, assignee, status" >&2
    return 1
  fi

  local issue_key
  issue_key=$(parse_jira_key "$input")

  # Check user relationship and confirm if needed
  if ! _confirm_unrelated_action "$issue_key" "$@"; then
    return 1
  fi

  # Use safe credentials loading
  _source_credentials || return 1

  local endpoint
  local data

  case "$field_type" in
    priority)
      # Convert priority name to ID if needed (PANK-1798 enhancement #2)
      local priority_id=$(get_priority_id "$field_value")
      if [ -z "$priority_id" ]; then
        echo "Error: Unknown priority: $field_value" >&2
        echo "Valid priorities: Highest, High, Medium, Low, Lowest, or numeric ID" >&2
        return 1
      fi
      endpoint="https://${ATLASSIAN_SITE_URL}/rest/api/3/issue/${issue_key}"
      data="{\"fields\":{\"priority\":{\"id\":\"${priority_id}\"}}}"
      _log "Updating ${issue_key} priority to ${field_value} (ID: ${priority_id})..."
      ;;
    assignee)
      # Convert email to account ID if it looks like an email
      local account_id="$field_value"
      if [[ "$field_value" == *"@"* ]]; then
        account_id=$(get_user_account_id "$field_value")
        if [ -z "$account_id" ]; then
          return 1
        fi
      fi
      endpoint="https://${ATLASSIAN_SITE_URL}/rest/api/3/issue/${issue_key}"
      data="{\"fields\":{\"assignee\":{\"accountId\":\"${account_id}\"}}}"
      _log "Assigning ${issue_key} to ${field_value} (Account ID: ${account_id})..."
      ;;
    status)
      # If field_value is not numeric, look up transition ID by name
      local transition_id="$field_value"
      if ! [[ "$field_value" =~ ^[0-9]+$ ]]; then
        # Look up transition ID from name
        transition_id=$(curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
          -H 'Accept: application/json' \
          "https://${ATLASSIAN_SITE_URL}/rest/api/3/issue/${issue_key}/transitions" | \
          jq -r --arg name "$field_value" '.transitions[] | select(.name | ascii_downcase == ($name | ascii_downcase)) | .id' | head -1)
        if [ -z "$transition_id" ]; then
          echo "Error: No transition found for status '$field_value'" >&2
          echo "Use 'jira-helper issue transitions ${issue_key}' to see available transitions" >&2
          return 1
        fi
      fi
      endpoint="https://${ATLASSIAN_SITE_URL}/rest/api/3/issue/${issue_key}/transitions"
      data="{\"transition\":{\"id\":\"${transition_id}\"}}"
      _log "Transitioning ${issue_key} to status ${field_value} (ID: ${transition_id})..."
      ;;
    *)
      echo "Error: Unsupported field type: $field_type" >&2
      echo "Supported types: priority, assignee, status" >&2
      return 1
      ;;
  esac

  local method
  if [ "$field_type" = "priority" ] || [ "$field_type" = "assignee" ]; then
    method="PUT"
  else
    method="POST"
  fi

  local http_code
  http_code=$(curl -s -w "%{http_code}" -o /dev/null -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
    -X "$method" \
    -H "Content-Type: application/json" \
    --data "$data" \
    "$endpoint")

  if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
    _log "✓ Update successful"
    # Invalidate cache for this issue
    rm -f "${CACHE_DIR}/jira-${issue_key}.json" "${CACHE_DIR}/jira-${issue_key}.timestamp"
    return 0
  else
    _log "✗ Update failed (HTTP $http_code)"
    return 1
  fi
}

# Upload attachment to Jira issue
# Usage: upload_jira_attachment <issue-key or URL> <file-path> [--yes|--force]
upload_jira_attachment() {
  local input="$1"
  local file_path="$2"

  if [ -z "$input" ] || [ -z "$file_path" ]; then
    echo "Error: Usage: upload_jira_attachment <issue-key or URL> <file-path> [--yes|--force]" >&2
    return 1
  fi

  # Validate file path
  if [ ! -f "$file_path" ]; then
    echo "Error: File not found: $file_path" >&2
    return 1
  fi

  local issue_key
  issue_key=$(parse_jira_key "$input")

  # Check user relationship and confirm if needed
  if ! _confirm_unrelated_action "$issue_key" "$@"; then
    return 1
  fi

  # Use safe credentials loading
  _source_credentials || return 1

  # Log operation
  local filename
  filename=$(basename "$file_path")
  _log "Uploading attachment '${filename}' to ${issue_key}..."

  # Construct URL
  local url="https://${ATLASSIAN_SITE_URL}/rest/api/3/issue/${issue_key}/attachments"

  # Make API call with X-Atlassian-Token header (required for attachments)
  local response
  response=$(curl -s -w "\n%{http_code}" -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
    -X POST \
    -H "X-Atlassian-Token: no-check" \
    -F "file=@${file_path}" \
    "$url")

  local http_code
  http_code=$(echo "$response" | tail -n1)
  local body
  body=$(echo "$response" | sed '$d')

  if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
    local attachment_id
    attachment_id=$(echo "$body" | jq -r '.[0].id')
    _log "Attachment uploaded successfully (ID: ${attachment_id})"
    echo "$attachment_id"
    return 0
  else
    _log "Failed to upload attachment (HTTP $http_code)"
    echo "$body" | jq '.' >&2
    return 1
  fi
}

# Update Jira issue summary
# Usage: update_jira_summary PANK-1499 "New summary text" [--yes|--force]
update_jira_summary() {
  local input="$1"
  local new_summary="$2"

  if [ -z "$input" ] || [ -z "$new_summary" ]; then
    echo "Error: Usage: update_jira_summary <issue-key or URL> \"summary text\" [--yes|--force]" >&2
    return 1
  fi

  local issue_key
  issue_key=$(parse_jira_key "$input")

  # Check user relationship and confirm if needed
  if ! _confirm_unrelated_action "$issue_key" "$@"; then
    return 1
  fi

  # Use safe credentials loading
  _source_credentials || return 1

  # Get issue ID (needed for proper API endpoint)
  local issue_id
  issue_id=$(get_jira_issue "$issue_key" --json 2>/dev/null | jq -r '.id')
  if [ -z "$issue_id" ] || [ "$issue_id" = "null" ]; then
    echo "Error: Could not find issue ${issue_key}" >&2
    return 1
  fi

  local endpoint="https://${ATLASSIAN_SITE_URL}/rest/api/3/issue/${issue_id}"
  local data
  data=$(jq -n --arg summary "$new_summary" '{fields: {summary: $summary}}')

  _log "Updating ${issue_key} summary..."

  local http_code
  http_code=$(curl -s -w "%{http_code}" -o /dev/null -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
    -X PUT \
    -H "Content-Type: application/json" \
    --data "$data" \
    "$endpoint")

  if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
    _log "Updated summary"
    # Invalidate cache for this issue
    rm -f "${CACHE_DIR}/jira-${issue_key}.json" "${CACHE_DIR}/jira-${issue_key}.timestamp"
    return 0
  else
    _log "Update failed (HTTP $http_code)"
    return 1
  fi
}

# Update Jira issue description
# Usage: update_jira_description PANK-1499 "New description text" [--yes|--force]
# Note: Description is plain text - will be converted to ADF format automatically
update_jira_description() {
  local input="$1"
  local new_description="$2"

  if [ -z "$input" ] || [ -z "$new_description" ]; then
    echo "Error: Usage: update_jira_description <issue-key or URL> \"description text\" [--yes|--force]" >&2
    return 1
  fi

  local issue_key
  issue_key=$(parse_jira_key "$input")

  # Check user relationship and confirm if needed
  if ! _confirm_unrelated_action "$issue_key" "$@"; then
    return 1
  fi

  # Use safe credentials loading
  _source_credentials || return 1

  # Get issue ID (needed for proper API endpoint)
  local issue_id
  issue_id=$(get_jira_issue "$issue_key" --json 2>/dev/null | jq -r '.id')
  if [ -z "$issue_id" ] || [ "$issue_id" = "null" ]; then
    echo "Error: Could not find issue ${issue_key}" >&2
    return 1
  fi

  local endpoint="https://${ATLASSIAN_SITE_URL}/rest/api/3/issue/${issue_id}"

  # Convert plain text to ADF format
  # Split on double newlines for paragraphs, preserve bullet lists
  local adf_content
  adf_content=$(echo "$new_description" | awk '
    BEGIN {
      print "{\"type\":\"doc\",\"version\":1,\"content\":["
      in_list = 0
      para_count = 0
    }

    /^[*-] / {
      # Bullet list item
      if (!in_list) {
        if (para_count > 0) print ","
        print "{\"type\":\"bulletList\",\"content\":["
        in_list = 1
        item_count = 0
      }
      if (item_count > 0) print ","
      text = substr($0, 3)
      gsub(/"/, "\\\"", text)
      print "{\"type\":\"listItem\",\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"" text "\"}]}]}"
      item_count++
      next
    }

    /^$/ {
      # Empty line - close list if open
      if (in_list) {
        print "]}"
        in_list = 0
        para_count++
      }
      next
    }

    {
      # Regular paragraph
      if (in_list) {
        print "]}"
        in_list = 0
        para_count++
      }
      if (para_count > 0) print ","
      gsub(/"/, "\\\"", $0)
      print "{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"" $0 "\"}]}"
      para_count++
    }

    END {
      if (in_list) print "]}"
      print "]}"
    }
  ')

  local data="{\"fields\":{\"description\":${adf_content}}}"

  _log "Updating ${issue_key} description..."

  local http_code
  http_code=$(curl -s -w "%{http_code}" -o /dev/null -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
    -X PUT \
    -H "Content-Type: application/json" \
    --data "$data" \
    "$endpoint")

  if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
    _log "Updated description"
    # Invalidate cache for this issue
    rm -f "${CACHE_DIR}/jira-${issue_key}.json" "${CACHE_DIR}/jira-${issue_key}.timestamp"
    return 0
  else
    _log "Update failed (HTTP $http_code)"
    return 1
  fi
}

# Add labels to a Jira issue
# Usage: add_jira_labels PANK-1499 label1 [label2 ...] [--yes|--force]
# Example: add_jira_labels PANK-1499 stale-issue wiz-noise
add_jira_labels() {
  local input="$1"
  shift

  if [ -z "$input" ] || [ $# -eq 0 ]; then
    echo "Error: Usage: add_jira_labels <issue-key or URL> <label1> [label2 ...] [--yes|--force]" >&2
    return 1
  fi

  local issue_key
  issue_key=$(parse_jira_key "$input")

  # Check user relationship and confirm if needed
  if ! _confirm_unrelated_action "$issue_key" "$@"; then
    return 1
  fi

  # Use safe credentials loading
  _source_credentials || return 1

  # Collect labels (filter out --yes/--force flags)
  local new_labels=()
  for arg in "$@"; do
    case "$arg" in
      --yes|--force)
        ;;
      *)
        new_labels+=("$arg")
        ;;
    esac
  done

  if [ ${#new_labels[@]} -eq 0 ]; then
    echo "Error: No labels provided" >&2
    return 1
  fi

  # Get current labels
  local current_labels
  current_labels=$(get_jira_issue "$issue_key" --json 2>/dev/null | jq -r '.fields.labels[]' 2>/dev/null)

  # Merge current and new labels (avoid duplicates)
  local all_labels=()
  if [ -n "$current_labels" ]; then
    while IFS= read -r label; do
      all_labels+=("$label")
    done <<< "$current_labels"
  fi

  for new_label in "${new_labels[@]}"; do
    # Check if label already exists
    local exists=false
    for existing_label in "${all_labels[@]}"; do
      if [ "$existing_label" = "$new_label" ]; then
        exists=true
        break
      fi
    done
    if ! $exists; then
      all_labels+=("$new_label")
    fi
  done

  # Build JSON array of all labels
  local labels_json="["
  local first=true
  for label in "${all_labels[@]}"; do
    if $first; then
      first=false
    else
      labels_json+=","
    fi
    labels_json+="\"$label\""
  done
  labels_json+="]"

  local endpoint="https://${ATLASSIAN_SITE_URL}/rest/api/3/issue/${issue_key}"
  local data="{\"fields\":{\"labels\":${labels_json}}}"

  _log "Adding labels to ${issue_key}: ${new_labels[*]}"

  local http_code
  http_code=$(curl -s -w "%{http_code}" -o /dev/null -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
    -X PUT \
    -H "Content-Type: application/json" \
    --data "$data" \
    "$endpoint")

  if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
    echo "Labels added to ${issue_key}: ${new_labels[*]}"
    # Invalidate cache for this issue
    rm -f "${CACHE_DIR}/jira-${issue_key}.json" "${CACHE_DIR}/jira-${issue_key}.timestamp"
    return 0
  else
    echo "Failed to add labels (HTTP $http_code)" >&2
    return 1
  fi
}

# Get available transitions for a Jira issue
# Usage: get_jira_transitions PANK-1499
get_jira_transitions() {
  local input="$1"

  if [ -z "$input" ]; then
    echo "Error: Usage: get_jira_transitions <issue-key or URL>" >&2
    return 1
  fi

  local issue_key
  issue_key=$(parse_jira_key "$input")

  # Use safe credentials loading
  _source_credentials || return 1

  _log "Available transitions for ${issue_key}:"
  _log ""

  curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
    "https://${ATLASSIAN_SITE_URL}/rest/api/3/issue/${issue_key}/transitions" | \
    jq -r '.transitions[] | "\(.id) | \(.name)"' | \
    column -t -s '|'
}

# Generate End of Day status report
# Usage: eod_report [days] [template_file] [is_slack]
eod_report() {
  local days="${1:-1}"
  local template_file="${2:-eod-report-template.txt}"
  local is_slack="${3:-false}"

  # Validate days parameter is a positive integer
  if ! [[ "$days" =~ ^[0-9]+$ ]] || [ "$days" -lt 1 ]; then
    echo "Error: First parameter must be a positive integer (days)" >&2
    echo "" >&2
    echo "Usage: eod_report [days] [template] [is_slack]" >&2
    echo "Examples:" >&2
    echo "  eod_report                        # Today's report" >&2
    echo "  eod_report 1                      # Today's report" >&2
    echo "  eod_report 7                      # Last 7 days" >&2
    echo "  eod_report 1 slack                # Today's report in slack format" >&2
    return 1
  fi

  # If template is relative path, look in script directory's templates/ folder
  if [[ "$template_file" != /* ]]; then
    local script_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

    # Handle short names like "slack", "slack_compact", "slack-compact"
    if [[ "$template_file" == "slack" ]]; then
      template_file="eod-report-slack-template.txt"
    elif [[ "$template_file" == "slack_compact" ]] || [[ "$template_file" == "slack-compact" ]]; then
      template_file="eod-report-slack-compact.txt"
    elif [[ "$template_file" == "slack_plain" ]] || [[ "$template_file" == "slack-plain" ]]; then
      template_file="eod-report-slack-plain.txt"
    elif [[ "$template_file" != *".txt" ]] && [[ "$template_file" != "eod-report-"* ]]; then
      # If no extension and doesn't start with eod-report-, assume it's a short name
      template_file="eod-report-${template_file}.txt"
    fi

    if [ -f "${script_dir}/templates/${template_file}" ]; then
      template_file="${script_dir}/templates/${template_file}"
    elif [ -f "${CACHE_DIR}/${template_file}" ]; then
      template_file="${CACHE_DIR}/${template_file}"
    else
      echo "Error: Template file not found: ${template_file}" >&2
      echo "Available templates:" >&2
      ls -1 "${script_dir}/templates/" 2>/dev/null | sed 's/^/  /' >&2
      return 1
    fi
  fi

  # Auto-detect slack format from template filename if not explicitly set
  if [[ "$is_slack" == "false" && "$template_file" == *"slack"* ]]; then
    is_slack=true
  fi

  # Use safe credentials loading
  _source_credentials || return 1

  # Set date label
  local date_label
  if [ "$days" -eq 1 ]; then
    date_label="Today"
  else
    date_label="Last ${days} Days"
  fi

  # Fetch Jira issues (recently updated) - cache for 5 minutes
  local jira_cache="${CACHE_DIR}/jira-my-updates-${days}days.json"
  if ! is_cache_fresh "$jira_cache" 300; then
    _log "Refreshing Jira updates cache..."
    local jql="issue in updatedBy(\"${ATLASSIAN_USER}\", \"-${days}d\") ORDER BY updated DESC"
    curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
      -H 'Accept: application/json' \
      -G \
      --data-urlencode "jql=${jql}" \
      --data-urlencode "fields=key,summary,updated,status,priority,comment,changelog" \
      --data-urlencode "expand=changelog" \
      --data-urlencode "maxResults=50" \
      "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
      -o "$jira_cache" 2>/dev/null
  else
    _log "Using cached Jira updates (fresh)"
  fi

  # Fetch Jira issues (next to work on) - cache for 5 minutes
  # Excludes Blocked status per PANK-1798 enhancement #1
  local jira_next_cache="${CACHE_DIR}/jira-my-next.json"
  if ! is_cache_fresh "$jira_next_cache" 300; then
    _log "Refreshing Jira next cache..."
    local jql_next="assignee = currentUser() AND status NOT IN (Done, Closed, Blocked) ORDER BY updated DESC, priority DESC, created ASC"
    curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
      -H 'Accept: application/json' \
      -G \
      --data-urlencode "jql=${jql_next}" \
      --data-urlencode "fields=key,summary,status,priority,created,updated" \
      --data-urlencode "maxResults=10" \
      "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
      -o "$jira_next_cache" 2>/dev/null
  else
    _log "Using cached Jira next (fresh)"
  fi

  # Fetch Jira issues created by you - cache for 5 minutes
  local jira_created_cache="${CACHE_DIR}/jira-my-created-${days}days.json"
  if ! is_cache_fresh "$jira_created_cache" 300; then
    _log "Refreshing Jira created cache..."
    local jql_created="creator = currentUser() AND created >= -${days}d ORDER BY created DESC"
    curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
      -H 'Accept: application/json' \
      -G \
      --data-urlencode "jql=${jql_created}" \
      --data-urlencode "fields=key,summary,status,priority,created" \
      --data-urlencode "maxResults=50" \
      "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
      -o "$jira_created_cache" 2>/dev/null
  else
    _log "Using cached Jira created (fresh)"
  fi

  # Fetch Confluence pages - cache for 5 minutes
  local confluence_cache="${CACHE_DIR}/confluence-my-updates-${days}days.json"
  if ! is_cache_fresh "$confluence_cache" 300; then
    _log "Refreshing Confluence pages cache..."
    local date_from
    date_from=$(date -u -v-"${days}"d +"%Y-%m-%d" 2>/dev/null || date -u -d "${days} days ago" +"%Y-%m-%d")
    local cql="type=page AND lastModified >= ${date_from} AND contributor = currentUser() ORDER BY lastmodified DESC"
    curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_DOCS}" \
      -H 'Accept: application/json' \
      -G \
      --data-urlencode "cql=${cql}" \
      --data-urlencode "limit=50" \
      --data-urlencode "expand=version,space" \
      "https://${ATLASSIAN_SITE_URL}/wiki/rest/api/content/search" \
      -o "$confluence_cache" 2>/dev/null
  else
    _log "Using cached Confluence pages (fresh)"
  fi

  # Determine if using plain slack format (no link syntax, just URLs)
  local is_slack_plain=false
  if [[ "$template_file" == *"slack-plain"* ]]; then
    is_slack_plain=true
  fi

  # Format Jira issues
  local jira_count
  jira_count=$(jq '.issues | length' "$jira_cache")
  local jira_issues
  if [ "$jira_count" -gt 0 ]; then
    if [ "$is_slack_plain" = true ]; then
      # Slack plain format: just URL on its own line
      jira_issues=$(jq -r --arg site "https://${ATLASSIAN_SITE_URL}" --arg days "$days" '.issues[] |
        # Get current priority and status
        .fields.priority.name as $priority |
        .fields.status.name as $status |

        # Check changelog for priority and status changes in timeframe
        (if .changelog and .changelog.histories then
          (.changelog.histories |
           map(select(
             .created |
             sub("\\.[0-9]+"; "") |
             sub("\\+0000$"; "Z") |
             fromdateiso8601 > (now - ($days | tonumber * 86400))
           )) |
           map(.items[] | select(.field == "status" or .field == "priority")) |
           {
             priority: map(select(.field == "priority"))[0] // null,
             status: map(select(.field == "status"))[0] // null
           }
          )
        else
          {priority: null, status: null}
        end) as $changes |

        # Abbreviation function
        def abbrev:
          gsub(" "; "") |
          gsub("InProgress"; "InProg") |
          gsub("Progress"; "Prog") |
          gsub("Blocked"; "Blkd") |
          gsub("BACKLOG"; "Bklog") |
          gsub("Critical"; "Crit") |
          gsub("Medium"; "Med") |
          gsub("Waiting"; "Wait");

        # Build priority display with transition (remove whitespace and abbreviate)
        ($changes.priority // null) as $prioChange |
        (if $prioChange then
          ($prioChange.fromString | abbrev) + "->" + ($prioChange.toString | abbrev)
        else
          ($priority | abbrev)
        end) as $prioDisplay |

        # Build status display with transition (remove whitespace and abbreviate)
        ($changes.status // null) as $statusChange |
        (if $statusChange then
          ($statusChange.fromString | abbrev) + "->" + ($status | abbrev)
        else
          ($status | abbrev)
        end) as $statusDisplay |

        "• \($site)/browse/\(.key) [\($prioDisplay)] \($statusDisplay): \(.fields.summary)" +
        (if .fields.comment.comments | length > 0 then
          "\n  " + (.fields.comment.comments[-1].body.content[0].content[0].text // "No text content")
        else
          ""
        end)' "$jira_cache" 2>/tmp/jira-helper-jq-stderr.log)
      # Validate output count matches expected (slack_plain format)
      local actual_issue_count
      actual_issue_count=$(echo "$jira_issues" | grep -c '^[^ ]' 2>/dev/null || echo 0)
      if [ "$actual_issue_count" -lt "$jira_count" ]; then
        echo "[WARN] EOD report: Expected $jira_count issues but only got $actual_issue_count. Check /tmp/jira-helper-jq-stderr.log" >&2
        if [ -s /tmp/jira-helper-jq-stderr.log ]; then
          echo "[WARN] jq stderr: $(head -3 /tmp/jira-helper-jq-stderr.log)" >&2
        fi
      fi
    elif [ "$is_slack" = true ]; then
      # Slack format: <url|text>
      jira_issues=$(jq -r --arg site "https://${ATLASSIAN_SITE_URL}" --arg days "$days" '.issues[] |
        # Get current priority and status
        .fields.priority.name as $priority |
        .fields.status.name as $status |

        # Check changelog for priority and status changes in timeframe
        (if .changelog and .changelog.histories then
          (.changelog.histories |
           map(select(
             .created |
             sub("\\.[0-9]+"; "") |
             sub("\\+0000$"; "Z") |
             fromdateiso8601 > (now - ($days | tonumber * 86400))
           )) |
           map(.items[] | select(.field == "status" or .field == "priority")) |
           {
             priority: map(select(.field == "priority"))[0] // null,
             status: map(select(.field == "status"))[0] // null
           }
          )
        else
          {priority: null, status: null}
        end) as $changes |

        # Abbreviation function
        def abbrev:
          gsub(" "; "") |
          gsub("InProgress"; "InProg") |
          gsub("Progress"; "Prog") |
          gsub("Blocked"; "Blkd") |
          gsub("BACKLOG"; "Bklog") |
          gsub("Critical"; "Crit") |
          gsub("Medium"; "Med") |
          gsub("Waiting"; "Wait");

        # Build priority display with transition (remove whitespace and abbreviate)
        ($changes.priority // null) as $prioChange |
        (if $prioChange then
          ($prioChange.fromString | abbrev) + "->" + ($prioChange.toString | abbrev)
        else
          ($priority | abbrev)
        end) as $prioDisplay |

        # Build status display with transition (remove whitespace and abbreviate)
        ($changes.status // null) as $statusChange |
        (if $statusChange then
          ($statusChange.fromString | abbrev) + "->" + ($status | abbrev)
        else
          ($status | abbrev)
        end) as $statusDisplay |

        "• \($site)/browse/\(.key) [\($prioDisplay)] \($statusDisplay): \(.fields.summary)" +
        (if .fields.comment.comments | length > 0 then
          # Get all text content from last comment, skip jira-helper auto-comments
          # Type-safe: collect texts into array, join to string, check type before contains()
          [.fields.comment.comments[-1].body.content[].content[]?.text // empty] as $allTexts |
          ($allTexts | join(" ")) as $combinedText |
          if (($combinedText | type) == "string" and ($combinedText | contains("Created with")) and ($combinedText | contains("jira-helper"))) then
            ""
          else
            $allTexts |
            if length > 0 then
              "\n    • " + join("\n    • ")
            else
              ""
            end
          end
        else
          ""
        end)' "$jira_cache" 2>/tmp/jira-helper-jq-stderr.log)
      # Validate output count matches expected
      local actual_issue_count
      actual_issue_count=$(echo "$jira_issues" | grep -c '^[^ ]' 2>/dev/null || echo 0)
      if [ "$actual_issue_count" -lt "$jira_count" ]; then
        echo "[WARN] EOD report: Expected $jira_count issues but only got $actual_issue_count. Check /tmp/jira-helper-jq-stderr.log" >&2
        if [ -s /tmp/jira-helper-jq-stderr.log ]; then
          echo "[WARN] jq stderr: $(head -3 /tmp/jira-helper-jq-stderr.log)" >&2
        fi
      fi
    else
      # Markdown format: [text](url)
      jira_issues=$(jq -r --arg site "https://${ATLASSIAN_SITE_URL}" --arg days "$days" '.issues[] |
        # Get current priority and status
        .fields.priority.name as $priority |
        .fields.status.name as $status |

        # Check changelog for priority and status changes in timeframe
        (if .changelog and .changelog.histories then
          (.changelog.histories |
           map(select(
             .created |
             sub("\\.[0-9]+"; "") |
             sub("\\+0000$"; "Z") |
             fromdateiso8601 > (now - ($days | tonumber * 86400))
           )) |
           map(.items[] | select(.field == "status" or .field == "priority")) |
           {
             priority: map(select(.field == "priority"))[0] // null,
             status: map(select(.field == "status"))[0] // null
           }
          )
        else
          {priority: null, status: null}
        end) as $changes |

        # Abbreviation function
        def abbrev:
          gsub(" "; "") |
          gsub("InProgress"; "InProg") |
          gsub("Progress"; "Prog") |
          gsub("Blocked"; "Blkd") |
          gsub("BACKLOG"; "Bklog") |
          gsub("Critical"; "Crit") |
          gsub("Medium"; "Med") |
          gsub("Waiting"; "Wait");

        # Build priority display with transition (remove whitespace and abbreviate)
        ($changes.priority // null) as $prioChange |
        (if $prioChange then
          ($prioChange.fromString | abbrev) + "->" + ($prioChange.toString | abbrev)
        else
          ($priority | abbrev)
        end) as $prioDisplay |

        # Build status display with transition (remove whitespace and abbreviate)
        ($changes.status // null) as $statusChange |
        (if $statusChange then
          ($statusChange.fromString | abbrev) + "->" + ($status | abbrev)
        else
          ($status | abbrev)
        end) as $statusDisplay |

        "  [\(.key)](\($site)/browse/\(.key)) [\($prioDisplay)] \($statusDisplay): \(.fields.summary)" +
        (if .fields.comment.comments | length > 0 then
          "\n    - " + (.fields.comment.comments[-1].body.content[0].content[0].text // "No text content")
        else
          ""
        end)' "$jira_cache" 2>/tmp/jira-helper-jq-stderr.log)
      # Validate output count matches expected (markdown format)
      local actual_issue_count
      actual_issue_count=$(echo "$jira_issues" | grep -c '^\s*\[' 2>/dev/null || echo 0)
      if [ "$actual_issue_count" -lt "$jira_count" ]; then
        echo "[WARN] EOD report: Expected $jira_count issues but only got $actual_issue_count. Check /tmp/jira-helper-jq-stderr.log" >&2
        if [ -s /tmp/jira-helper-jq-stderr.log ]; then
          echo "[WARN] jq stderr: $(head -3 /tmp/jira-helper-jq-stderr.log)" >&2
        fi
      fi
    fi
  else
    jira_issues="  No Jira issues updated"
  fi

  # Format Jira next issues (top 3 by priority and age)
  local jira_next_count
  jira_next_count=$(jq '.issues | length' "$jira_next_cache")
  local jira_issues_next
  if [ "$jira_next_count" -gt 0 ]; then
    if [ "$is_slack_plain" = true ]; then
      # Slack plain format: just URL on its own line
      jira_issues_next=$(jq -r --arg site "https://${ATLASSIAN_SITE_URL}" '.issues[] |
        "• \($site)/browse/\(.key) [\(.fields.priority.name)] \(.fields.summary)"' "$jira_next_cache")
    elif [ "$is_slack" = true ]; then
      # Slack format: url [priority] summary
      jira_issues_next=$(jq -r --arg site "https://${ATLASSIAN_SITE_URL}" '.issues[] |
        "• \($site)/browse/\(.key) [\(.fields.priority.name)] \(.fields.summary)"' "$jira_next_cache")
    else
      # Markdown format: [text](url)
      jira_issues_next=$(jq -r --arg site "https://${ATLASSIAN_SITE_URL}" '.issues[] |
        "  [\(.key)](\($site)/browse/\(.key)) [\(.fields.priority.name)] \(.fields.summary)"' "$jira_next_cache")
    fi
  else
    jira_issues_next="  No pending issues"
  fi

  # Format Jira created issues
  local jira_created_count
  jira_created_count=$(jq '.issues | length' "$jira_created_cache")
  local jira_issues_created
  if [ "$jira_created_count" -gt 0 ]; then
    if [ "$is_slack_plain" = true ]; then
      # Slack plain format: just URL on its own line
      jira_issues_created=$(jq -r --arg site "https://${ATLASSIAN_SITE_URL}" '.issues[] |
        "• \($site)/browse/\(.key) [\(.fields.status.name)]: \(.fields.summary)"' "$jira_created_cache")
    elif [ "$is_slack" = true ]; then
      # Slack format: url [status] summary
      jira_issues_created=$(jq -r --arg site "https://${ATLASSIAN_SITE_URL}" '.issues[] |
        "• \($site)/browse/\(.key) [\(.fields.status.name)] \(.fields.summary)"' "$jira_created_cache")
    else
      # Markdown format: [text](url)
      jira_issues_created=$(jq -r --arg site "https://${ATLASSIAN_SITE_URL}" '.issues[] |
        "  [\(.key)](\($site)/browse/\(.key)) [\(.fields.status.name)]: \(.fields.summary)"' "$jira_created_cache")
    fi
  else
    jira_issues_created="  No issues created"
  fi

  # Format Confluence pages
  local confluence_count
  confluence_count=$(jq '.results | length' "$confluence_cache")
  local confluence_pages
  if [ "$confluence_count" -gt 0 ]; then
    if [ "$is_slack_plain" = true ]; then
      # Slack plain format: just URL on its own line
      confluence_pages=$(jq -r --arg site "https://${ATLASSIAN_SITE_URL}" '.results[] |
        "• \($site)/wiki\(._links.webui)"' "$confluence_cache")
    elif [ "$is_slack" = true ]; then
      # Slack format: url
      confluence_pages=$(jq -r --arg site "https://${ATLASSIAN_SITE_URL}" '.results[] |
        "• \($site)/wiki\(._links.webui)"' "$confluence_cache")
    else
      # Markdown format: [text](url)
      confluence_pages=$(jq -r --arg site "https://${ATLASSIAN_SITE_URL}" '.results[] |
        "  [\(.title)](\($site)/wiki\(._links.webui))"' "$confluence_cache")
    fi
  else
    confluence_pages="  No Confluence pages updated"
  fi

  # Generate report from template using variable substitution
  local report_date
  report_date=$(date '+%Y-%m-%d %H:%M %Z')

  # Read template and substitute variables
  while IFS= read -r line; do
    line="${line//\{\{DATE_LABEL\}\}/$date_label}"
    line="${line//\{\{REPORT_DATE\}\}/$report_date}"
    line="${line//\{\{USER_EMAIL\}\}/$ATLASSIAN_USER}"
    line="${line//\{\{JIRA_COUNT\}\}/$jira_count}"
    line="${line//\{\{CONFLUENCE_COUNT\}\}/$confluence_count}"

    # Handle multi-line variables (with or without spaces around macro names)
    if [[ "$line" == *"{{JIRA_ISSUES}}"* ]] || [[ "$line" == *"{{ JIRA_ISSUES }}"* ]]; then
      echo "$jira_issues"
    elif [[ "$line" == *"{{JIRA_ISSUES_CREATED}}"* ]] || [[ "$line" == *"{{ JIRA_ISSUES_CREATED }}"* ]]; then
      echo "$jira_issues_created"
    elif [[ "$line" == *"{{JIRA_ISSUES_NEXT}}"* ]] || [[ "$line" == *"{{ JIRA_ISSUES_NEXT }}"* ]]; then
      echo "$jira_issues_next"
    elif [[ "$line" == *"{{CONFLUENCE_PAGES}}"* ]] || [[ "$line" == *"{{ CONFLUENCE_PAGES }}"* ]]; then
      echo "$confluence_pages"
    else
      echo "$line"
    fi
  done < "$template_file"

  # Add personal metrics summary at the end
  echo ""
  echo "## Personal Workload Summary"
  echo ""

  local personal_cache="${CACHE_DIR}/eod-personal-metrics.json"

  # Fetch personal metrics - cache for 5 minutes
  # PANK-1798 enhancement #3: Fetch all tickets to show complete status histogram
  if ! is_cache_fresh "$personal_cache" 300; then
    _log "Refreshing personal metrics cache..."
    # Include all statuses to show complete histogram including Done/To Do
    local jql_personal="assignee = \"${ATLASSIAN_USER}\" ORDER BY priority DESC, updated DESC"

    curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
      -H 'Accept: application/json' \
      -G \
      --data-urlencode "jql=${jql_personal}" \
      --data-urlencode "fields=key,priority,status,created,updated" \
      --data-urlencode "maxResults=1000" \
      "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
      -o "$personal_cache" 2>/dev/null
  else
    _log "Using cached personal metrics (fresh)"
  fi

  local total_open
  # Count only non-done tickets for "open" count
  total_open=$(jq '[.issues[] | select(.fields.status.statusCategory.key != "done")] | length' "$personal_cache")

  echo "Total open tickets: $total_open"
  echo ""

  # Status breakdown - PANK-1798 enhancement #3: Use statusCategory for reliable grouping with histogram
  echo "Status Breakdown:"
  echo ""

  # Extract counts using statusCategory for reliable grouping
  local to_do in_progress done
  to_do=$(jq '[.issues[] | select(.fields.status.statusCategory.key == "new")] | length' "$personal_cache")
  in_progress=$(jq '[.issues[] | select(.fields.status.statusCategory.key == "indeterminate")] | length' "$personal_cache")
  done=$(jq '[.issues[] | select(.fields.status.statusCategory.key == "done")] | length' "$personal_cache")

  # Calculate bar widths (scale to 40 characters max)
  local max_val=$to_do
  [ "$in_progress" -gt "$max_val" ] && max_val=$in_progress
  [ "$done" -gt "$max_val" ] && max_val=$done

  # Avoid division by zero
  [ "$max_val" -eq 0 ] && max_val=1

  local todo_w=$((to_do * 40 / max_val))
  local prog_w=$((in_progress * 40 / max_val))
  local done_w=$((done * 40 / max_val))

  # Generate horizontal bars
  printf "To Do       [%2d] " "$to_do"
  printf '%*s' "$todo_w" '' | tr ' ' '█'
  echo ""
  printf "In Progress [%2d] " "$in_progress"
  printf '%*s' "$prog_w" '' | tr ' ' '█'
  echo ""
  printf "Done        [%2d] " "$done"
  printf '%*s' "$done_w" '' | tr ' ' '█'
  echo ""
  echo ""

  # Priority breakdown with histogram
  echo "By Priority:"

  # Get priority counts as tab-separated values
  local max_priority
  max_priority=$(jq -r '.issues | group_by(.fields.priority.name // "No Priority") | map(length) | max // 1' "$personal_cache")

  jq -r '.issues | group_by(.fields.priority.name // "No Priority") | map({priority: .[0].fields.priority.name // "No Priority", count: length}) | sort_by(-.count) | .[] | "\(.priority)\t\(.count)"' "$personal_cache" | while IFS=$'\t' read -r prio_name prio_count; do
    local prio_w=$((prio_count * 40 / max_priority))
    printf "  %-12s [%2d] " "$prio_name" "$prio_count"
    printf '%*s' "$prio_w" '' | tr ' ' '█'
    echo ""
  done
  echo ""

  # Age breakdown with histogram
  echo "By Age:"

  # Get age counts as tab-separated values
  local max_age
  max_age=$(jq -r '.issues | map((.fields.created | sub("\\.[0-9]+\\+0000$"; "Z") | fromdateiso8601) as $created | ((now - $created) / 86400 | floor) as $age | if $age < 7 then "< 7 days" elif $age < 30 then "7-30 days" elif $age < 90 then "30-90 days" else "> 90 days" end) | group_by(.) | map(length) | max // 1' "$personal_cache")

  jq -r '.issues | map((.fields.created | sub("\\.[0-9]+\\+0000$"; "Z") | fromdateiso8601) as $created | ((now - $created) / 86400 | floor) as $age | if $age < 7 then "< 7 days" elif $age < 30 then "7-30 days" elif $age < 90 then "30-90 days" else "> 90 days" end) | group_by(.) | map({bucket: .[0], count: length}) | [{bucket: "< 7 days", count: 0}, {bucket: "7-30 days", count: 0}, {bucket: "30-90 days", count: 0}, {bucket: "> 90 days", count: 0}] as $default | reduce .[] as $item ($default; map(if .bucket == $item.bucket then $item else . end)) | .[] | "\(.bucket)\t\(.count)"' "$personal_cache" | while IFS=$'\t' read -r age_bucket age_count; do
    local age_w=$((age_count * 40 / max_age))
    printf "  %-12s [%2d] " "$age_bucket" "$age_count"
    printf '%*s' "$age_w" '' | tr ' ' '█'
    echo ""
  done
}

# ============================================================================
# Command Aliases - Hyphenated alternatives for CLI-style naming
# ============================================================================

# Jira commands
alias needs-jira-refresh='needs_jira_refresh'
alias fetch-jira-issue='fetch_jira_issue'
alias get-jira-issue='get_jira_issue'

# Confluence commands
alias needs-confluence-refresh='needs_confluence_refresh'
alias fetch-confluence-page='fetch_confluence_page'
alias get-confluence-page='get_confluence_page'

# Confluence source mapping
alias set-confluence-source='set_confluence_source'
alias get-confluence-source='get_confluence_source'
alias list-confluence-sources='list_confluence_sources'

# Search commands
alias search-my-jira-updates='search_my_jira_updates'
alias search-my-confluence-updates='search_my_confluence_updates'

# Jira update commands
alias add-jira-comment='add_jira_comment'
alias update-jira-issue='update_jira_issue'
alias get-jira-transitions='get_jira_transitions'

# Reporting commands
alias eod-report='eod_report'

# Utility
alias jira-helper-cmd='jira_helper_cmd'
alias jira-helper-info='jira_helper_info'
alias jira-info='jira_helper_info'  # Short form
alias jh='jira_helper'  # Ultra-short form for minimal keystrokes
alias self-update='self_update'

# Jira Metrics (hyphenated aliases for consistency)
alias jira-metrics-volume='jira_metrics_volume'
alias jira-metrics-creation='jira_metrics_creation'
alias jira-metrics-age='jira_metrics_age'
alias jira-metrics-priority='jira_metrics_priority'
alias jira-metrics-churn='jira_metrics_churn'
alias jira-metrics-personal='jira_metrics_personal'
alias jira-metrics-painpoints='jira_metrics_painpoints'

# Jira Metrics - Volume & Flow
# Usage: jira_metrics_volume [days] [project]
jira_metrics_volume() {
  local days="${1:-7}"
  local project="${2:-}"

  _source_credentials || return 1

  local project_filter=""
  if [ -n "$project" ]; then
    project_filter="AND project = $project"
  fi

  _log "=== VOLUME & FLOW METRICS (Last ${days} days) ==="
  _log ""

  # Created tickets - cache for 1 hour
  local created_cache="${CACHE_DIR}/metrics-created-${days}d.json"
  if ! is_cache_fresh "$created_cache" 3600; then
    _log "Refreshing created tickets cache..."
    local jql_created="created >= -${days}d ${project_filter} ORDER BY created DESC"
    curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
      -H 'Accept: application/json' \
      -G \
      --data-urlencode "jql=${jql_created}" \
      --data-urlencode "fields=key" \
      --data-urlencode "maxResults=5000" \
      "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
      -o "$created_cache" 2>/dev/null
  else
    _log "Using cached created tickets (fresh)"
  fi
  local created_count
  created_count=$(jq '.issues | length' "$created_cache")

  # Closed tickets - cache for 1 hour
  local closed_cache="${CACHE_DIR}/metrics-closed-${days}d.json"
  if ! is_cache_fresh "$closed_cache" 3600; then
    _log "Refreshing closed tickets cache..."
    local jql_closed="status changed TO (Done, Closed) DURING (-${days}d, now()) ${project_filter}"
    curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
      -H 'Accept: application/json' \
      -G \
      --data-urlencode "jql=${jql_closed}" \
      --data-urlencode "fields=key" \
      --data-urlencode "maxResults=5000" \
      "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
      -o "$closed_cache" 2>/dev/null
  else
    _log "Using cached closed tickets (fresh)"
  fi
  local closed_count
  closed_count=$(jq '.issues | length' "$closed_cache")

  # Current open tickets - cache for 1 hour
  local open_cache="${CACHE_DIR}/metrics-open.json"
  if ! is_cache_fresh "$open_cache" 3600; then
    _log "Refreshing open tickets cache..."
    local jql_open="status NOT IN (Done, Closed) ${project_filter}"
    curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
      -H 'Accept: application/json' \
      -G \
      --data-urlencode "jql=${jql_open}" \
      --data-urlencode "fields=key" \
      --data-urlencode "maxResults=5000" \
      "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
      -o "$open_cache" 2>/dev/null
  else
    _log "Using cached open tickets (fresh)"
  fi
  local open_count
  open_count=$(jq '.issues | length' "$open_cache")

  # Status distribution - cache for 1 hour
  local status_cache="${CACHE_DIR}/metrics-status-dist.json"
  if ! is_cache_fresh "$status_cache" 3600; then
    _log "Refreshing status distribution cache..."
    local jql_open="status NOT IN (Done, Closed) ${project_filter}"
    curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
      -H 'Accept: application/json' \
      -G \
      --data-urlencode "jql=${jql_open}" \
      --data-urlencode "fields=status" \
      --data-urlencode "maxResults=1000" \
      "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
      -o "$status_cache" 2>/dev/null
  else
    _log "Using cached status distribution (fresh)"
  fi

  # Check if we hit the limit
  local created_display="$created_count"
  local closed_display="$closed_count"
  local open_display="$open_count"

  if [ "$(jq -r '.isLast' "$created_cache")" = "false" ]; then
    created_display="${created_count}+"
  fi
  if [ "$(jq -r '.isLast' "$closed_cache")" = "false" ]; then
    closed_display="${closed_count}+"
  fi
  if [ "$(jq -r '.isLast' "$open_cache")" = "false" ]; then
    open_display="${open_count}+"
  fi

  local net_change=$((created_count - closed_count))
  local net_sign=""
  if [ "$net_change" -gt 0 ]; then
    net_sign="+"
  fi

  echo "Created tickets: $created_display"
  echo "Closed tickets:  $closed_display"
  echo "Net change:      ${net_sign}${net_change}"
  echo "Currently open:  $open_display"
  echo ""
  echo "Status distribution:"
  jq -r '.issues | group_by(.fields.status.name) | map({status: .[0].fields.status.name, count: length}) | sort_by(-.count) | .[] | "  \(.status | gsub(" "; "")): \(.count)"' "$status_cache"
}

# Jira Metrics - Ticket Creation Analysis
# Usage: jira_metrics_creation [days]
jira_metrics_creation() {
  local days="${1:-7}"

  _source_credentials || return 1

  _log "=== TICKET CREATION ANALYSIS (Last ${days} days) ==="
  _log ""

  # Fetch created tickets with creator info - cache for 1 hour
  local created_cache="${CACHE_DIR}/metrics-creation-${days}d.json"
  local jql_created="created >= -${days}d ORDER BY created DESC"

  if ! is_cache_fresh "$created_cache" 3600; then
    _log "Refreshing creation analysis cache..."

    # Paginate through all pages until isLast = true
    local next_token=""
    local page_num=0
    local temp_files=""

    _log "Fetching all created tickets (this may take a while)..."

    while true; do
    page_num=$((page_num + 1))
    local page_cache="${CACHE_DIR}/metrics-creation-page${page_num}.json"
    temp_files="$temp_files $page_cache"

    if [ -z "$next_token" ]; then
      curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
        -H 'Accept: application/json' \
        -G \
        --data-urlencode "jql=${jql_created}" \
        --data-urlencode "fields=key,created,creator" \
        --data-urlencode "maxResults=100" \
        "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
        -o "$page_cache" 2>/dev/null
    else
      curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
        -H 'Accept: application/json' \
        -G \
        --data-urlencode "jql=${jql_created}" \
        --data-urlencode "fields=key,created,creator" \
        --data-urlencode "maxResults=100" \
        --data-urlencode "nextPageToken=$next_token" \
        "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
        -o "$page_cache" 2>/dev/null
    fi

    local page_count
    page_count=$(jq '.issues | length' "$page_cache")
    _log "  Page $page_num: fetched $page_count tickets"

    local is_last
    is_last=$(jq -r '.isLast' "$page_cache")
    if [ "$is_last" = "true" ]; then
      _log "Reached last page"
      break
    fi
    next_token=$(jq -r '.nextPageToken' "$page_cache")
  done

    # Combine all pages into one file (use find to avoid command line too long)
    _log "Combining all pages..."
    find "${CACHE_DIR}" -name "metrics-creation-page*.json" -print0 | \
      xargs -0 jq -s '{issues: [.[] | .issues[]]}' > "$created_cache"
    local total_count
    total_count=$(jq '.issues | length' "$created_cache")
    _log "Total tickets fetched: $total_count"
    _log ""
  else
    _log "Using cached creation analysis (fresh)"
    local total_count
    total_count=$(jq '.issues | length' "$created_cache")
    _log "Total tickets in cache: $total_count"
    _log ""
  fi

  echo "Top 10 ticket creators:"
  jq -r '.issues |
    group_by(.fields.creator.displayName // .fields.creator.emailAddress) |
    map({creator: (.[0].fields.creator.displayName // .[0].fields.creator.emailAddress), count: length}) |
    sort_by(-.count) |
    .[:10] |
    .[] |
    "  \(.creator): \(.count)"' "$created_cache"

  echo ""
  echo "Tickets created by date:"
  jq -r '.issues |
    map(.fields.created | sub("\\.[0-9]+\\+0000$"; "Z") | fromdateiso8601 | strftime("%Y-%m-%d")) |
    group_by(.) |
    map({date: .[0], count: length}) |
    sort_by(.date) |
    reverse |
    .[] |
    "  \(.date): \(.count)"' "$created_cache"
}

# Age & Staleness metrics
jira_metrics_age() {
  local project="${1:-}"
  local max_pages="${2:-10}"  # Default to 10 pages (1000 tickets) for testing

  _source_credentials || return 1

  local project_filter=""
  if [ -n "$project" ]; then
    project_filter="AND project = $project"
  fi

  _log "=== AGE & STALENESS METRICS ==="
  _log "(Excluding automated security scanner tickets: Snyk-Jira, Wiz-Jira)"
  _log ""

  # Fetch all open tickets with created date and creator field - cache for 6 hours (expensive query)
  local open_cache_raw="${CACHE_DIR}/metrics-age-open-raw.json"
  local open_cache="${CACHE_DIR}/metrics-age-open.json"
  local jql_open="status NOT IN (Done, Closed, Resolved) ${project_filter} ORDER BY created ASC"

  if ! is_cache_fresh "$open_cache" 21600; then
    _log "Refreshing age metrics cache..."

    # Paginate through all open tickets
    local next_token=""
    local page_num=0

    _log "Fetching all open tickets..."

    while true; do
    page_num=$((page_num + 1))
    local page_cache="${CACHE_DIR}/metrics-age-page${page_num}.json"

    if [ -z "$next_token" ]; then
      curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
        -H 'Accept: application/json' \
        -G \
        --data-urlencode "jql=${jql_open}" \
        --data-urlencode "fields=key,created,status,creator" \
        --data-urlencode "maxResults=100" \
        "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
        -o "$page_cache" 2>/dev/null
    else
      curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
        -H 'Accept: application/json' \
        -G \
        --data-urlencode "jql=${jql_open}" \
        --data-urlencode "fields=key,created,status,creator" \
        --data-urlencode "maxResults=100" \
        --data-urlencode "nextPageToken=$next_token" \
        "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
        -o "$page_cache" 2>/dev/null
    fi

    local page_count
    page_count=$(jq '.issues | length' "$page_cache")
    _log "  Page $page_num: fetched $page_count tickets"

    # Check if we've hit max pages limit
    if [ "$max_pages" != "all" ] && [ "$page_num" -ge "$max_pages" ]; then
      _log "Reached max pages limit ($max_pages)"
      break
    fi

    local is_last
    is_last=$(jq -r '.isLast' "$page_cache")
    if [ "$is_last" = "true" ]; then
      _log "Reached last page"
      break
    fi
    next_token=$(jq -r '.nextPageToken' "$page_cache")
  done

    # Combine all pages
    _log "Combining all pages..."
    find "${CACHE_DIR}" -name "metrics-age-page*.json" -print0 | \
      xargs -0 jq -s '{issues: [.[] | .issues[]]}' > "$open_cache_raw"

    # Filter out automated security scanner tickets
    _log "Filtering out automated scanner tickets..."
    jq '{issues: [.issues[] | select(
      (.fields.creator.displayName // .fields.creator.emailAddress) != "Snyk-Jira Integration" and
      (.fields.creator.displayName // .fields.creator.emailAddress) != "Wiz-Jira Integration"
    )]}' "$open_cache_raw" > "$open_cache"

    local total_count
    total_count=$(jq '.issues | length' "$open_cache")
    _log "Total open tickets (excluding scanners): $total_count"
    _log ""
  else
    _log "Using cached age metrics (fresh)"
    local total_count
    total_count=$(jq '.issues | length' "$open_cache")
    _log "Total tickets in cache: $total_count"
    _log ""
  fi

  # Calculate age metrics
  echo "Average age of open tickets:"
  local avg_age
  avg_age=$(jq -r '
    [.issues[] |
      (.fields.created | sub("\\.[0-9]+\\+0000$"; "Z") | fromdateiso8601) as $created |
      ((now - $created) / 86400)
    ] |
    add / length |
    floor
  ' "$open_cache")
  echo "  $avg_age days"
  echo ""

  echo "Tickets by age bucket:"
  jq -r '
    .issues |
    map(
      (.fields.created | sub("\\.[0-9]+\\+0000$"; "Z") | fromdateiso8601) as $created |
      ((now - $created) / 86400 | floor) as $age |
      if $age < 30 then "< 30 days"
      elif $age < 60 then "30-60 days"
      elif $age < 90 then "60-90 days"
      else "> 90 days"
      end
    ) |
    group_by(.) |
    map({bucket: .[0], count: length}) |
    [
      {bucket: "< 30 days", count: 0},
      {bucket: "30-60 days", count: 0},
      {bucket: "60-90 days", count: 0},
      {bucket: "> 90 days", count: 0}
    ] as $default |
    reduce .[] as $item ($default;
      map(if .bucket == $item.bucket then $item else . end)
    ) |
    .[] |
    "  \(.bucket): \(.count)"
  ' "$open_cache"
  echo ""

  echo "Oldest open tickets (top 10):"
  jq -r '
    .issues |
    map(
      (.fields.created | sub("\\.[0-9]+\\+0000$"; "Z") | fromdateiso8601) as $created |
      {
        key: .key,
        status: .fields.status.name,
        age: ((now - $created) / 86400 | floor),
        created: ($created | strftime("%Y-%m-%d"))
      }
    ) |
    sort_by(-.age) |
    .[:10] |
    .[] |
    "  \(.key) | \(.status) | \(.age) days | Created: \(.created)"
  ' "$open_cache"

  # Clean up temp page files
  rm -f "${CACHE_DIR}"/metrics-age-page*.json
}

# Priority Health metrics
jira_metrics_priority() {
  local days="${1:-30}"
  local project="${2:-}"

  _source_credentials || return 1

  local project_filter=""
  if [ -n "$project" ]; then
    project_filter="AND project = $project"
  fi

  _log "=== PRIORITY HEALTH METRICS (Last ${days} days) ==="
  _log ""

  # Fetch open tickets with priority info - cache for 1 hour
  local priority_cache="${CACHE_DIR}/metrics-priority-${days}d.json"

  if ! is_cache_fresh "$priority_cache" 3600; then
    _log "Refreshing priority metrics cache..."
    local jql_priority="status NOT IN (Done, Closed, Resolved) ${project_filter} ORDER BY priority DESC, created ASC"

    curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
      -H 'Accept: application/json' \
      -G \
      --data-urlencode "jql=${jql_priority}" \
      --data-urlencode "fields=key,priority,created,status" \
      --data-urlencode "maxResults=5000" \
      "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
      -o "$priority_cache" 2>/dev/null
  else
    _log "Using cached priority metrics (fresh)"
  fi

  local total_count
  total_count=$(jq '.issues | length' "$priority_cache")
  _log "Total tickets: $total_count"
  _log ""

  # Priority distribution
  echo "Priority distribution (open tickets):"
  jq -r '
    .issues |
    group_by(.fields.priority.name // "No Priority") |
    map({priority: .[0].fields.priority.name // "No Priority", count: length}) |
    sort_by(-.count) |
    .[] |
    "  \(.priority): \(.count)"
  ' "$priority_cache"
  echo ""

  # High priority tickets open > 30 days
  echo "High/Critical priority tickets open > 30 days:"
  local high_old_count
  high_old_count=$(jq -r '
    [.issues[] |
      select(
        (.fields.priority.name == "Critical" or .fields.priority.name == "High") and
        ((.fields.created | sub("\\.[0-9]+\\+0000$"; "Z") | fromdateiso8601) as $created |
         ((now - $created) / 86400) > 30)
      )
    ] | length
  ' "$priority_cache")
  echo "  Count: $high_old_count"

  if [ "$high_old_count" -gt 0 ]; then
    echo ""
    echo "  Top 10 oldest high priority tickets:"
    jq -r '
      [.issues[] |
        select(.fields.priority.name == "Critical" or .fields.priority.name == "High") |
        (.fields.created | sub("\\.[0-9]+\\+0000$"; "Z") | fromdateiso8601) as $created |
        {
          key: .key,
          priority: .fields.priority.name,
          status: .fields.status.name,
          age: ((now - $created) / 86400 | floor)
        }
      ] |
      sort_by(-.age) |
      .[0:10] |
      .[] |
      "    \(.key) - [\(.priority)] \(.status) - \(.age) days"
    ' "$priority_cache"
  fi
  echo ""

  # Tickets with priority changes in last N days (requires changelog)
  echo "Note: Priority change tracking requires changelog data (not yet implemented)"
}

# Churn & Turbulence metrics
jira_metrics_churn() {
  local days="${1:-30}"
  local project="${2:-}"

  _source_credentials || return 1

  local project_filter=""
  if [ -n "$project" ]; then
    project_filter="AND project = $project"
  fi

  _log "=== CHURN & TURBULENCE METRICS (Last ${days} days) ==="
  _log ""

  # Tickets with status changes in the time period - cache for 1 hour
  local churn_cache="${CACHE_DIR}/metrics-churn-${days}d.json"

  if ! is_cache_fresh "$churn_cache" 3600; then
    _log "Refreshing churn metrics cache..."
    local jql_churn="updated >= -${days}d ${project_filter} ORDER BY updated DESC"

    curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
      -H 'Accept: application/json' \
      -G \
      --data-urlencode "jql=${jql_churn}" \
      --data-urlencode "fields=key,status,created,updated,resolutiondate" \
      --data-urlencode "maxResults=5000" \
      "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
      -o "$churn_cache" 2>/dev/null
  else
    _log "Using cached churn metrics (fresh)"
  fi

  local total_count
  total_count=$(jq '.issues | length' "$churn_cache")
  _log "Total tickets: $total_count"
  _log ""

  # Tickets reopened (has resolutiondate but currently open)
  echo "Tickets reopened (resolved then reopened):"
  local reopened_count
  reopened_count=$(jq -r '
    [.issues[] |
      select(
        .fields.resolutiondate != null and
        (.fields.status.name != "Done" and .fields.status.name != "Closed" and .fields.status.name != "Resolved")
      )
    ] | length
  ' "$churn_cache")
  echo "  Count: $reopened_count"

  if [ "$reopened_count" -gt 0 ]; then
    echo ""
    echo "  Sample reopened tickets:"
    jq -r '
      [.issues[] |
        select(
          .fields.resolutiondate != null and
          (.fields.status.name != "Done" and .fields.status.name != "Closed" and .fields.status.name != "Resolved")
        )
      ] |
      .[0:10] |
      .[] |
      "    \(.key) - \(.fields.status.name)"
    ' "$churn_cache"
  fi
  echo ""

  # Activity metrics
  echo "Activity summary:"
  echo "  Total tickets updated in last ${days} days: $total_count"

  local avg_age
  avg_age=$(jq -r '
    [.issues[] |
      (.fields.created | sub("\\.[0-9]+\\+0000$"; "Z") | fromdateiso8601) as $created |
      ((now - $created) / 86400)
    ] |
    add / length |
    floor
  ' "$churn_cache")
  echo "  Average age of updated tickets: $avg_age days"
  echo ""

  echo "Note: Status change frequency requires changelog data (not yet implemented)"
}

# Personal Metrics
jira_metrics_personal() {
  local user_email="${1:-}"

  _source_credentials || return 1

  # Use current user if not specified
  if [ -z "$user_email" ]; then
    user_email="${ATLASSIAN_USER}"
  fi

  _log "=== PERSONAL METRICS FOR: ${user_email} ==="
  _log ""

  # Fetch user's tickets - cache for 5 minutes
  local personal_cache="${CACHE_DIR}/metrics-personal-${user_email}.json"

  if ! is_cache_fresh "$personal_cache" 300; then
    _log "Refreshing personal metrics cache..."
    local jql_personal="assignee = \"${user_email}\" AND status NOT IN (Done, Closed, Resolved) ORDER BY priority DESC, created ASC"

    curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
      -H 'Accept: application/json' \
      -G \
      --data-urlencode "jql=${jql_personal}" \
      --data-urlencode "fields=key,priority,status,created,updated" \
      --data-urlencode "maxResults=1000" \
      "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
      -o "$personal_cache" 2>/dev/null
  else
    _log "Using cached personal metrics (fresh)"
  fi

  local total_count
  total_count=$(jq '.issues | length' "$personal_cache")
  _log "Total open tickets: $total_count"
  _log ""

  # My tickets by status
  echo "Your tickets by status:"
  jq -r '
    .issues |
    group_by(.fields.status.name) |
    map({status: .[0].fields.status.name, count: length}) |
    sort_by(-.count) |
    .[] |
    "  \(.status): \(.count)"
  ' "$personal_cache"
  echo ""

  # My tickets by priority
  echo "Your tickets by priority:"
  jq -r '
    .issues |
    group_by(.fields.priority.name // "No Priority") |
    map({priority: .[0].fields.priority.name // "No Priority", count: length}) |
    sort_by(-.count) |
    .[] |
    "  \(.priority): \(.count)"
  ' "$personal_cache"
  echo ""

  # Age breakdown
  echo "Your tickets by age:"
  jq -r '
    .issues |
    map(
      (.fields.created | sub("\\.[0-9]+\\+0000$"; "Z") | fromdateiso8601) as $created |
      ((now - $created) / 86400 | floor) as $age |
      if $age < 7 then "< 7 days"
      elif $age < 30 then "7-30 days"
      elif $age < 90 then "30-90 days"
      else "> 90 days"
      end
    ) |
    group_by(.) |
    map({bucket: .[0], count: length}) |
    [
      {bucket: "< 7 days", count: 0},
      {bucket: "7-30 days", count: 0},
      {bucket: "30-90 days", count: 0},
      {bucket: "> 90 days", count: 0}
    ] as $default |
    reduce .[] as $item ($default;
      map(if .bucket == $item.bucket then $item else . end)
    ) |
    .[] |
    "  \(.bucket): \(.count)"
  ' "$personal_cache"
  echo ""

  # Oldest tickets
  echo "Your 10 oldest open tickets:"
  jq -r '
    .issues |
    map(
      (.fields.created | sub("\\.[0-9]+\\+0000$"; "Z") | fromdateiso8601) as $created |
      {
        key: .key,
        status: .fields.status.name,
        priority: (.fields.priority.name // "None"),
        age: ((now - $created) / 86400 | floor)
      }
    ) |
    sort_by(-.age) |
    .[0:10] |
    .[] |
    "  \(.key) - [\(.priority)] \(.status) - \(.age) days"
  ' "$personal_cache"
}

# Team Pain Points
jira_metrics_painpoints() {
  local project="${1:-}"

  _source_credentials || return 1

  local project_filter=""
  if [ -n "$project" ]; then
    project_filter="AND project = $project"
  fi

  _log "=== TEAM PAIN POINTS ==="
  _log ""

  # Fetch blocked tickets - cache for 30 minutes
  local blocked_cache="${CACHE_DIR}/metrics-blocked.json"

  if ! is_cache_fresh "$blocked_cache" 1800; then
    _log "Refreshing blocked tickets cache..."
    local jql_blocked="status = Blocked ${project_filter} ORDER BY created ASC"

    curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
      -H 'Accept: application/json' \
      -G \
      --data-urlencode "jql=${jql_blocked}" \
      --data-urlencode "fields=key,assignee,created,summary,priority" \
      --data-urlencode "maxResults=1000" \
      "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
      -o "$blocked_cache" 2>/dev/null
  else
    _log "Using cached blocked tickets (fresh)"
  fi

  local blocked_count
  blocked_count=$(jq '.issues | length' "$blocked_cache")
  _log "Total blocked tickets: $blocked_count"
  _log ""

  echo "Blocked tickets summary:"
  echo "  Total blocked: $blocked_count"

  if [ "$blocked_count" -gt 0 ]; then
    echo ""
    echo "  Longest blocked tickets (top 10):"
    jq -r '
      .issues |
      map(
        (.fields.created | sub("\\.[0-9]+\\+0000$"; "Z") | fromdateiso8601) as $created |
        {
          key: .key,
          assignee: (.fields.assignee.displayName // .fields.assignee.emailAddress // "Unassigned"),
          days_blocked: ((now - $created) / 86400 | floor),
          priority: (.fields.priority.name // "None")
        }
      ) |
      sort_by(-.days_blocked) |
      .[0:10] |
      .[] |
      "    \(.key) - \(.assignee) - [\(.priority)] \(.days_blocked) days"
    ' "$blocked_cache"

    echo ""
    echo "  Blocked by assignee:"
    jq -r '
      .issues |
      group_by(.fields.assignee.displayName // .fields.assignee.emailAddress // "Unassigned") |
      map({assignee: (.[0].fields.assignee.displayName // .[0].fields.assignee.emailAddress // "Unassigned"), count: length}) |
      sort_by(-.count) |
      .[0:10] |
      .[] |
      "    \(.assignee): \(.count)"
    ' "$blocked_cache"
  fi
  echo ""

  # High priority with no assignee - cache for 30 minutes
  local unassigned_cache="${CACHE_DIR}/metrics-unassigned.json"

  if ! is_cache_fresh "$unassigned_cache" 1800; then
    _log "Refreshing unassigned tickets cache..."
    local jql_unassigned="assignee is EMPTY AND priority IN (Critical, High) AND status NOT IN (Done, Closed, Resolved) ${project_filter}"

    curl -s -u "${ATLASSIAN_USER}:${ATLASSIAN_API_TOKEN}" \
      -H 'Accept: application/json' \
      -G \
      --data-urlencode "jql=${jql_unassigned}" \
      --data-urlencode "fields=key,priority,status,created" \
      --data-urlencode "maxResults=100" \
      "https://${ATLASSIAN_SITE_URL}/rest/api/3/search/jql" \
      -o "$unassigned_cache" 2>/dev/null
  else
    _log "Using cached unassigned tickets (fresh)"
  fi

  local unassigned_count
  unassigned_count=$(jq '.issues | length' "$unassigned_cache")

  echo "High priority unassigned tickets: $unassigned_count"

  if [ "$unassigned_count" -gt 0 ]; then
    echo ""
    echo "  Sample (first 10):"
    jq -r '
      .issues[0:10] |
      .[] |
      "    \(.key) - [\(.fields.priority.name)] \(.fields.status.name)"
    ' "$unassigned_cache"
  fi
}

# ============================================================================
# Main Dispatcher Command
# ============================================================================

# Main jira-helper dispatcher command
# Usage: jira-helper <command> [args...]
# Note: Function name is jira_helper (underscore) but use jira-helper (hyphen) alias
jira_helper() {
  local command="$1"
  shift

  case "$command" in
    # Help and utility commands
    help|--help|-h|"")
      jira_helper_cmd "$@"
      ;;
    info|cache-info)
      jira_helper_info "$@"
      ;;
    version)
      jira_helper_version "$@"
      ;;
    update|self-update)
      self_update "$@"
      ;;

    # NEW: Hierarchical workspace command
    workspace)
      local subcommand="$1"
      shift
      case "$subcommand" in
        list)
          list_workspaces "$@"
          ;;
        discover)
          discover_workspaces "$@"
          ;;
        stats)
          workspace_stats "$@"
          ;;
        cleanup)
          cleanup_workspaces "$@"
          ;;
        "")
          # Default to list
          list_workspaces "$@"
          ;;
        *)
          echo "Error: Unknown workspace subcommand '${subcommand}'" >&2
          echo "Usage: jira-helper workspace <list|discover|stats|cleanup>" >&2
          return 1
          ;;
      esac
      ;;

    # Reporting commands
    eod|eod-report)
      eod_report "$@"
      ;;

    # NEW: Hierarchical issue commands (Option 3)
    issue)
      local subcommand="$1"
      # Strip completion hints for validation (everything after colon)
      local subcommand_clean="${subcommand%%:*}"

      # Check for known subcommands first
      if [ "$subcommand" = "open" ]; then
        shift
        open_jira_issue "$@"
      elif [ "$subcommand" = "transitions" ]; then
        shift
        get_jira_transitions "$@"
      elif [ "$subcommand" = "comment" ]; then
        shift
        add_jira_comment "$@"
      elif [ "$subcommand" = "update" ]; then
        shift
        update_jira_issue "$@"
      elif [ "$subcommand" = "update-comment" ]; then
        shift
        update_jira_comment "$@"
      elif [ "$subcommand" = "last-comment-id" ]; then
        shift
        get_last_comment_id "$@"
      elif [ "$subcommand" = "create" ]; then
        shift
        create_jira_ticket "$@"
      elif [ "$subcommand" = "create-subtask" ]; then
        shift
        create_jira_subtask "$@"
      # Then check if it looks like a ticket key or URL (check cleaned version)
      elif [[ "$subcommand_clean" =~ ^[A-Z]+-[0-9]+$ ]] || [[ "$subcommand" =~ /browse/ ]]; then
        get_jira_issue "$@"
      elif [ -z "$subcommand" ]; then
        echo "Error: Usage: jira-helper issue <key|URL> [--json]" >&2
        echo "       jira-helper issue transitions <key|URL>" >&2
        echo "       jira-helper issue comment <key|URL> \"comment text\"" >&2
        echo "       jira-helper issue update <key|URL> <field> <value>" >&2
        echo "       jira-helper issue update-comment <key|URL> <comment-id> \"text\"" >&2
        echo "       jira-helper issue last-comment-id <key|URL>" >&2
        echo "       jira-helper issue create <project> \"summary\" \"description\" [type]" >&2
        echo "       jira-helper issue create-subtask <parent-key> \"summary\" \"description\"" >&2
        return 1
      else
        # Invalid input - show error
        echo "Error: Invalid ticket key or subcommand '${subcommand}'" >&2
        echo "Usage: jira-helper issue <key|URL> [--json]" >&2
        echo "       jira-helper issue transitions <key|URL>" >&2
        echo "       jira-helper issue comment <key|URL> \"comment text\"" >&2
        echo "       jira-helper issue update <key|URL> <field> <value>" >&2
        echo "       jira-helper issue update-comment <key|URL> <comment-id> \"text\"" >&2
        echo "       jira-helper issue last-comment-id <key|URL>" >&2
        echo "       jira-helper issue create <project> \"summary\" \"description\" [type]" >&2
        echo "       jira-helper issue create-subtask <parent-key> \"summary\" \"description\"" >&2
        echo "" >&2
        echo "Ticket keys must be in format: PROJECT-123 (e.g., PANK-1485)" >&2
        return 1
      fi
      ;;

    # NEW: Hierarchical issues commands (plural = search/list)
    issues)
      local subcommand="$1"
      shift
      case "$subcommand" in
        search)
          search_my_jira_updates "$@"
          ;;
        mine)
          search_my_jira_updates "$@"
          ;;
        "")
          # Default to mine
          search_my_jira_updates "$@"
          ;;
        *)
          echo "Error: Unknown issues subcommand '${subcommand}'" >&2
          echo "Usage: jira-helper issues [search|mine] [days]" >&2
          return 1
          ;;
      esac
      ;;

    # NEW: Hierarchical doc command (Confluence single page operations)
    doc)
      local subcommand="$1"
      # Check for subcommands first
      if [ "$subcommand" = "open" ]; then
        shift
        open_confluence_page "$@"
      elif [ "$subcommand" = "source" ]; then
        shift
        get_confluence_source "$@"
      elif [ "$subcommand" = "set-source" ]; then
        shift
        set_confluence_source "$@"
      elif [ "$subcommand" = "update" ]; then
        shift
        update_confluence_page "$@"
      elif [ "$subcommand" = "replace" ]; then
        shift
        replace_confluence_page "$@"
      # Then check if it looks like a page ID (numeric)
      elif [[ "$subcommand" =~ ^[0-9]+$ ]]; then
        get_confluence_page "$@"
      elif [ -z "$subcommand" ]; then
        echo "Error: Usage: jira-helper doc <page-id|source|set-source|update|replace>" >&2
        echo "  jira-helper doc <page-id>                    # Get page" >&2
        echo "  jira-helper doc source <page-id>             # Get source path" >&2
        echo "  jira-helper doc set-source <page-id> <path>  # Set source path" >&2
        echo "  jira-helper doc update <page-id> \"content\"   # Update page" >&2
        echo "  jira-helper doc replace <page-id> \"content\"  # Replace page" >&2
        return 1
      else
        echo "Error: Invalid doc command or page-id '${subcommand}'" >&2
        echo "Usage: jira-helper doc <page-id|source|set-source|update|replace>" >&2
        return 1
      fi
      ;;

    # NEW: Hierarchical docs command (Confluence search/list operations)
    docs)
      local subcommand="$1"
      shift
      case "$subcommand" in
        sources)
          list_confluence_sources "$@"
          ;;
        mine)
          search_my_confluence_updates "$@"
          ;;
        "")
          # Default to mine
          search_my_confluence_updates "$@"
          ;;
        *)
          echo "Error: Unknown docs subcommand '${subcommand}'" >&2
          echo "Usage: jira-helper docs [mine|sources] [days]" >&2
          return 1
          ;;
      esac
      ;;

    # NEW: Hierarchical metrics command
    metrics)
      local subcommand="$1"
      shift
      case "$subcommand" in
        volume)
          jira_metrics_volume "$@"
          ;;
        creation)
          jira_metrics_creation "$@"
          ;;
        age)
          jira_metrics_age "$@"
          ;;
        priority)
          jira_metrics_priority "$@"
          ;;
        churn)
          jira_metrics_churn "$@"
          ;;
        personal)
          jira_metrics_personal "$@"
          ;;
        painpoints)
          jira_metrics_painpoints "$@"
          ;;
        "")
          echo "Error: Usage: jira-helper metrics <subcommand>" >&2
          echo "" >&2
          echo "Available subcommands:" >&2
          echo "  volume       - Volume and flow metrics" >&2
          echo "  creation     - Creation metrics and trends" >&2
          echo "  age          - Age distribution" >&2
          echo "  priority     - Priority health" >&2
          echo "  churn        - Reopened tickets" >&2
          echo "  personal     - Your workload" >&2
          echo "  painpoints   - Blocked and unassigned" >&2
          return 1
          ;;
        *)
          echo "Error: Unknown metrics subcommand '${subcommand}'" >&2
          echo "Usage: jira-helper metrics <volume|creation|age|priority|churn|personal|painpoints>" >&2
          return 1
          ;;
      esac
      ;;

    # OLD: Legacy standalone commands (kept for backward compatibility, but hidden from help)
    get-issue|show-issue)
      get_jira_issue "$@"
      ;;
    search-issues|search)
      search_my_jira_updates "$@"
      ;;
    get-page|page)
      get_confluence_page "$@"
      ;;
    search-pages)
      search_my_confluence_updates "$@"
      ;;
    set-source)
      set_confluence_source "$@"
      ;;
    get-source)
      get_confluence_source "$@"
      ;;
    list-sources)
      list_confluence_sources "$@"
      ;;
    get-transitions|transitions)
      get_jira_transitions "$@"
      ;;

    *)
      echo "Error: Unknown command '${command}'" >&2
      echo "" >&2
      echo "Usage: jira-helper <command> [args...]" >&2
      echo "" >&2
      echo "Common commands:" >&2
      echo "  help              Show full help" >&2
      echo "  info              Show working directory info" >&2
      echo "  version           Show version information" >&2
      echo "  workspaces        List all workspaces" >&2
      echo "  update            Update from GitHub" >&2
      echo "  eod               Generate end-of-day report" >&2
      echo "  search            Search your recent Jira updates" >&2
      echo "  my                Show your personal metrics" >&2
      echo "" >&2
      echo "Run 'jira-helper help' for all commands" >&2
      return 1
      ;;
  esac
}

# Create jira-helper function (hyphens work in function declarations with 'function' keyword)
# This allows both jira-helper and jira_helper to work
function jira-helper() {
  jira_helper "$@"
}

# ============================================================================
# Investigation Documentation Functions
# ============================================================================

# Publish investigation document to Confluence
# Creates child page under Platform Investigation Workflow parent page
# Supports both markdown files and Jira ticket keys
# Usage: jira_investigation <markdown-file-or-jira-ticket> [parent-page-id]
# Example: jira_investigation PANK-2079-investigation.md
# Example: jira_investigation PANK-2079
jira_investigation() {
  local input="$1"
  local parent_page_id="${2:-4325900356}"  # Default: Platform Investigation Workflow page

  if [ -z "$input" ]; then
    echo "Error: Usage: jira_investigation <markdown-file-or-jira-ticket> [parent-page-id]" >&2
    echo "Examples:" >&2
    echo "  jira_investigation PANK-2079-investigation.md" >&2
    echo "  jira_investigation PANK-2079" >&2
    echo "  jira_investigation PANK-2079-investigation.md 4220125196" >&2
    return 1
  fi

  # Use safe credentials loading
  _source_credentials || return 1

  # Get parent page details to determine space key
  echo "Fetching parent page details..."
  local parent_page_json
  parent_page_json=$(JIRA_HELPER_QUIET=true get_confluence_page "$parent_page_id" 2>/dev/null)

  if [ $? -ne 0 ] || [ -z "$parent_page_json" ]; then
    echo "Error: Failed to fetch parent page $parent_page_id" >&2
    return 1
  fi

  # Extract space key from _expandable.space path (format: /rest/api/space/KEY)
  local space_key
  space_key=$(echo "$parent_page_json" | jq -r '._expandable.space' | grep -oE '[A-Z]+$')

  if [ -z "$space_key" ]; then
    echo "Error: Could not determine space key from parent page" >&2
    return 1
  fi

  echo "Using Confluence space: $space_key"

  local title=""
  local content=""
  local jira_key=""

  # Determine if input is a file or a Jira ticket key
  if [ -f "$input" ]; then
    # Input is a markdown file
    echo "Reading investigation document: $input"
    content=$(cat "$input")

    # Extract Jira key from filename (e.g., PANK-2079-investigation.md -> PANK-2079)
    local filename=$(basename "$input")
    jira_key=$(echo "$filename" | grep -oE '[A-Z]+-[0-9]+' | head -1)

    # Use filename without extension as title if Jira key found, otherwise use full filename
    if [ -n "$jira_key" ]; then
      title="$jira_key: $(echo "$filename" | sed "s/${jira_key}-\?//" | sed 's/\.[^.]*$//' | tr '-' ' ' | sed 's/\b\(.\)/\u\1/g')"
    else
      title=$(echo "$filename" | sed 's/\.[^.]*$//' | tr '-' ' ' | sed 's/\b\(.\)/\u\1/g')
    fi
  else
    # Input might be a Jira ticket key
    jira_key=$(echo "$input" | grep -oE '[A-Z]+-[0-9]+')

    if [ -z "$jira_key" ]; then
      echo "Error: Input is neither a valid file nor a Jira ticket key" >&2
      return 1
    fi

    echo "Fetching Jira ticket: $jira_key"

    # Get Jira ticket details
    local ticket_json
    ticket_json=$(get_jira_issue "$jira_key" --json 2>/dev/null)

    if [ $? -ne 0 ] || [ -z "$ticket_json" ]; then
      echo "Error: Failed to fetch ticket $jira_key" >&2
      return 1
    fi

    local summary=$(echo "$ticket_json" | jq -r '.fields.summary // "Unknown"')
    title="$jira_key: $summary"

    # Create basic investigation document from ticket
    content="# $title

**Jira Ticket:** https://uniphore.atlassian.net/browse/$jira_key
**Date:** $(date +%Y-%m-%d)
**Status:** Investigating

## Problem Summary

$summary

## Investigation Notes

(Add investigation details here)

## Next Steps

(Add follow-up actions here)
"
  fi

  echo "Title: $title"
  echo "Parent Page ID: $parent_page_id"

  # Check if a child page with this Jira key already exists under the parent
  if [ -n "$jira_key" ]; then
    echo "Checking for existing investigation page for $jira_key..."

    # Search for child pages under parent that start with the Jira key
    local existing_page_id=""
    local child_pages_json
    child_pages_json=$(curl -s -X GET \
      -H "Authorization: Basic $(echo -n "${ATLASSIAN_USER}:${ATLASSIAN_DOCS}" | base64)" \
      -H "Content-Type: application/json" \
      "${ATLASSIAN_SITE_URL}/rest/api/content/${parent_page_id}/child/page?limit=100" 2>/dev/null)

    if [ -n "$child_pages_json" ]; then
      # Look for a page title that starts with the Jira key
      existing_page_id=$(echo "$child_pages_json" | jq -r ".results[] | select(.title | startswith(\"${jira_key}\")) | .id" 2>/dev/null | head -1)

      if [ -n "$existing_page_id" ]; then
        local existing_title=$(echo "$child_pages_json" | jq -r ".results[] | select(.id == \"${existing_page_id}\") | .title" 2>/dev/null)
        echo "Found existing page: $existing_title (ID: $existing_page_id)"
        echo "Creating update as nested child page under existing investigation..."

        # Create nested update under the existing page
        local update_title="Update: $(date +%Y-%m-%d)"
        local new_page_id
        new_page_id=$(create_confluence_page "$space_key" "$update_title" "$content" "$existing_page_id" 2>/dev/null)

        if [ $? -eq 0 ] && [ -n "$new_page_id" ]; then
          echo "Successfully created update page (ID: $new_page_id)"
          echo "View at: ${ATLASSIAN_SITE_URL}/wiki/spaces/${space_key}/pages/${new_page_id}"
          return 0
        else
          echo "Error: Failed to create update page" >&2
          return 1
        fi
      fi
    fi
  fi

  # No existing page found, create new child page under parent
  echo "Creating new investigation page..."
  local new_page_id
  new_page_id=$(create_confluence_page "$space_key" "$title" "$content" "$parent_page_id" 2>/dev/null)

  if [ $? -eq 0 ] && [ -n "$new_page_id" ]; then
    echo "Successfully created investigation page (ID: $new_page_id)"
    echo "View at: ${ATLASSIAN_SITE_URL}/wiki/spaces/${space_key}/pages/${new_page_id}"

    # If we have a Jira key, add comment to the ticket with the Confluence link
    if [ -n "$jira_key" ]; then
      echo "Adding Confluence link to Jira ticket..."
      local comment="Investigation documented in Confluence: ${ATLASSIAN_SITE_URL}/wiki/spaces/${space_key}/pages/${new_page_id}"
      add_jira_comment "$jira_key" "$comment" --yes >/dev/null 2>&1
    fi

    return 0
  else
    echo "Error: Failed to create investigation page" >&2
    return 1
  fi
}

# ============================================================================
# Git Analysis Functions
# ============================================================================

# Generate remediation document for InfoSec APPCLD tickets
# Analyzes ticket, discovers related files, suggests reviewers, creates comprehensive doc
# Usage: remediate <ticket-key>
# Example: remediate APPCLD-51251
remediate() {
  local ticket_key="$1"

  if [ -z "$ticket_key" ]; then
    echo "Error: Usage: remediate <ticket-key>" >&2
    echo "Example: remediate APPCLD-51251" >&2
    return 1
  fi

  # Use safe credentials loading
  _source_credentials || return 1

  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  echo "Remediate: Analyzing $ticket_key"
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  echo ""

  # Fetch ticket details
  echo "[1/5] Fetching ticket from Jira..."
  local ticket_json
  ticket_json=$(get_jira_issue "$ticket_key" --json 2>/dev/null)

  if [ $? -ne 0 ] || [ -z "$ticket_json" ]; then
    echo "Error: Failed to fetch ticket $ticket_key" >&2
    return 1
  fi

  local summary=$(echo "$ticket_json" | jq -r '.fields.summary // "Unknown"')
  local description_raw=$(echo "$ticket_json" | jq -r '.fields.description // ""')
  local priority=$(echo "$ticket_json" | jq -r '.fields.priority.name // "Unknown"')
  local status=$(echo "$ticket_json" | jq -r '.fields.status.name // "Unknown"')

  # Extract text from ADF format if description is JSON
  local description="$description_raw"
  local description_markdown=""

  if echo "$description_raw" | jq -e '.type == "doc"' >/dev/null 2>&1; then
    # Description is in ADF format - extract text for analysis
    description=$(echo "$description_raw" | jq -r '.. | select(.type? == "text") | .text' | tr '\n' ' ')

    # Try to convert ADF to better formatted markdown for display
    # Extract key sections from ADF structure
    local cluster_name=$(echo "$description_raw" | jq -r '.. | select(.type? == "text" and (.text? | contains("uniphore-"))) | .text' | grep -oE 'uniphore-[a-z0-9-]+-gke' | head -1)
    local affected_file=$(echo "$description_raw" | jq -r '.. | select(.type? == "text" and (.text? | contains(".yaml"))) | .text' | grep -oE '[a-z0-9._-]+\.yaml' | head -1)
    local resource_names=$(echo "$description_raw" | jq -r '.. | select(.type? == "text" and (.text? | contains("reconciler") or contains("velero"))) | .text' | grep -oE '(Cluster-reconciler[^ ]*|velero-server)')

    # Build structured markdown
    if [ -n "$cluster_name" ] || [ -n "$affected_file" ]; then
      description_markdown+="| Attribute | Value |\n"
      description_markdown+="|-----------|-------|\n"
      [ -n "$cluster_name" ] && description_markdown+="| **Cluster** | $cluster_name |\n"

      # For affected file, try to find it and create GitHub link
      if [ -n "$affected_file" ]; then
        local file_found=""
        local file_github_url=""

        # Skip Kubernetes resource type names (not actual files)
        if [[ "$affected_file" =~ \.k8s\.io\.yaml$ ]]; then
          # This is a kubectl resource type, not a file - skip it
          :
        else
          # Search for the file in discovered_files or local repos
          for search_path in ~/repos/platform-flux ~/repos/platform-terraform; do
            if [ -d "$search_path" ]; then
              file_found=$(find "$search_path" -type f -name "$affected_file" 2>/dev/null | head -1)
              if [ -n "$file_found" ]; then
                local rel_path=${file_found#$HOME/repos/}
                local repo_name=$(echo "$rel_path" | cut -d'/' -f1)
                local file_path=$(echo "$rel_path" | cut -d'/' -f2-)
                file_github_url="https://github.com/uniphore/$repo_name/blob/main/$file_path"
                description_markdown+="| **Affected File** | [\`$rel_path\`]($file_github_url) |\n"
                break
              fi
            fi
          done

          # If not found in repos, just show the filename
          if [ -z "$file_found" ]; then
            description_markdown+="| **Affected File** | \`$affected_file\` |\n"
          fi
        fi
      fi

      if [ -n "$resource_names" ]; then
        description_markdown+="| **Resources** | $(echo "$resource_names" | tr '\n' ', ' | sed 's/,$//' | sed 's/,/, /g') |\n"
      fi
      description_markdown+="\n"
    fi

    # Extract issue description text (skip table data)
    local issue_text=$(echo "$description_raw" | jq -r '.. | select(.type? == "text") | .text' | grep -E '(Background:|Issue:|recommend)' | head -20 | tr '\n' ' ')
    if [ -n "$issue_text" ]; then
      description_markdown+="$issue_text\n\n"
    fi
  fi

  echo "  ✓ Ticket: $summary"
  echo "  ✓ Priority: $priority"
  echo "  ✓ Status: $status"
  echo ""

  # Parse description for key information
  echo "[2/5] Analyzing ticket description..."

  # Extract IPs (pattern: xxx.xxx.xxx.xxx)
  local ips=$(echo "$description" | grep -oE '\b([0-9]{1,3}\.){3}[0-9]{1,3}\b' | sort -u)

  # Extract ports (pattern: xxxx/tcp or port xxxx)
  local ports=$(echo "$description" | grep -oiE '(port[s]?[:]?\s*[0-9]+|[0-9]+/(tcp|udp))' | grep -oE '[0-9]+' | sort -u)

  # Extract file paths (simple heuristic: paths with / and common extensions)
  local files=$(echo "$description" | grep -oE '[a-zA-Z0-9/_.-]+(\.yaml|\.yml|\.json|\.tf|\.sh|\.py)' | sort -u)

  echo "  ✓ Found ${ips:+IPs, }${ports:+ports, }${files:+file references}"
  echo ""

  # Discover related files in local repos
  echo "[3/5] Searching for related files in local repositories..."
  local discovered_files=()
  local search_paths=(
    ~/repos/platform-flux
    ~/repos/cloud-uassist
    ~/repos/platform-terraform
    ~/repos/platform-k8s
  )

  for file_ref in $files; do
    for search_path in "${search_paths[@]}"; do
      if [ -d "$search_path" ]; then
        local found=$(find "$search_path" -type f -name "$(basename "$file_ref")" 2>/dev/null | head -3)
        if [ -n "$found" ]; then
          while IFS= read -r found_file; do
            discovered_files+=("$found_file")
            echo "  ✓ Found: $found_file"
          done <<< "$found"
        fi
      fi
    done
  done

  if [ ${#discovered_files[@]} -eq 0 ]; then
    echo "  ⚠ No files discovered automatically"
  fi
  echo ""

  # Deep analysis phase
  echo "[4/7] Performing deep analysis..."
  local analysis_findings=""
  local config_analysis=""
  local remediation_steps=""

  # Extract key entities from description
  local cluster_name=$(echo "$description" | grep -oE 'uniphore-[a-z0-9-]+-gke' | head -1)
  local resource_names=$(echo "$description" | grep -oE '(Cluster-reconciler|velero-server|[a-z-]+reconciler[a-z-]*)')

  # Search for RBAC configurations if this is an RBAC access control issue
  # Match only when describing RBAC permission problems, not when RBAC is mentioned as a solution
  if echo "$description" | grep -qi "clusterrole.*secret\|serviceaccount.*read.*secret\|secret.*permission.*get\|read all secret\|unauthorized access.*secret"; then
    echo "  → Detected RBAC/Secrets access control issue, searching for ClusterRoleBindings..."

    # Search for ClusterRoleBinding and ClusterRole definitions
    local rbac_files=()
    for search_path in "${search_paths[@]}"; do
      if [ -d "$search_path" ]; then
        while IFS= read -r rbac_file; do
          rbac_files+=("$rbac_file")
          echo "  ✓ Found RBAC config: $rbac_file"
        done < <(find "$search_path" -type f \( -name "*clusterrole*.yaml" -o -name "*rbac*.yaml" \) 2>/dev/null)
      fi
    done

    # Analyze RBAC files for secret permissions
    if [ ${#rbac_files[@]} -gt 0 ]; then
      analysis_findings+="### RBAC Analysis\n\n"
      analysis_findings+="**Files analyzed:** ${#rbac_files[@]} RBAC configuration files\n\n"

      local files_with_secret_perms=0
      for rbac_file in "${rbac_files[@]}"; do
        # Check if file grants secret permissions
        if grep -q "secrets" "$rbac_file" 2>/dev/null; then
          files_with_secret_perms=$((files_with_secret_perms + 1))
          local rel_path=${rbac_file#$HOME/repos/}

          # Extract resource names and verbs
          local verbs=$(grep -A5 "secrets" "$rbac_file" | grep "verbs:" | head -1)

          analysis_findings+="- \`$rel_path\`: Contains secret permissions\n"
          if echo "$verbs" | grep -q "get.*list.*watch"; then
            analysis_findings+="  - ⚠️ Grants broad read access (get, list, watch)\n"
          fi
        fi
      done

      analysis_findings+="\n**Finding:** $files_with_secret_perms files grant secret permissions\n\n"
    fi
  fi

  # Search for Flux configurations if cluster name found
  if [ -n "$cluster_name" ]; then
    echo "  → Searching for cluster configurations for: $cluster_name"

    local flux_configs=()
    for search_path in "${search_paths[@]}"; do
      if [ -d "$search_path" ]; then
        while IFS= read -r flux_file; do
          flux_configs+=("$flux_file")
          discovered_files+=("$flux_file")
          echo "  ✓ Found cluster config: $flux_file"
        done < <(find "$search_path" -type f -name "*.yaml" -exec grep -lF "$cluster_name" {} \; 2>/dev/null | head -5)
      fi
    done
  fi

  # Generate configuration analysis section with GitHub links
  if [ ${#discovered_files[@]} -gt 0 ]; then
    config_analysis+="## Current Configuration\n\n"
    config_analysis+="**Cluster:** ${cluster_name:-Unknown}\n\n"
    config_analysis+="**Related Files:**\n"

    for file in "${discovered_files[@]}"; do
      local rel_path=${file#$HOME/repos/}
      local repo_name=$(echo "$rel_path" | cut -d'/' -f1)
      local file_path=$(echo "$rel_path" | cut -d'/' -f2-)

      # Generate GitHub URL
      local github_url="https://github.com/uniphore/$repo_name/blob/main/$file_path"
      config_analysis+="- [\`$rel_path\`]($github_url)\n"
    done
    config_analysis+="\n"
  fi

  # Generate remediation steps based on issue type (RBAC access control only)
  if echo "$description" | grep -qi "serviceaccount.*read.*secret\|secret.*permission\|read all secret\|rbac.*secret.*access"; then
    remediation_steps+="## Remediation Plan\n\n"
    remediation_steps+="### Immediate Fix ($priority Priority)\n\n"
    remediation_steps+="**Objective:** Restrict ClusterRoleBindings to prevent broad secret access\n\n"
    remediation_steps+="**Steps:**\n"
    remediation_steps+="1. Audit current ClusterRoleBindings:\n"
    remediation_steps+='   ```bash\n'
    remediation_steps+='   kubectl get clusterrolebindings -o yaml | grep -A20 "secrets"\n'
    remediation_steps+='   ```\n\n'
    remediation_steps+="2. Identify service accounts with cluster-wide secret read access\n\n"
    remediation_steps+="3. Scope down permissions:\n"
    remediation_steps+="   - Replace ClusterRoles with namespace-scoped Roles\n"
    remediation_steps+="   - Add \`resourceNames\` to limit to specific secrets\n"
    remediation_steps+="   - Remove unnecessary \`list\` and \`watch\` verbs\n\n"
    remediation_steps+="4. Update Flux configurations:\n"

    if [ ${#discovered_files[@]} -gt 0 ]; then
      for file in "${discovered_files[@]:0:3}"; do
        local rel_path=${file#$HOME/repos/}
        remediation_steps+="   - Edit \`$rel_path\`\n"
      done
    fi
    remediation_steps+="\n"
    remediation_steps+="### Testing Strategy\n\n"
    remediation_steps+="1. Test in non-production cluster first\n"
    remediation_steps+="2. Verify affected services still function:\n"
    remediation_steps+="   - Flux reconciliation\n"
    remediation_steps+="   - Velero backups\n"
    remediation_steps+="3. Monitor for permission errors in logs\n"
    remediation_steps+="4. Gradually roll out to production\n\n"
  fi

  echo "  ✓ Deep analysis complete"
  echo ""

  # Generate reviewer suggestions
  echo "[5/7] Analyzing git history for reviewer suggestions..."
  local reviewers_markdown=""
  local temp_reviewers=$(mktemp)

  if [ ${#discovered_files[@]} -gt 0 ]; then
    # Analyze first discovered file for reviewers
    local primary_file="${discovered_files[0]}"
    local file_dir=$(dirname "$primary_file")

    (cd "$file_dir" && suggest_reviewers "$(basename "$primary_file")" 365) > "$temp_reviewers" 2>/dev/null

    # Convert text table to markdown table
    reviewers_markdown="| Name | Email | Commits | Score | GitHub Status |\n"
    reviewers_markdown+="|------|-------|---------|-------|---------------|\n"

    # Parse the text table output and convert to markdown
    # Table format has 2 separators at start (before/after header) and 1 at end
    local separator_count=0
    local in_table=false
    while IFS= read -r line; do
      # Count separator lines
      if [[ "$line" =~ ^━ ]]; then
        separator_count=$((separator_count + 1))
        # After second separator, we're in the data section
        if [ $separator_count -eq 2 ]; then
          in_table=true
        fi
        # Third separator ends the table
        if [ $separator_count -eq 3 ]; then
          break
        fi
        continue
      fi

      # Skip if not in table data section yet
      if [ "$in_table" = false ]; then
        continue
      fi

      # Skip empty lines
      if [[ -z "$line" ]]; then
        continue
      fi

      # Parse fixed-width columns from suggest_reviewers output
      # Format: Name (col 0-25) Email (col 26-46) Commits (col 47-57) Score (col 58-73) GitHub Status (col 74+)
      local name=$(echo "$line" | cut -c1-25 | xargs)
      local email=$(echo "$line" | cut -c26-46 | xargs)
      local commits=$(echo "$line" | cut -c47-57 | xargs)
      local score=$(echo "$line" | cut -c58-73 | xargs)
      local gh_status=$(echo "$line" | cut -c74- | xargs)

      # Only add row if we have valid email (key identifier)
      if [ -n "$email" ] && [[ "$email" == *"@"* ]]; then
        reviewers_markdown+="| $name | $email | $commits | $score | $gh_status |\n"
      fi
    done < "$temp_reviewers"

    echo "  ✓ Generated reviewer suggestions"
  else
    echo "  ⚠ No files to analyze for reviewers"
  fi
  echo ""

  # Generate markdown document
  echo "[6/7] Generating remediation document..."
  local output_file="$HOME/markdowns/${ticket_key}.md"
  mkdir -p "$HOME/markdowns"

  cat > "$output_file" <<EOF
# ${ticket_key}: ${summary}

| Field | Value |
|-------|-------|
| **Priority** | ${priority} |
| **Status** | ${status} |
| **Ticket** | [${ticket_key}](https://${ATLASSIAN_SITE_URL}/browse/${ticket_key}) |

## Issue

EOF

  # Add formatted description if available, otherwise use raw text
  if [ -n "$description_markdown" ]; then
    echo -e "$description_markdown" >> "$output_file"
  else
    echo "$description" >> "$output_file"
    echo "" >> "$output_file"
  fi

  # Add suggested reviewers section right after Issue
  if [ -n "$reviewers_markdown" ]; then
    cat >> "$output_file" <<EOF

## Suggested Reviewers/Owners

Based on git history analysis:

EOF
    echo -e "$reviewers_markdown" >> "$output_file"
    cat >> "$output_file" <<EOF

**Score calculation:**
- Recent commits weighted higher (exponential decay)
- Half-life: 182 days (commits at this age worth 50%)
- GitHub status: ✓ active, ✗ suspended, Unknown = no GitHub match

EOF
  fi

  # Add analysis findings if generated
  if [ -n "$analysis_findings" ]; then
    echo -e "$analysis_findings" >> "$output_file"
  fi

  # Add affected systems section if IPs/ports found
  if [ -n "$ips" ] || [ -n "$ports" ]; then
    cat >> "$output_file" <<EOF
## Affected Systems

EOF
    if [ -n "$ips" ]; then
      echo "**IPs:**" >> "$output_file"
      echo "$ips" | while read ip; do
        echo "- $ip" >> "$output_file"
      done
      echo "" >> "$output_file"
    fi

    if [ -n "$ports" ]; then
      echo "**Ports:**" >> "$output_file"
      echo "$ports" | while read port; do
        echo "- $port" >> "$output_file"
      done
      echo "" >> "$output_file"
    fi
  fi

  # Add configuration analysis if generated
  if [ -n "$config_analysis" ]; then
    echo -e "$config_analysis" >> "$output_file"
  elif [ ${#discovered_files[@]} -gt 0 ]; then
    # Fallback to simple file list
    cat >> "$output_file" <<EOF
## Related Files

EOF
    for file in "${discovered_files[@]}"; do
      echo "- \`$file\`" >> "$output_file"
    done
    echo "" >> "$output_file"
  fi

  # Add remediation steps if generated
  if [ -n "$remediation_steps" ]; then
    echo -e "$remediation_steps" >> "$output_file"
  else
    # Add template sections for manual completion
    cat >> "$output_file" <<EOF
## Current Configuration

<!-- TODO: Analyze current configuration -->

## Security Findings

| Issue | Current State | Risk |
|-------|---------------|------|
| <!-- TODO --> | <!-- TODO --> | <!-- TODO --> |

## Remediation Plan

### Immediate Fix (P0)

<!-- TODO: Document immediate remediation steps -->

### Testing Strategy

<!-- TODO: Define testing approach -->

### Additional Actions

| Action | Implementation | Priority |
|--------|----------------|----------|
| <!-- TODO --> | <!-- TODO --> | <!-- TODO --> |

EOF
  fi

  # Add references and status
  cat >> "$output_file" <<EOF
## References

- [VAPT Report](<!-- TODO: Add link to VAPT report -->)
- [Kubernetes RBAC Documentation](https://kubernetes.io/docs/reference/access-authn-authz/rbac/)
- [GKE Security Best Practices](https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster)

## Status
- **Created**: $(date +%Y-%m-%d)
- **Status**: Analysis in progress
- **Priority**: ${priority}
EOF

  echo "  ✓ Document saved: $output_file"
  echo ""

  rm -f "$temp_reviewers"

  echo "[7/7] Analysis complete!"
  echo ""

  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  echo "Remediation Document Generated"
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  echo ""
  echo "📄 Document: $output_file"
  echo ""

  # Show what was analyzed
  if [ -n "$cluster_name" ]; then
    echo "✓ Cluster identified: $cluster_name"
  fi
  if [ ${#discovered_files[@]} -gt 0 ]; then
    echo "✓ Files analyzed: ${#discovered_files[@]}"
  fi
  if [ -n "$analysis_findings" ]; then
    echo "✓ RBAC analysis complete"
  fi
  if [ -n "$remediation_steps" ]; then
    echo "✓ Remediation plan generated"
  fi
  echo ""

  echo "Next steps:"
  if [ -n "$remediation_steps" ]; then
    echo "  1. Review generated remediation plan"
    echo "  2. Complete any remaining TODO sections"
    echo "  3. Test changes in non-production environment"
    echo "  4. Update Jira ticket with findings"
  else
    echo "  1. Review and complete TODO sections in the document"
    echo "  2. Analyze configuration details"
    echo "  3. Define remediation steps"
    echo "  4. Update Jira ticket with findings"
  fi
  echo ""
}

# Suggest reviewers/owners for a file or directory based on git history
# Weights contributors by frequency and recency, checks GitHub account status
# Usage: suggest_reviewers <path> [days-back]
# Example: suggest_reviewers platform-flux/apps-v2/environments/prod/clusters/qc/uassist.yaml
# Example: suggest_reviewers cloud-uassist/ 180
suggest_reviewers() {
  local path="$1"
  local days="${2:-365}"  # Default to 1 year of history

  if [ -z "$path" ]; then
    echo "Error: Usage: suggest_reviewers <path> [days-back]" >&2
    echo "Example: suggest_reviewers path/to/file.yaml" >&2
    echo "Example: suggest_reviewers path/to/directory/ 180" >&2
    return 1
  fi

  if [ ! -e "$path" ]; then
    echo "Error: Path does not exist: $path" >&2
    return 1
  fi

  echo "Analyzing git history for: $path"
  echo "Looking back: $days days"
  echo ""

  # Get contributors with commit counts, weighted by recency
  # More recent commits get higher weight (exponential decay)
  local temp_file=$(mktemp)

  git log --since="${days} days ago" --format='%ae|%an|%at' --follow -- "$path" | \
  awk -F'|' -v now="$(date +%s)" -v days="$days" '
  {
    email = $1
    name = $2
    timestamp = $3

    # Calculate age in days
    age_days = (now - timestamp) / 86400

    # Exponential decay weight: newer commits worth more
    # Weight = e^(-age_days / half_life)
    # half_life = days/2 means commits at half the time window are worth 50%
    half_life = days / 2
    weight = exp(-age_days / half_life)

    # Accumulate weighted score and raw count
    scores[email] += weight
    counts[email] += 1
    names[email] = name
  }
  END {
    for (email in scores) {
      printf "%s|%s|%.2f|%d\n", email, names[email], scores[email], counts[email]
    }
  }
  ' | sort -t'|' -k3 -rn > "$temp_file"

  # Display results with GitHub account status
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  printf "%-25s %-20s %-10s %-15s %s\n" "Name" "Email" "Commits" "Score" "GitHub Status"
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

  local count=0
  while IFS='|' read -r email name score commits; do
    count=$((count + 1))

    # Truncate long names/emails for display
    local display_name="${name:0:24}"
    local display_email="${email:0:19}"

    # Try to find GitHub username
    local gh_user=""
    local gh_status="Unknown"

    # First try: search by email
    gh_user=$(gh api "/search/users?q=${email}" 2>/dev/null | jq -r '.items[0].login // empty' 2>/dev/null)

    # Second try: if email format is github email, extract username
    if [ -z "$gh_user" ] && [[ "$email" =~ ^([^@]+)@users\.noreply\.github\.com$ ]]; then
      gh_user="${BASH_REMATCH[1]}"
    fi

    if [ -n "$gh_user" ]; then
      # Check if account is suspended
      local suspended=$(gh api "/users/${gh_user}" 2>/dev/null | jq -r '.suspended_at // empty' 2>/dev/null)
      if [ -z "$suspended" ]; then
        gh_status="✓ ${gh_user}"
      else
        gh_status="✗ ${gh_user} (suspended)"
      fi
    fi

    printf "%-25s %-20s %-10s %-15s %s\n" "$display_name" "$display_email" "$commits" "$score" "$gh_status"

    # Limit to top 10 contributors
    if [ "$count" -ge 10 ]; then
      break
    fi
  done < "$temp_file"

  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  echo ""
  echo "Score calculation:"
  echo "  - Recent commits weighted higher (exponential decay)"
  echo "  - Half-life: $((days / 2)) days (commits at this age worth 50%)"
  echo "  - GitHub status: ✓ active, ✗ suspended, Unknown = no GitHub match"
  echo ""

  rm -f "$temp_file"
}

# ============================================================================
# Bash Completion
# ============================================================================

# Tab completion for jira_helper dispatcher
# Load bash completion if available
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
if [ -f "${SCRIPT_DIR}/jira-helper-completion.sh" ]; then
  source "${SCRIPT_DIR}/jira-helper-completion.sh"
  _log "Bash completion loaded for jira-helper functions"
fi
